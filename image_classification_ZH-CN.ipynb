{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 图像分类\n",
    "\n",
    "在该项目中，你将会对来自 [CIFAR-10 数据集](https://www.cs.toronto.edu/~kriz/cifar.html) 中的图像进行分类。数据集中图片的内容包括飞机（airplane）、狗（dogs）、猫（cats）及其他物体。你需要处理这些图像，接着对所有的样本训练一个卷积神经网络。\n",
    "\n",
    "具体而言，在项目中你要对图像进行正规化处理（normalization)，同时还要对图像的标签进行 one-hot 编码。接着你将会应用到你所学的技能来搭建一个具有卷积层、最大池化（Max Pooling）层、Dropout  层及全连接（fully connected）层的神经网络。最后，你会训练你的神经网络，会得到你神经网络在样本图像上的预测结果。\n",
    "\n",
    "## 下载数据\n",
    "\n",
    "运行如下代码下载 [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "3\n",
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile('cifar-10-python.tar.gz'):\n",
    "    print(\"2\")\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            'cifar-10-python.tar.gz',\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open('cifar-10-python.tar.gz') as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 探索数据集\n",
    "\n",
    "\n",
    "为防止在运行过程中内存不足的问题，该数据集已经事先被分成了5批（batch），名为`data_batch_1`、`data_batch_2`等。每一批中都含有 *图像* 及对应的 *标签*，都是如下类别中的一种：\n",
    "\n",
    "* 飞机\n",
    "* 汽车\n",
    "* 鸟\n",
    "* 鹿\n",
    "* 狗\n",
    "* 青蛙\n",
    "* 马\n",
    "* 船\n",
    "* 卡车\n",
    "\n",
    "理解数据集也是对数据进行预测的一部分。修改如下代码中的 `batch_id` 和 `sample_id`，看看输出的图像是什么样子。其中，`batch_id` 代表着批次数（1-5），`sample_id` 代表着在该批内图像及标签的编号。\n",
    "\n",
    "你可以尝试回答如下问题：\n",
    "* 可能出现的 *标签* 都包括哪些？\n",
    "* 图像数据的取值范围是多少？\n",
    "* *标签* 的排列顺序是随机的还是有序的？\n",
    "\n",
    "对这些问题的回答，会有助于更好地处理数据，并能更好地进行预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 1:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1005, 1: 974, 2: 1032, 3: 1016, 4: 999, 5: 937, 6: 1030, 7: 1001, 8: 1025, 9: 981}\n",
      "First 20 Labels: [6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6]\n",
      "\n",
      "Example of Image 0:\n",
      "Image - Min Value: 0 Max Value: 255\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 6 Name: frog\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n+4ZgtwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1bafb3b2d30>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 1\n",
    "sample_id = 0\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 图像预处理功能的实现\n",
    "### 正规化\n",
    "\n",
    "在如下的代码中，修改 `normalize` 函数，使之能够对输入的图像数据 `x` 进行处理，输出一个经过正规化的、Numpy array 格式的图像数据。\n",
    "\n",
    "**注意：**\n",
    "处理后的值应当在 $[0,1]$ 的范围之内。返回值应当和输入值具有相同的形状。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    max_value = np.max(x)\n",
    "    min_value = np.min(x)\n",
    "    \n",
    "    x = (x - min_value)/max_value\n",
    "        \n",
    "    return x\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.33333333]\n",
      " [ 0.33333333  0.66666667]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[1,2],[2,3]])\n",
    "print(normalize(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "**Hint:**\n",
    "\n",
    "Look into LabelBinarizer in the preprocessing module of sklearn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot 编码\n",
    "\n",
    "在如下代码中，你将继续实现预处理的功能，实现一个 `one_hot_encode` 函数。函数的输入 `x` 是 *标签* 构成的列表，返回值是经过 One_hot 处理过后的这列 *标签* 对应的 One_hot 编码，以 Numpy array 储存。其中，*标签* 的取值范围从0到9。每次调用该函数时，对相同的标签值，它输出的编码也是相同的。请确保在函数外保存编码的映射（map of encodings）。\n",
    "\n",
    "**提示：**\n",
    "\n",
    "你可以尝试使用 sklearn preprocessing 模块中的 `LabelBinarizer` 函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "     # TODO: Implement Function\n",
    "    lb = preprocessing.LabelBinarizer()\n",
    "    lb.fit(list(range(10)))\n",
    "    return lb.transform(x)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(one_hot_encode([1,2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 随机打乱数据\n",
    "\n",
    "正如你在上方探索数据部分所看到的，样本的顺序已经被随机打乱了。尽管再随机处理一次也没问题，不过对于该数据我们没必要再进行一次相关操作了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对所有图像数据进行预处理并保存结果\n",
    "\n",
    "运行如下代码，它将会预处理所有的 CIFAR-10 数据并将它另存为文件。此外，如下的代码还将会把 10% 的训练数据留出作为验证数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 检查点\n",
    "\n",
    "这是你的首个检查点。因为预处理完的数据已经被保存到硬盘上了，所以如果你需要回顾或重启该 notebook，你可以在这里重新开始。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 搭建神经网络\n",
    "\n",
    "为搭建神经网络，你需要将搭建每一层的过程封装到一个函数中。大部分的代码你在函数外已经见过。为能够更透彻地测试你的代码，我们要求你把每一层都封装到一个函数中。这能够帮助我们给予你更好的回复，同时还能让我们使用 unittests 在你提交报告前检测出你项目中的小问题。\n",
    "\n",
    ">**注意：** 如果你时间紧迫，那么在该部分我们为你提供了一个便捷方法。在接下来的一些问题中，你可以使用来自 [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) 或 [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) 包中的函数来搭建各层，不过不可以用他们搭建卷积-最大池化层。TF Layers 和 Keras 及 TFLean 中对层的抽象比较相似，所以你应该很容易上手。\n",
    "\n",
    ">\n",
    "\n",
    "\n",
    "However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "不过，如果你希望能够更多地实践，我们希望你能够在**不**使用 TF Layers 的情况下解决所有问题。你依然**能**使用来自其他包但和 layers 中重名的函数。例如，你可以使用 TF Neural Network 版本的 `conv_2d\n",
    "\n",
    "让我们开始吧！\n",
    "\n",
    "### 输入\n",
    "\n",
    "神经网络需要能够读取图像数据、经 one-hot 编码之后的标签及 dropout 中的保留概率。修改如下函数：\n",
    "\n",
    "* 修改 `neural_net_image_input` 函数：\n",
    " * 返回 [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)。\n",
    " * 使用 `image_shape` 设定形状，设定批大小（batch size)为 `None`。\n",
    " * 使用 [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) 中的 `Name` 参数，命名该 TensorFlow placeholder 为 \"x\"。\n",
    "* 修改 `neural_net_label_input` 函数： \n",
    " * 返回 [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)。\n",
    " * 使用 `n_classes` 设定形状，设定批大小（batch size)为 `None`。\n",
    " * 使用 [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) 中的 `Name` 参数，命名该 TensorFlow placeholder 为 \"y\"。\n",
    "* 修改 `neural_net_keep_prob_input` 函数：\n",
    " * 返回 [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) 作为 dropout 的保留概率（keep probability）。\n",
    " * 使用 [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) 中的 `Name` 参数，命名该 TensorFlow placeholder 为 \"keep_prob\"。\n",
    " \n",
    "我们会在项目最后使用这些名字，来载入你储存的模型。\n",
    "\n",
    "**注意：**在 TensorFlow 中，对形状设定为 `None`，能帮助设定一个动态的大小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    #print(image_shape)\n",
    "    #rows = image_shape[0]\n",
    "    #print(rows)\n",
    "    dtype = tf.float32\n",
    "    shape = [None,image_shape[0],image_shape[1],image_shape[2]]\n",
    "    name = \"x\"\n",
    "    return tf.placeholder(dtype, shape, name)\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    dtype = tf.int32\n",
    "    shape = [None,n_classes]\n",
    "    name = \"y\"\n",
    "    return tf.placeholder(dtype, shape, name)\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    dtype = tf.float32\n",
    "    shape = None\n",
    "    name = \"keep_prob\"\n",
    "    return tf.placeholder(dtype, shape, name)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers.\n",
    "\n",
    "** Hint: **\n",
    "\n",
    "When unpacking values as an argument in Python, look into the [unpacking](https://docs.python.org/3/tutorial/controlflow.html#unpacking-argument-lists) operator. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 卷积-最大池（Convolution and Max Pooling）化层\n",
    "\n",
    "卷积层在图像处理中取得了不小的成功。在这部分的代码中，你需要修改 `conv2d_maxpool` 函数来先后实现卷积及最大池化的功能。\n",
    "\n",
    "* 使用 `conv_ksize`、`conv_num_outputs` 及 `x_tensor` 来创建权重（weight）及偏差（bias）变量。\n",
    "* 对 `x_tensor` 进行卷积，使用 `conv_strides` 及*权重*。\n",
    " * 我们建议使用 SAME padding，不过你也可尝试其他 padding 模式。 \n",
    "* 加上*偏差*。\n",
    "* 对卷积结果加上一个非线性函数作为激活层。\n",
    "* 基于 `pool_kszie` 及 `pool_strides` 进行最大池化。\n",
    " * 我们建议使用 SAME padding，不过你也可尝试其他 padding 模式。\n",
    " \n",
    "**注意：**\n",
    "你**不**可以使用来自 [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) 或 [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) 包中的函数来实现**这一层**的功能。但是你可以使用 TensorFlow 的[Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn)包。\n",
    "\n",
    "对于如上的快捷方法，你在**其他层**中可以尝试使用。\n",
    "\n",
    "\n",
    "**提示：**\n",
    "当你在 Python 中希望展开（unpacking）某个变量的值作为函数的参数，你可以参考 [unpacking](https://docs.python.org/3/tutorial/controlflow.html#unpacking-argument-lists) 运算符。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    # Weight and bias\n",
    "    shape = x_tensor.get_shape().as_list()\n",
    "    #print(shape[3])\n",
    "    weight = tf.Variable(tf.truncated_normal([*conv_ksize,shape[3],conv_num_outputs]))\n",
    "    strides = [1,*conv_strides,1]\n",
    "    bias = tf.Variable(tf.zeros(conv_num_outputs))\n",
    "    \n",
    "    conv_layer = tf.nn.conv2d(x_tensor, weight, strides, padding='SAME')\n",
    "    conv_layer = tf.nn.bias_add(conv_layer, bias)\n",
    "    conv_layer = tf.nn.relu(conv_layer)\n",
    "    \n",
    "    p_size = [1,*pool_ksize,1]\n",
    "    p_strides = [1,*pool_strides,1]\n",
    "    return tf.nn.max_pool(conv_layer, p_size, p_strides, padding='SAME')\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 展开层\n",
    "\n",
    "修改 `flatten` 函数，来将4维的输入张量 `x_tensor` 转换为一个二维的张量。输出的形状应当是 `(Batch Size, Flattened Image Size)`。\n",
    "快捷方法：你可以使用来自 [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) 或 [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) 包中的函数来实现该功能。不过你也可以只使用 TensorFlow 包中的函数来挑战自己。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Flatten_1/Reshape:0\", shape=(2, 4), dtype=int32)\n",
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    #shape = x_tensor.get_shape().as_list()\n",
    "    return tf.contrib.layers.flatten(x_tensor)\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "x = [[[1,2],[1,2]],[[1,2],[1,2]]]\n",
    "print(flatten(x))\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 全连接层\n",
    "\n",
    "修改 `fully_conn` 函数，来对形如 `(batch Size, num_outputs)` 的输入 `x_tensor` 应用一个全连接层。快捷方法：你可以使用来自 [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) 或 [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) 包中的函数来实现该功能。不过你也可以只使用 TensorFlow 包中的函数来挑战自己。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.contrib.layers.fully_connected(x_tensor, num_outputs)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 输出层\n",
    "\n",
    "修改 `output` 函数，来对形如 `(batch Size, num_outputs)` 的输入 `x_tensor` 应用一个全连接层。快捷方法：你可以使用来自 [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) 或 [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) 包中的函数来实现该功能。不过你也可以只使用 TensorFlow 包中的函数来挑战自己。\n",
    "\n",
    "**注意：**\n",
    "激活函数、softmax 或者交叉熵（corss entropy）**不**应被加入到该层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.contrib.layers.fully_connected(x_tensor, num_outputs)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建卷积模型\n",
    "\n",
    "修改 `conv_net` 函数，使之能够生成一个卷积神经网络模型。该函数的输入为一批图像数据 `x`，输出为 logits。在函数中，使用上方你修改的创建各种层的函数来创建该模型：\n",
    "\n",
    "* 使用 1 到 3 个卷积-最大池化层\n",
    "* 使用一个展开层\n",
    "* 使用 1 到 3 个全连接层\n",
    "* 使用一个输出层\n",
    "* 返回呼出结果\n",
    "* 在一个或多个层上使用 [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout)，对应的保留概率为 `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "3\n",
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    \n",
    "    #这些参数如何获取，还是自己设置的\n",
    "    \n",
    "    conv_num_outputs = 10\n",
    "    conv_ksize = [5,5]\n",
    "    conv_strides = [2,2]\n",
    "    pool_ksize = [3,3]\n",
    "    pool_strides = [2,2]\n",
    "    hidden_layer_1 = conv2d_maxpool(x, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    #hidden_layer_1 = tf.nn.dropout(hidden_layer_1,keep_prob)\n",
    "    \n",
    "    '''\n",
    "    conv_num_outputs = 30\n",
    "    conv_ksize = [3,3]\n",
    "    conv_strides = [2,2]\n",
    "    pool_ksize = [3,3]\n",
    "    pool_strides = [2,2]\n",
    "    hidden_layer_2 = conv2d_maxpool(hidden_layer_1, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    #hidden_layer_2 = tf.nn.dropout(hidden_layer_2,keep_prob)\n",
    "    '''\n",
    "    \n",
    "\n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    hidden_layer_flatten = flatten(hidden_layer_1)\n",
    "    #hidden_layer_flatten = tf.nn.dropout(hidden_layer_flatten,keep_prob)\n",
    "\n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    num_outputs = 30\n",
    "    f_c_1 = fully_conn(hidden_layer_flatten, num_outputs)\n",
    "    #f_c_1 = tf.nn.dropout(f_c_1,keep_prob)\n",
    "    \n",
    "    num_outputs = 20\n",
    "    f_c_2 = fully_conn(f_c_1, num_outputs)\n",
    "    \n",
    "    \n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    num_outputs = 10\n",
    "    logits = output(f_c_2, num_outputs)\n",
    "    \n",
    "    # TODO: return output\n",
    "    return logits\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练该神经网络\n",
    "\n",
    "### 最优化\n",
    "\n",
    "修改 `train_neural_network` 函数以执行单次最优化。该最优化过程应在一个 `session` 中使用 `optimizer` 来进行该过程，它的 `feed_dict` 包括：\n",
    "* `x` 代表输入图像\n",
    "* `y` 代表*标签*\n",
    "* `keep_prob` 为 Dropout 过程中的保留概率\n",
    "\n",
    "对每批数据该函数都会被调用，因而 `tf.global_variables_initializer()` 已经被调用过。\n",
    "\n",
    "注意：该函数并不要返回某个值，它只对神经网络进行最优化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    #pass\n",
    "    \n",
    "    session.run(optimizer, feed_dict={x: feature_batch,y: label_batch,keep_prob: keep_probability})\n",
    "    \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 显示状态\n",
    "\n",
    "修改 `print_stats` 函数来打印 loss 值及验证准确率。 使用全局的变量 `valid_features` 及 `valid_labels` 来计算验证准确率。 设定保留概率为 1.0 来计算 loss 值及验证准确率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    #pass\n",
    "    loss = sess.run(cost, feed_dict={x: feature_batch,y: label_batch,keep_prob: 1.})\n",
    "    valid_acc = sess.run(accuracy, feed_dict={x: feature_batch,y: label_batch,keep_prob: 1.})\n",
    "    print('Loss: {:>10.4f} Validation Accuracy: {:.6f}'.format(loss,valid_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 超参数调节\n",
    "\n",
    "你需要调节如下的参数：\n",
    "* 设定 `epoches` 为模型停止学习或开始过拟合时模型的迭代次数。\n",
    "* 设定 `batch_size` 为你内存能支持的最大值。一般我们设定该值为：\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* 设定 `keep_probability` 为在 dropout 过程中保留一个节点的概率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 512\n",
    "batch_size = 256\n",
    "keep_probability = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对单批 CIFAR-10 数据进行训练\n",
    "\n",
    "相比于在所有 CIFAR-10 数据上训练神经网络，我们首先使用一批数据进行训练。这会帮助你在调节模型提高精度的过程中节省时间。当最终的验证精度超过 50% 之后，你就可以前往下一节在所有数据上运行该模型了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss:     2.2909 Validation Accuracy: 0.175000\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss:     2.3465 Validation Accuracy: 0.225000\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss:     2.2806 Validation Accuracy: 0.275000\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss:     2.0252 Validation Accuracy: 0.350000\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss:     1.9002 Validation Accuracy: 0.400000\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss:     1.8617 Validation Accuracy: 0.475000\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss:     1.7933 Validation Accuracy: 0.475000\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss:     1.7254 Validation Accuracy: 0.475000\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss:     1.6729 Validation Accuracy: 0.475000\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss:     1.6359 Validation Accuracy: 0.475000\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss:     1.5980 Validation Accuracy: 0.475000\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss:     1.5600 Validation Accuracy: 0.475000\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss:     1.5193 Validation Accuracy: 0.500000\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss:     1.4833 Validation Accuracy: 0.500000\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss:     1.4575 Validation Accuracy: 0.500000\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss:     1.4318 Validation Accuracy: 0.500000\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss:     1.4112 Validation Accuracy: 0.525000\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss:     1.3922 Validation Accuracy: 0.525000\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss:     1.3727 Validation Accuracy: 0.550000\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss:     1.3542 Validation Accuracy: 0.575000\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss:     1.3354 Validation Accuracy: 0.575000\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss:     1.3199 Validation Accuracy: 0.575000\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss:     1.3083 Validation Accuracy: 0.550000\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss:     1.2932 Validation Accuracy: 0.550000\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss:     1.2783 Validation Accuracy: 0.550000\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss:     1.2617 Validation Accuracy: 0.550000\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss:     1.2421 Validation Accuracy: 0.550000\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss:     1.2200 Validation Accuracy: 0.550000\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss:     1.1959 Validation Accuracy: 0.575000\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss:     1.1734 Validation Accuracy: 0.575000\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss:     1.1557 Validation Accuracy: 0.575000\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss:     1.1365 Validation Accuracy: 0.575000\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss:     1.1240 Validation Accuracy: 0.600000\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss:     1.1031 Validation Accuracy: 0.625000\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss:     1.0877 Validation Accuracy: 0.625000\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss:     1.0723 Validation Accuracy: 0.650000\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss:     1.0607 Validation Accuracy: 0.650000\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss:     1.0430 Validation Accuracy: 0.650000\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss:     1.0341 Validation Accuracy: 0.675000\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss:     1.0232 Validation Accuracy: 0.675000\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss:     1.0003 Validation Accuracy: 0.675000\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss:     0.9875 Validation Accuracy: 0.675000\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss:     0.9754 Validation Accuracy: 0.700000\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss:     0.9648 Validation Accuracy: 0.700000\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss:     0.9583 Validation Accuracy: 0.700000\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss:     0.9464 Validation Accuracy: 0.700000\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss:     0.9367 Validation Accuracy: 0.700000\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss:     0.9220 Validation Accuracy: 0.700000\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss:     0.9108 Validation Accuracy: 0.700000\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss:     0.8980 Validation Accuracy: 0.700000\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss:     0.8907 Validation Accuracy: 0.725000\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss:     0.8739 Validation Accuracy: 0.750000\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss:     0.8682 Validation Accuracy: 0.775000\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss:     0.8632 Validation Accuracy: 0.775000\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss:     0.8520 Validation Accuracy: 0.775000\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss:     0.8466 Validation Accuracy: 0.775000\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss:     0.8390 Validation Accuracy: 0.775000\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss:     0.8292 Validation Accuracy: 0.775000\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss:     0.8266 Validation Accuracy: 0.775000\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss:     0.8147 Validation Accuracy: 0.775000\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss:     0.8085 Validation Accuracy: 0.775000\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss:     0.8014 Validation Accuracy: 0.775000\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss:     0.7975 Validation Accuracy: 0.775000\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss:     0.7884 Validation Accuracy: 0.775000\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss:     0.7774 Validation Accuracy: 0.775000\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss:     0.7709 Validation Accuracy: 0.800000\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss:     0.7655 Validation Accuracy: 0.800000\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss:     0.7609 Validation Accuracy: 0.800000\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss:     0.7553 Validation Accuracy: 0.800000\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss:     0.7497 Validation Accuracy: 0.825000\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss:     0.7420 Validation Accuracy: 0.825000\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss:     0.7360 Validation Accuracy: 0.825000\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss:     0.7350 Validation Accuracy: 0.800000\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss:     0.7282 Validation Accuracy: 0.800000\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss:     0.7252 Validation Accuracy: 0.825000\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss:     0.7171 Validation Accuracy: 0.825000\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss:     0.7136 Validation Accuracy: 0.775000\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss:     0.7081 Validation Accuracy: 0.800000\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss:     0.7028 Validation Accuracy: 0.800000\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss:     0.6955 Validation Accuracy: 0.800000\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss:     0.6943 Validation Accuracy: 0.800000\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss:     0.6868 Validation Accuracy: 0.775000\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss:     0.6858 Validation Accuracy: 0.775000\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss:     0.6803 Validation Accuracy: 0.775000\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss:     0.6757 Validation Accuracy: 0.775000\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss:     0.6724 Validation Accuracy: 0.775000\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss:     0.6691 Validation Accuracy: 0.775000\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss:     0.6646 Validation Accuracy: 0.800000\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss:     0.6627 Validation Accuracy: 0.800000\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss:     0.6627 Validation Accuracy: 0.825000\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss:     0.6601 Validation Accuracy: 0.825000\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss:     0.6571 Validation Accuracy: 0.825000\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss:     0.6611 Validation Accuracy: 0.825000\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss:     0.6608 Validation Accuracy: 0.850000\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss:     0.6716 Validation Accuracy: 0.875000\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss:     0.6649 Validation Accuracy: 0.875000\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss:     0.6586 Validation Accuracy: 0.875000\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss:     0.6427 Validation Accuracy: 0.875000\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss:     0.6336 Validation Accuracy: 0.900000\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss:     0.6349 Validation Accuracy: 0.900000\n",
      "Epoch 101, CIFAR-10 Batch 1:  Loss:     0.6280 Validation Accuracy: 0.900000\n",
      "Epoch 102, CIFAR-10 Batch 1:  Loss:     0.6305 Validation Accuracy: 0.900000\n",
      "Epoch 103, CIFAR-10 Batch 1:  Loss:     0.6245 Validation Accuracy: 0.900000\n",
      "Epoch 104, CIFAR-10 Batch 1:  Loss:     0.6226 Validation Accuracy: 0.900000\n",
      "Epoch 105, CIFAR-10 Batch 1:  Loss:     0.6198 Validation Accuracy: 0.900000\n",
      "Epoch 106, CIFAR-10 Batch 1:  Loss:     0.6168 Validation Accuracy: 0.900000\n",
      "Epoch 107, CIFAR-10 Batch 1:  Loss:     0.6105 Validation Accuracy: 0.900000\n",
      "Epoch 108, CIFAR-10 Batch 1:  Loss:     0.6134 Validation Accuracy: 0.900000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 109, CIFAR-10 Batch 1:  Loss:     0.6122 Validation Accuracy: 0.875000\n",
      "Epoch 110, CIFAR-10 Batch 1:  Loss:     0.6057 Validation Accuracy: 0.900000\n",
      "Epoch 111, CIFAR-10 Batch 1:  Loss:     0.6100 Validation Accuracy: 0.900000\n",
      "Epoch 112, CIFAR-10 Batch 1:  Loss:     0.6057 Validation Accuracy: 0.875000\n",
      "Epoch 113, CIFAR-10 Batch 1:  Loss:     0.6026 Validation Accuracy: 0.900000\n",
      "Epoch 114, CIFAR-10 Batch 1:  Loss:     0.6011 Validation Accuracy: 0.875000\n",
      "Epoch 115, CIFAR-10 Batch 1:  Loss:     0.6010 Validation Accuracy: 0.875000\n",
      "Epoch 116, CIFAR-10 Batch 1:  Loss:     0.5965 Validation Accuracy: 0.900000\n",
      "Epoch 117, CIFAR-10 Batch 1:  Loss:     0.5968 Validation Accuracy: 0.900000\n",
      "Epoch 118, CIFAR-10 Batch 1:  Loss:     0.5886 Validation Accuracy: 0.900000\n",
      "Epoch 119, CIFAR-10 Batch 1:  Loss:     0.5908 Validation Accuracy: 0.900000\n",
      "Epoch 120, CIFAR-10 Batch 1:  Loss:     0.5879 Validation Accuracy: 0.925000\n",
      "Epoch 121, CIFAR-10 Batch 1:  Loss:     0.5815 Validation Accuracy: 0.900000\n",
      "Epoch 122, CIFAR-10 Batch 1:  Loss:     0.5843 Validation Accuracy: 0.900000\n",
      "Epoch 123, CIFAR-10 Batch 1:  Loss:     0.5821 Validation Accuracy: 0.900000\n",
      "Epoch 124, CIFAR-10 Batch 1:  Loss:     0.5810 Validation Accuracy: 0.900000\n",
      "Epoch 125, CIFAR-10 Batch 1:  Loss:     0.5778 Validation Accuracy: 0.900000\n",
      "Epoch 126, CIFAR-10 Batch 1:  Loss:     0.5801 Validation Accuracy: 0.900000\n",
      "Epoch 127, CIFAR-10 Batch 1:  Loss:     0.5769 Validation Accuracy: 0.925000\n",
      "Epoch 128, CIFAR-10 Batch 1:  Loss:     0.5742 Validation Accuracy: 0.925000\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 完全训练该模型\n",
    "\n",
    "因为你在单批 CIFAR-10 数据上已经得到了一个不错的准确率了，那你可以尝试在所有五批数据上进行训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss:     2.2836 Validation Accuracy: 0.150000\n",
      "Epoch  1, CIFAR-10 Batch 2:  Loss:     2.2837 Validation Accuracy: 0.125000\n",
      "Epoch  1, CIFAR-10 Batch 3:  Loss:     2.2173 Validation Accuracy: 0.125000\n",
      "Epoch  1, CIFAR-10 Batch 4:  Loss:     2.2791 Validation Accuracy: 0.200000\n",
      "Epoch  1, CIFAR-10 Batch 5:  Loss:     2.2120 Validation Accuracy: 0.250000\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss:     2.2746 Validation Accuracy: 0.125000\n",
      "Epoch  2, CIFAR-10 Batch 2:  Loss:     2.2124 Validation Accuracy: 0.175000\n",
      "Epoch  2, CIFAR-10 Batch 3:  Loss:     2.2131 Validation Accuracy: 0.200000\n",
      "Epoch  2, CIFAR-10 Batch 4:  Loss:     2.2594 Validation Accuracy: 0.175000\n",
      "Epoch  2, CIFAR-10 Batch 5:  Loss:     2.1656 Validation Accuracy: 0.225000\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss:     2.2293 Validation Accuracy: 0.175000\n",
      "Epoch  3, CIFAR-10 Batch 2:  Loss:     2.1452 Validation Accuracy: 0.225000\n",
      "Epoch  3, CIFAR-10 Batch 3:  Loss:     2.2000 Validation Accuracy: 0.225000\n",
      "Epoch  3, CIFAR-10 Batch 4:  Loss:     2.2030 Validation Accuracy: 0.250000\n",
      "Epoch  3, CIFAR-10 Batch 5:  Loss:     2.1324 Validation Accuracy: 0.225000\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss:     2.2149 Validation Accuracy: 0.175000\n",
      "Epoch  4, CIFAR-10 Batch 2:  Loss:     2.1247 Validation Accuracy: 0.225000\n",
      "Epoch  4, CIFAR-10 Batch 3:  Loss:     2.1690 Validation Accuracy: 0.250000\n",
      "Epoch  4, CIFAR-10 Batch 4:  Loss:     2.1886 Validation Accuracy: 0.225000\n",
      "Epoch  4, CIFAR-10 Batch 5:  Loss:     2.0893 Validation Accuracy: 0.250000\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss:     2.1594 Validation Accuracy: 0.250000\n",
      "Epoch  5, CIFAR-10 Batch 2:  Loss:     1.9623 Validation Accuracy: 0.275000\n",
      "Epoch  5, CIFAR-10 Batch 3:  Loss:     1.8733 Validation Accuracy: 0.325000\n",
      "Epoch  5, CIFAR-10 Batch 4:  Loss:     1.9184 Validation Accuracy: 0.325000\n",
      "Epoch  5, CIFAR-10 Batch 5:  Loss:     1.8194 Validation Accuracy: 0.425000\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss:     2.0211 Validation Accuracy: 0.300000\n",
      "Epoch  6, CIFAR-10 Batch 2:  Loss:     1.8398 Validation Accuracy: 0.325000\n",
      "Epoch  6, CIFAR-10 Batch 3:  Loss:     1.6623 Validation Accuracy: 0.375000\n",
      "Epoch  6, CIFAR-10 Batch 4:  Loss:     1.8268 Validation Accuracy: 0.400000\n",
      "Epoch  6, CIFAR-10 Batch 5:  Loss:     1.7539 Validation Accuracy: 0.425000\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss:     1.9107 Validation Accuracy: 0.325000\n",
      "Epoch  7, CIFAR-10 Batch 2:  Loss:     1.7649 Validation Accuracy: 0.350000\n",
      "Epoch  7, CIFAR-10 Batch 3:  Loss:     1.5722 Validation Accuracy: 0.475000\n",
      "Epoch  7, CIFAR-10 Batch 4:  Loss:     1.7670 Validation Accuracy: 0.325000\n",
      "Epoch  7, CIFAR-10 Batch 5:  Loss:     1.7314 Validation Accuracy: 0.450000\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss:     1.8064 Validation Accuracy: 0.350000\n",
      "Epoch  8, CIFAR-10 Batch 2:  Loss:     1.6926 Validation Accuracy: 0.325000\n",
      "Epoch  8, CIFAR-10 Batch 3:  Loss:     1.5018 Validation Accuracy: 0.500000\n",
      "Epoch  8, CIFAR-10 Batch 4:  Loss:     1.7291 Validation Accuracy: 0.400000\n",
      "Epoch  8, CIFAR-10 Batch 5:  Loss:     1.6754 Validation Accuracy: 0.475000\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss:     1.7722 Validation Accuracy: 0.350000\n",
      "Epoch  9, CIFAR-10 Batch 2:  Loss:     1.6256 Validation Accuracy: 0.375000\n",
      "Epoch  9, CIFAR-10 Batch 3:  Loss:     1.4490 Validation Accuracy: 0.575000\n",
      "Epoch  9, CIFAR-10 Batch 4:  Loss:     1.7041 Validation Accuracy: 0.400000\n",
      "Epoch  9, CIFAR-10 Batch 5:  Loss:     1.6420 Validation Accuracy: 0.500000\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss:     1.7457 Validation Accuracy: 0.400000\n",
      "Epoch 10, CIFAR-10 Batch 2:  Loss:     1.5489 Validation Accuracy: 0.400000\n",
      "Epoch 10, CIFAR-10 Batch 3:  Loss:     1.4009 Validation Accuracy: 0.550000\n",
      "Epoch 10, CIFAR-10 Batch 4:  Loss:     1.7042 Validation Accuracy: 0.400000\n",
      "Epoch 10, CIFAR-10 Batch 5:  Loss:     1.6196 Validation Accuracy: 0.500000\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss:     1.6642 Validation Accuracy: 0.475000\n",
      "Epoch 11, CIFAR-10 Batch 2:  Loss:     1.2919 Validation Accuracy: 0.500000\n",
      "Epoch 11, CIFAR-10 Batch 3:  Loss:     1.2168 Validation Accuracy: 0.700000\n",
      "Epoch 11, CIFAR-10 Batch 4:  Loss:     1.7034 Validation Accuracy: 0.525000\n",
      "Epoch 11, CIFAR-10 Batch 5:  Loss:     1.4803 Validation Accuracy: 0.600000\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss:     1.5876 Validation Accuracy: 0.500000\n",
      "Epoch 12, CIFAR-10 Batch 2:  Loss:     1.2024 Validation Accuracy: 0.575000\n",
      "Epoch 12, CIFAR-10 Batch 3:  Loss:     1.1725 Validation Accuracy: 0.700000\n",
      "Epoch 12, CIFAR-10 Batch 4:  Loss:     1.6811 Validation Accuracy: 0.550000\n",
      "Epoch 12, CIFAR-10 Batch 5:  Loss:     1.4325 Validation Accuracy: 0.600000\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss:     1.5369 Validation Accuracy: 0.475000\n",
      "Epoch 13, CIFAR-10 Batch 2:  Loss:     1.1704 Validation Accuracy: 0.600000\n",
      "Epoch 13, CIFAR-10 Batch 3:  Loss:     1.1242 Validation Accuracy: 0.750000\n",
      "Epoch 13, CIFAR-10 Batch 4:  Loss:     1.6753 Validation Accuracy: 0.500000\n",
      "Epoch 13, CIFAR-10 Batch 5:  Loss:     1.3974 Validation Accuracy: 0.600000\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss:     1.5151 Validation Accuracy: 0.475000\n",
      "Epoch 14, CIFAR-10 Batch 2:  Loss:     1.1381 Validation Accuracy: 0.600000\n",
      "Epoch 14, CIFAR-10 Batch 3:  Loss:     1.0871 Validation Accuracy: 0.750000\n",
      "Epoch 14, CIFAR-10 Batch 4:  Loss:     1.6538 Validation Accuracy: 0.525000\n",
      "Epoch 14, CIFAR-10 Batch 5:  Loss:     1.3640 Validation Accuracy: 0.600000\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss:     1.4845 Validation Accuracy: 0.500000\n",
      "Epoch 15, CIFAR-10 Batch 2:  Loss:     1.1116 Validation Accuracy: 0.625000\n",
      "Epoch 15, CIFAR-10 Batch 3:  Loss:     1.0655 Validation Accuracy: 0.750000\n",
      "Epoch 15, CIFAR-10 Batch 4:  Loss:     1.6269 Validation Accuracy: 0.575000\n",
      "Epoch 15, CIFAR-10 Batch 5:  Loss:     1.3333 Validation Accuracy: 0.600000\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss:     1.4592 Validation Accuracy: 0.500000\n",
      "Epoch 16, CIFAR-10 Batch 2:  Loss:     1.0885 Validation Accuracy: 0.650000\n",
      "Epoch 16, CIFAR-10 Batch 3:  Loss:     1.0455 Validation Accuracy: 0.750000\n",
      "Epoch 16, CIFAR-10 Batch 4:  Loss:     1.5940 Validation Accuracy: 0.550000\n",
      "Epoch 16, CIFAR-10 Batch 5:  Loss:     1.3100 Validation Accuracy: 0.600000\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss:     1.4435 Validation Accuracy: 0.525000\n",
      "Epoch 17, CIFAR-10 Batch 2:  Loss:     1.0765 Validation Accuracy: 0.650000\n",
      "Epoch 17, CIFAR-10 Batch 3:  Loss:     1.0330 Validation Accuracy: 0.725000\n",
      "Epoch 17, CIFAR-10 Batch 4:  Loss:     1.5736 Validation Accuracy: 0.550000\n",
      "Epoch 17, CIFAR-10 Batch 5:  Loss:     1.2846 Validation Accuracy: 0.650000\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss:     1.4307 Validation Accuracy: 0.525000\n",
      "Epoch 18, CIFAR-10 Batch 2:  Loss:     1.0600 Validation Accuracy: 0.675000\n",
      "Epoch 18, CIFAR-10 Batch 3:  Loss:     1.0151 Validation Accuracy: 0.750000\n",
      "Epoch 18, CIFAR-10 Batch 4:  Loss:     1.5482 Validation Accuracy: 0.550000\n",
      "Epoch 18, CIFAR-10 Batch 5:  Loss:     1.2641 Validation Accuracy: 0.650000\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss:     1.4181 Validation Accuracy: 0.525000\n",
      "Epoch 19, CIFAR-10 Batch 2:  Loss:     1.0475 Validation Accuracy: 0.675000\n",
      "Epoch 19, CIFAR-10 Batch 3:  Loss:     1.0055 Validation Accuracy: 0.750000\n",
      "Epoch 19, CIFAR-10 Batch 4:  Loss:     1.5303 Validation Accuracy: 0.550000\n",
      "Epoch 19, CIFAR-10 Batch 5:  Loss:     1.2494 Validation Accuracy: 0.650000\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss:     1.4007 Validation Accuracy: 0.550000\n",
      "Epoch 20, CIFAR-10 Batch 2:  Loss:     1.0458 Validation Accuracy: 0.675000\n",
      "Epoch 20, CIFAR-10 Batch 3:  Loss:     0.9802 Validation Accuracy: 0.750000\n",
      "Epoch 20, CIFAR-10 Batch 4:  Loss:     1.5117 Validation Accuracy: 0.575000\n",
      "Epoch 20, CIFAR-10 Batch 5:  Loss:     1.2320 Validation Accuracy: 0.650000\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss:     1.3876 Validation Accuracy: 0.550000\n",
      "Epoch 21, CIFAR-10 Batch 2:  Loss:     1.0398 Validation Accuracy: 0.675000\n",
      "Epoch 21, CIFAR-10 Batch 3:  Loss:     0.9754 Validation Accuracy: 0.775000\n",
      "Epoch 21, CIFAR-10 Batch 4:  Loss:     1.4915 Validation Accuracy: 0.575000\n",
      "Epoch 21, CIFAR-10 Batch 5:  Loss:     1.2212 Validation Accuracy: 0.650000\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss:     1.3767 Validation Accuracy: 0.575000\n",
      "Epoch 22, CIFAR-10 Batch 2:  Loss:     1.0303 Validation Accuracy: 0.675000\n",
      "Epoch 22, CIFAR-10 Batch 3:  Loss:     0.9591 Validation Accuracy: 0.775000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22, CIFAR-10 Batch 4:  Loss:     1.4744 Validation Accuracy: 0.575000\n",
      "Epoch 22, CIFAR-10 Batch 5:  Loss:     1.2093 Validation Accuracy: 0.625000\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss:     1.3574 Validation Accuracy: 0.575000\n",
      "Epoch 23, CIFAR-10 Batch 2:  Loss:     1.0220 Validation Accuracy: 0.675000\n",
      "Epoch 23, CIFAR-10 Batch 3:  Loss:     0.9429 Validation Accuracy: 0.800000\n",
      "Epoch 23, CIFAR-10 Batch 4:  Loss:     1.4555 Validation Accuracy: 0.575000\n",
      "Epoch 23, CIFAR-10 Batch 5:  Loss:     1.1985 Validation Accuracy: 0.650000\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss:     1.3463 Validation Accuracy: 0.575000\n",
      "Epoch 24, CIFAR-10 Batch 2:  Loss:     1.0034 Validation Accuracy: 0.675000\n",
      "Epoch 24, CIFAR-10 Batch 3:  Loss:     0.9394 Validation Accuracy: 0.800000\n",
      "Epoch 24, CIFAR-10 Batch 4:  Loss:     1.4370 Validation Accuracy: 0.575000\n",
      "Epoch 24, CIFAR-10 Batch 5:  Loss:     1.1919 Validation Accuracy: 0.650000\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss:     1.3362 Validation Accuracy: 0.575000\n",
      "Epoch 25, CIFAR-10 Batch 2:  Loss:     0.9879 Validation Accuracy: 0.675000\n",
      "Epoch 25, CIFAR-10 Batch 3:  Loss:     0.9190 Validation Accuracy: 0.800000\n",
      "Epoch 25, CIFAR-10 Batch 4:  Loss:     1.4102 Validation Accuracy: 0.575000\n",
      "Epoch 25, CIFAR-10 Batch 5:  Loss:     1.1839 Validation Accuracy: 0.675000\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss:     1.3260 Validation Accuracy: 0.550000\n",
      "Epoch 26, CIFAR-10 Batch 2:  Loss:     0.9661 Validation Accuracy: 0.675000\n",
      "Epoch 26, CIFAR-10 Batch 3:  Loss:     0.9058 Validation Accuracy: 0.800000\n",
      "Epoch 26, CIFAR-10 Batch 4:  Loss:     1.3899 Validation Accuracy: 0.575000\n",
      "Epoch 26, CIFAR-10 Batch 5:  Loss:     1.1744 Validation Accuracy: 0.700000\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss:     1.3233 Validation Accuracy: 0.550000\n",
      "Epoch 27, CIFAR-10 Batch 2:  Loss:     0.9423 Validation Accuracy: 0.700000\n",
      "Epoch 27, CIFAR-10 Batch 3:  Loss:     0.8996 Validation Accuracy: 0.800000\n",
      "Epoch 27, CIFAR-10 Batch 4:  Loss:     1.3743 Validation Accuracy: 0.600000\n",
      "Epoch 27, CIFAR-10 Batch 5:  Loss:     1.1651 Validation Accuracy: 0.700000\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss:     1.3163 Validation Accuracy: 0.550000\n",
      "Epoch 28, CIFAR-10 Batch 2:  Loss:     0.9294 Validation Accuracy: 0.700000\n",
      "Epoch 28, CIFAR-10 Batch 3:  Loss:     0.8873 Validation Accuracy: 0.800000\n",
      "Epoch 28, CIFAR-10 Batch 4:  Loss:     1.3592 Validation Accuracy: 0.600000\n",
      "Epoch 28, CIFAR-10 Batch 5:  Loss:     1.1600 Validation Accuracy: 0.700000\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss:     1.3115 Validation Accuracy: 0.550000\n",
      "Epoch 29, CIFAR-10 Batch 2:  Loss:     0.9216 Validation Accuracy: 0.675000\n",
      "Epoch 29, CIFAR-10 Batch 3:  Loss:     0.8811 Validation Accuracy: 0.800000\n",
      "Epoch 29, CIFAR-10 Batch 4:  Loss:     1.3500 Validation Accuracy: 0.600000\n",
      "Epoch 29, CIFAR-10 Batch 5:  Loss:     1.1545 Validation Accuracy: 0.700000\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss:     1.3030 Validation Accuracy: 0.550000\n",
      "Epoch 30, CIFAR-10 Batch 2:  Loss:     0.9102 Validation Accuracy: 0.675000\n",
      "Epoch 30, CIFAR-10 Batch 3:  Loss:     0.8635 Validation Accuracy: 0.800000\n",
      "Epoch 30, CIFAR-10 Batch 4:  Loss:     1.3374 Validation Accuracy: 0.600000\n",
      "Epoch 30, CIFAR-10 Batch 5:  Loss:     1.1502 Validation Accuracy: 0.700000\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss:     1.2998 Validation Accuracy: 0.550000\n",
      "Epoch 31, CIFAR-10 Batch 2:  Loss:     0.9033 Validation Accuracy: 0.675000\n",
      "Epoch 31, CIFAR-10 Batch 3:  Loss:     0.8552 Validation Accuracy: 0.800000\n",
      "Epoch 31, CIFAR-10 Batch 4:  Loss:     1.3311 Validation Accuracy: 0.600000\n",
      "Epoch 31, CIFAR-10 Batch 5:  Loss:     1.1441 Validation Accuracy: 0.700000\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss:     1.2920 Validation Accuracy: 0.550000\n",
      "Epoch 32, CIFAR-10 Batch 2:  Loss:     0.8921 Validation Accuracy: 0.700000\n",
      "Epoch 32, CIFAR-10 Batch 3:  Loss:     0.8457 Validation Accuracy: 0.800000\n",
      "Epoch 32, CIFAR-10 Batch 4:  Loss:     1.3198 Validation Accuracy: 0.600000\n",
      "Epoch 32, CIFAR-10 Batch 5:  Loss:     1.1416 Validation Accuracy: 0.700000\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss:     1.2894 Validation Accuracy: 0.550000\n",
      "Epoch 33, CIFAR-10 Batch 2:  Loss:     0.8884 Validation Accuracy: 0.700000\n",
      "Epoch 33, CIFAR-10 Batch 3:  Loss:     0.8364 Validation Accuracy: 0.800000\n",
      "Epoch 33, CIFAR-10 Batch 4:  Loss:     1.3102 Validation Accuracy: 0.600000\n",
      "Epoch 33, CIFAR-10 Batch 5:  Loss:     1.1334 Validation Accuracy: 0.700000\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss:     1.2890 Validation Accuracy: 0.550000\n",
      "Epoch 34, CIFAR-10 Batch 2:  Loss:     0.8801 Validation Accuracy: 0.700000\n",
      "Epoch 34, CIFAR-10 Batch 3:  Loss:     0.8288 Validation Accuracy: 0.800000\n",
      "Epoch 34, CIFAR-10 Batch 4:  Loss:     1.2992 Validation Accuracy: 0.600000\n",
      "Epoch 34, CIFAR-10 Batch 5:  Loss:     1.1293 Validation Accuracy: 0.700000\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss:     1.2813 Validation Accuracy: 0.550000\n",
      "Epoch 35, CIFAR-10 Batch 2:  Loss:     0.8723 Validation Accuracy: 0.700000\n",
      "Epoch 35, CIFAR-10 Batch 3:  Loss:     0.8233 Validation Accuracy: 0.800000\n",
      "Epoch 35, CIFAR-10 Batch 4:  Loss:     1.2749 Validation Accuracy: 0.600000\n",
      "Epoch 35, CIFAR-10 Batch 5:  Loss:     1.1258 Validation Accuracy: 0.700000\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss:     1.2694 Validation Accuracy: 0.550000\n",
      "Epoch 36, CIFAR-10 Batch 2:  Loss:     0.8659 Validation Accuracy: 0.700000\n",
      "Epoch 36, CIFAR-10 Batch 3:  Loss:     0.8146 Validation Accuracy: 0.800000\n",
      "Epoch 36, CIFAR-10 Batch 4:  Loss:     1.2545 Validation Accuracy: 0.600000\n",
      "Epoch 36, CIFAR-10 Batch 5:  Loss:     1.1212 Validation Accuracy: 0.700000\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss:     1.2646 Validation Accuracy: 0.550000\n",
      "Epoch 37, CIFAR-10 Batch 2:  Loss:     0.8619 Validation Accuracy: 0.700000\n",
      "Epoch 37, CIFAR-10 Batch 3:  Loss:     0.8085 Validation Accuracy: 0.800000\n",
      "Epoch 37, CIFAR-10 Batch 4:  Loss:     1.2357 Validation Accuracy: 0.600000\n",
      "Epoch 37, CIFAR-10 Batch 5:  Loss:     1.1169 Validation Accuracy: 0.725000\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss:     1.2623 Validation Accuracy: 0.550000\n",
      "Epoch 38, CIFAR-10 Batch 2:  Loss:     0.8589 Validation Accuracy: 0.725000\n",
      "Epoch 38, CIFAR-10 Batch 3:  Loss:     0.8038 Validation Accuracy: 0.800000\n",
      "Epoch 38, CIFAR-10 Batch 4:  Loss:     1.2247 Validation Accuracy: 0.600000\n",
      "Epoch 38, CIFAR-10 Batch 5:  Loss:     1.1131 Validation Accuracy: 0.725000\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss:     1.2604 Validation Accuracy: 0.550000\n",
      "Epoch 39, CIFAR-10 Batch 2:  Loss:     0.8537 Validation Accuracy: 0.700000\n",
      "Epoch 39, CIFAR-10 Batch 3:  Loss:     0.7930 Validation Accuracy: 0.800000\n",
      "Epoch 39, CIFAR-10 Batch 4:  Loss:     1.2195 Validation Accuracy: 0.625000\n",
      "Epoch 39, CIFAR-10 Batch 5:  Loss:     1.1137 Validation Accuracy: 0.725000\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss:     1.2545 Validation Accuracy: 0.550000\n",
      "Epoch 40, CIFAR-10 Batch 2:  Loss:     0.8508 Validation Accuracy: 0.700000\n",
      "Epoch 40, CIFAR-10 Batch 3:  Loss:     0.7879 Validation Accuracy: 0.800000\n",
      "Epoch 40, CIFAR-10 Batch 4:  Loss:     1.2121 Validation Accuracy: 0.650000\n",
      "Epoch 40, CIFAR-10 Batch 5:  Loss:     1.1084 Validation Accuracy: 0.725000\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss:     1.2503 Validation Accuracy: 0.550000\n",
      "Epoch 41, CIFAR-10 Batch 2:  Loss:     0.8434 Validation Accuracy: 0.725000\n",
      "Epoch 41, CIFAR-10 Batch 3:  Loss:     0.7840 Validation Accuracy: 0.800000\n",
      "Epoch 41, CIFAR-10 Batch 4:  Loss:     1.2053 Validation Accuracy: 0.625000\n",
      "Epoch 41, CIFAR-10 Batch 5:  Loss:     1.1035 Validation Accuracy: 0.725000\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss:     1.2466 Validation Accuracy: 0.550000\n",
      "Epoch 42, CIFAR-10 Batch 2:  Loss:     0.8399 Validation Accuracy: 0.725000\n",
      "Epoch 42, CIFAR-10 Batch 3:  Loss:     0.7805 Validation Accuracy: 0.800000\n",
      "Epoch 42, CIFAR-10 Batch 4:  Loss:     1.2003 Validation Accuracy: 0.625000\n",
      "Epoch 42, CIFAR-10 Batch 5:  Loss:     1.1027 Validation Accuracy: 0.725000\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss:     1.2436 Validation Accuracy: 0.550000\n",
      "Epoch 43, CIFAR-10 Batch 2:  Loss:     0.8371 Validation Accuracy: 0.725000\n",
      "Epoch 43, CIFAR-10 Batch 3:  Loss:     0.7769 Validation Accuracy: 0.800000\n",
      "Epoch 43, CIFAR-10 Batch 4:  Loss:     1.1977 Validation Accuracy: 0.625000\n",
      "Epoch 43, CIFAR-10 Batch 5:  Loss:     1.1022 Validation Accuracy: 0.725000\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss:     1.2428 Validation Accuracy: 0.550000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44, CIFAR-10 Batch 2:  Loss:     0.8321 Validation Accuracy: 0.725000\n",
      "Epoch 44, CIFAR-10 Batch 3:  Loss:     0.7719 Validation Accuracy: 0.800000\n",
      "Epoch 44, CIFAR-10 Batch 4:  Loss:     1.1917 Validation Accuracy: 0.625000\n",
      "Epoch 44, CIFAR-10 Batch 5:  Loss:     1.0998 Validation Accuracy: 0.725000\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss:     1.2321 Validation Accuracy: 0.550000\n",
      "Epoch 45, CIFAR-10 Batch 2:  Loss:     0.8262 Validation Accuracy: 0.725000\n",
      "Epoch 45, CIFAR-10 Batch 3:  Loss:     0.7701 Validation Accuracy: 0.800000\n",
      "Epoch 45, CIFAR-10 Batch 4:  Loss:     1.1869 Validation Accuracy: 0.600000\n",
      "Epoch 45, CIFAR-10 Batch 5:  Loss:     1.0997 Validation Accuracy: 0.725000\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss:     1.2290 Validation Accuracy: 0.550000\n",
      "Epoch 46, CIFAR-10 Batch 2:  Loss:     0.8218 Validation Accuracy: 0.725000\n",
      "Epoch 46, CIFAR-10 Batch 3:  Loss:     0.7645 Validation Accuracy: 0.800000\n",
      "Epoch 46, CIFAR-10 Batch 4:  Loss:     1.1865 Validation Accuracy: 0.600000\n",
      "Epoch 46, CIFAR-10 Batch 5:  Loss:     1.0992 Validation Accuracy: 0.725000\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss:     1.2274 Validation Accuracy: 0.550000\n",
      "Epoch 47, CIFAR-10 Batch 2:  Loss:     0.8196 Validation Accuracy: 0.725000\n",
      "Epoch 47, CIFAR-10 Batch 3:  Loss:     0.7598 Validation Accuracy: 0.800000\n",
      "Epoch 47, CIFAR-10 Batch 4:  Loss:     1.1826 Validation Accuracy: 0.600000\n",
      "Epoch 47, CIFAR-10 Batch 5:  Loss:     1.0948 Validation Accuracy: 0.725000\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss:     1.2237 Validation Accuracy: 0.550000\n",
      "Epoch 48, CIFAR-10 Batch 2:  Loss:     0.8157 Validation Accuracy: 0.725000\n",
      "Epoch 48, CIFAR-10 Batch 3:  Loss:     0.7599 Validation Accuracy: 0.800000\n",
      "Epoch 48, CIFAR-10 Batch 4:  Loss:     1.1810 Validation Accuracy: 0.600000\n",
      "Epoch 48, CIFAR-10 Batch 5:  Loss:     1.0933 Validation Accuracy: 0.725000\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss:     1.2168 Validation Accuracy: 0.550000\n",
      "Epoch 49, CIFAR-10 Batch 2:  Loss:     0.8092 Validation Accuracy: 0.750000\n",
      "Epoch 49, CIFAR-10 Batch 3:  Loss:     0.7566 Validation Accuracy: 0.800000\n",
      "Epoch 49, CIFAR-10 Batch 4:  Loss:     1.1791 Validation Accuracy: 0.625000\n",
      "Epoch 49, CIFAR-10 Batch 5:  Loss:     1.0950 Validation Accuracy: 0.725000\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss:     1.2188 Validation Accuracy: 0.550000\n",
      "Epoch 50, CIFAR-10 Batch 2:  Loss:     0.8043 Validation Accuracy: 0.750000\n",
      "Epoch 50, CIFAR-10 Batch 3:  Loss:     0.7569 Validation Accuracy: 0.800000\n",
      "Epoch 50, CIFAR-10 Batch 4:  Loss:     1.1731 Validation Accuracy: 0.625000\n",
      "Epoch 50, CIFAR-10 Batch 5:  Loss:     1.0933 Validation Accuracy: 0.725000\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss:     1.2180 Validation Accuracy: 0.550000\n",
      "Epoch 51, CIFAR-10 Batch 2:  Loss:     0.8041 Validation Accuracy: 0.750000\n",
      "Epoch 51, CIFAR-10 Batch 3:  Loss:     0.7537 Validation Accuracy: 0.800000\n",
      "Epoch 51, CIFAR-10 Batch 4:  Loss:     1.1710 Validation Accuracy: 0.625000\n",
      "Epoch 51, CIFAR-10 Batch 5:  Loss:     1.0898 Validation Accuracy: 0.725000\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss:     1.2167 Validation Accuracy: 0.550000\n",
      "Epoch 52, CIFAR-10 Batch 2:  Loss:     0.8039 Validation Accuracy: 0.750000\n",
      "Epoch 52, CIFAR-10 Batch 3:  Loss:     0.7516 Validation Accuracy: 0.800000\n",
      "Epoch 52, CIFAR-10 Batch 4:  Loss:     1.1698 Validation Accuracy: 0.625000\n",
      "Epoch 52, CIFAR-10 Batch 5:  Loss:     1.0913 Validation Accuracy: 0.725000\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss:     1.2132 Validation Accuracy: 0.550000\n",
      "Epoch 53, CIFAR-10 Batch 2:  Loss:     0.7968 Validation Accuracy: 0.750000\n",
      "Epoch 53, CIFAR-10 Batch 3:  Loss:     0.7503 Validation Accuracy: 0.800000\n",
      "Epoch 53, CIFAR-10 Batch 4:  Loss:     1.1670 Validation Accuracy: 0.625000\n",
      "Epoch 53, CIFAR-10 Batch 5:  Loss:     1.0887 Validation Accuracy: 0.725000\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss:     1.2124 Validation Accuracy: 0.575000\n",
      "Epoch 54, CIFAR-10 Batch 2:  Loss:     0.7912 Validation Accuracy: 0.750000\n",
      "Epoch 54, CIFAR-10 Batch 3:  Loss:     0.7461 Validation Accuracy: 0.800000\n",
      "Epoch 54, CIFAR-10 Batch 4:  Loss:     1.1648 Validation Accuracy: 0.625000\n",
      "Epoch 54, CIFAR-10 Batch 5:  Loss:     1.0884 Validation Accuracy: 0.725000\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss:     1.2042 Validation Accuracy: 0.575000\n",
      "Epoch 55, CIFAR-10 Batch 2:  Loss:     0.7892 Validation Accuracy: 0.750000\n",
      "Epoch 55, CIFAR-10 Batch 3:  Loss:     0.7455 Validation Accuracy: 0.800000\n",
      "Epoch 55, CIFAR-10 Batch 4:  Loss:     1.1652 Validation Accuracy: 0.625000\n",
      "Epoch 55, CIFAR-10 Batch 5:  Loss:     1.0869 Validation Accuracy: 0.725000\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss:     1.2025 Validation Accuracy: 0.575000\n",
      "Epoch 56, CIFAR-10 Batch 2:  Loss:     0.7852 Validation Accuracy: 0.750000\n",
      "Epoch 56, CIFAR-10 Batch 3:  Loss:     0.7449 Validation Accuracy: 0.800000\n",
      "Epoch 56, CIFAR-10 Batch 4:  Loss:     1.1672 Validation Accuracy: 0.600000\n",
      "Epoch 56, CIFAR-10 Batch 5:  Loss:     1.0828 Validation Accuracy: 0.725000\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss:     1.1973 Validation Accuracy: 0.625000\n",
      "Epoch 57, CIFAR-10 Batch 2:  Loss:     0.7859 Validation Accuracy: 0.750000\n",
      "Epoch 57, CIFAR-10 Batch 3:  Loss:     0.7417 Validation Accuracy: 0.800000\n",
      "Epoch 57, CIFAR-10 Batch 4:  Loss:     1.1646 Validation Accuracy: 0.625000\n",
      "Epoch 57, CIFAR-10 Batch 5:  Loss:     1.0835 Validation Accuracy: 0.725000\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss:     1.1943 Validation Accuracy: 0.625000\n",
      "Epoch 58, CIFAR-10 Batch 2:  Loss:     0.7835 Validation Accuracy: 0.750000\n",
      "Epoch 58, CIFAR-10 Batch 3:  Loss:     0.7353 Validation Accuracy: 0.800000\n",
      "Epoch 58, CIFAR-10 Batch 4:  Loss:     1.1657 Validation Accuracy: 0.625000\n",
      "Epoch 58, CIFAR-10 Batch 5:  Loss:     1.0814 Validation Accuracy: 0.725000\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss:     1.1948 Validation Accuracy: 0.625000\n",
      "Epoch 59, CIFAR-10 Batch 2:  Loss:     0.7813 Validation Accuracy: 0.750000\n",
      "Epoch 59, CIFAR-10 Batch 3:  Loss:     0.7376 Validation Accuracy: 0.825000\n",
      "Epoch 59, CIFAR-10 Batch 4:  Loss:     1.1652 Validation Accuracy: 0.625000\n",
      "Epoch 59, CIFAR-10 Batch 5:  Loss:     1.0803 Validation Accuracy: 0.725000\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss:     1.1867 Validation Accuracy: 0.625000\n",
      "Epoch 60, CIFAR-10 Batch 2:  Loss:     0.7798 Validation Accuracy: 0.750000\n",
      "Epoch 60, CIFAR-10 Batch 3:  Loss:     0.7347 Validation Accuracy: 0.800000\n",
      "Epoch 60, CIFAR-10 Batch 4:  Loss:     1.1617 Validation Accuracy: 0.625000\n",
      "Epoch 60, CIFAR-10 Batch 5:  Loss:     1.0802 Validation Accuracy: 0.725000\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss:     1.1839 Validation Accuracy: 0.625000\n",
      "Epoch 61, CIFAR-10 Batch 2:  Loss:     0.7798 Validation Accuracy: 0.750000\n",
      "Epoch 61, CIFAR-10 Batch 3:  Loss:     0.7314 Validation Accuracy: 0.825000\n",
      "Epoch 61, CIFAR-10 Batch 4:  Loss:     1.1646 Validation Accuracy: 0.625000\n",
      "Epoch 61, CIFAR-10 Batch 5:  Loss:     1.0731 Validation Accuracy: 0.725000\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss:     1.1785 Validation Accuracy: 0.650000\n",
      "Epoch 62, CIFAR-10 Batch 2:  Loss:     0.7764 Validation Accuracy: 0.750000\n",
      "Epoch 62, CIFAR-10 Batch 3:  Loss:     0.7291 Validation Accuracy: 0.825000\n",
      "Epoch 62, CIFAR-10 Batch 4:  Loss:     1.1597 Validation Accuracy: 0.625000\n",
      "Epoch 62, CIFAR-10 Batch 5:  Loss:     1.0698 Validation Accuracy: 0.725000\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss:     1.1759 Validation Accuracy: 0.650000\n",
      "Epoch 63, CIFAR-10 Batch 2:  Loss:     0.7730 Validation Accuracy: 0.750000\n",
      "Epoch 63, CIFAR-10 Batch 3:  Loss:     0.7259 Validation Accuracy: 0.825000\n",
      "Epoch 63, CIFAR-10 Batch 4:  Loss:     1.1572 Validation Accuracy: 0.625000\n",
      "Epoch 63, CIFAR-10 Batch 5:  Loss:     1.0647 Validation Accuracy: 0.725000\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss:     1.1761 Validation Accuracy: 0.650000\n",
      "Epoch 64, CIFAR-10 Batch 2:  Loss:     0.7684 Validation Accuracy: 0.750000\n",
      "Epoch 64, CIFAR-10 Batch 3:  Loss:     0.7236 Validation Accuracy: 0.775000\n",
      "Epoch 64, CIFAR-10 Batch 4:  Loss:     1.1565 Validation Accuracy: 0.625000\n",
      "Epoch 64, CIFAR-10 Batch 5:  Loss:     1.0598 Validation Accuracy: 0.725000\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss:     1.1709 Validation Accuracy: 0.650000\n",
      "Epoch 65, CIFAR-10 Batch 2:  Loss:     0.7651 Validation Accuracy: 0.750000\n",
      "Epoch 65, CIFAR-10 Batch 3:  Loss:     0.7140 Validation Accuracy: 0.800000\n",
      "Epoch 65, CIFAR-10 Batch 4:  Loss:     1.1569 Validation Accuracy: 0.625000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65, CIFAR-10 Batch 5:  Loss:     1.0564 Validation Accuracy: 0.750000\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss:     1.1706 Validation Accuracy: 0.650000\n",
      "Epoch 66, CIFAR-10 Batch 2:  Loss:     0.7616 Validation Accuracy: 0.750000\n",
      "Epoch 66, CIFAR-10 Batch 3:  Loss:     0.7125 Validation Accuracy: 0.800000\n",
      "Epoch 66, CIFAR-10 Batch 4:  Loss:     1.1577 Validation Accuracy: 0.650000\n",
      "Epoch 66, CIFAR-10 Batch 5:  Loss:     1.0498 Validation Accuracy: 0.750000\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss:     1.1687 Validation Accuracy: 0.650000\n",
      "Epoch 67, CIFAR-10 Batch 2:  Loss:     0.7613 Validation Accuracy: 0.725000\n",
      "Epoch 67, CIFAR-10 Batch 3:  Loss:     0.7077 Validation Accuracy: 0.825000\n",
      "Epoch 67, CIFAR-10 Batch 4:  Loss:     1.1549 Validation Accuracy: 0.625000\n",
      "Epoch 67, CIFAR-10 Batch 5:  Loss:     1.0464 Validation Accuracy: 0.750000\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss:     1.1705 Validation Accuracy: 0.650000\n",
      "Epoch 68, CIFAR-10 Batch 2:  Loss:     0.7550 Validation Accuracy: 0.725000\n",
      "Epoch 68, CIFAR-10 Batch 3:  Loss:     0.7076 Validation Accuracy: 0.825000\n",
      "Epoch 68, CIFAR-10 Batch 4:  Loss:     1.1550 Validation Accuracy: 0.650000\n",
      "Epoch 68, CIFAR-10 Batch 5:  Loss:     1.0405 Validation Accuracy: 0.750000\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss:     1.1670 Validation Accuracy: 0.650000\n",
      "Epoch 69, CIFAR-10 Batch 2:  Loss:     0.7451 Validation Accuracy: 0.750000\n",
      "Epoch 69, CIFAR-10 Batch 3:  Loss:     0.7051 Validation Accuracy: 0.800000\n",
      "Epoch 69, CIFAR-10 Batch 4:  Loss:     1.1558 Validation Accuracy: 0.650000\n",
      "Epoch 69, CIFAR-10 Batch 5:  Loss:     1.0385 Validation Accuracy: 0.750000\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss:     1.1684 Validation Accuracy: 0.650000\n",
      "Epoch 70, CIFAR-10 Batch 2:  Loss:     0.7394 Validation Accuracy: 0.750000\n",
      "Epoch 70, CIFAR-10 Batch 3:  Loss:     0.7001 Validation Accuracy: 0.800000\n",
      "Epoch 70, CIFAR-10 Batch 4:  Loss:     1.1553 Validation Accuracy: 0.675000\n",
      "Epoch 70, CIFAR-10 Batch 5:  Loss:     1.0320 Validation Accuracy: 0.750000\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss:     1.1651 Validation Accuracy: 0.650000\n",
      "Epoch 71, CIFAR-10 Batch 2:  Loss:     0.7383 Validation Accuracy: 0.750000\n",
      "Epoch 71, CIFAR-10 Batch 3:  Loss:     0.7004 Validation Accuracy: 0.800000\n",
      "Epoch 71, CIFAR-10 Batch 4:  Loss:     1.1535 Validation Accuracy: 0.650000\n",
      "Epoch 71, CIFAR-10 Batch 5:  Loss:     1.0308 Validation Accuracy: 0.750000\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss:     1.1589 Validation Accuracy: 0.650000\n",
      "Epoch 72, CIFAR-10 Batch 2:  Loss:     0.7300 Validation Accuracy: 0.750000\n",
      "Epoch 72, CIFAR-10 Batch 3:  Loss:     0.6917 Validation Accuracy: 0.825000\n",
      "Epoch 72, CIFAR-10 Batch 4:  Loss:     1.1538 Validation Accuracy: 0.650000\n",
      "Epoch 72, CIFAR-10 Batch 5:  Loss:     1.0244 Validation Accuracy: 0.750000\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss:     1.1560 Validation Accuracy: 0.625000\n",
      "Epoch 73, CIFAR-10 Batch 2:  Loss:     0.7290 Validation Accuracy: 0.750000\n",
      "Epoch 73, CIFAR-10 Batch 3:  Loss:     0.6919 Validation Accuracy: 0.825000\n",
      "Epoch 73, CIFAR-10 Batch 4:  Loss:     1.1510 Validation Accuracy: 0.650000\n",
      "Epoch 73, CIFAR-10 Batch 5:  Loss:     1.0235 Validation Accuracy: 0.750000\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss:     1.1547 Validation Accuracy: 0.625000\n",
      "Epoch 74, CIFAR-10 Batch 2:  Loss:     0.7257 Validation Accuracy: 0.750000\n",
      "Epoch 74, CIFAR-10 Batch 3:  Loss:     0.6937 Validation Accuracy: 0.825000\n",
      "Epoch 74, CIFAR-10 Batch 4:  Loss:     1.1507 Validation Accuracy: 0.650000\n",
      "Epoch 74, CIFAR-10 Batch 5:  Loss:     1.0219 Validation Accuracy: 0.750000\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss:     1.1520 Validation Accuracy: 0.625000\n",
      "Epoch 75, CIFAR-10 Batch 2:  Loss:     0.7216 Validation Accuracy: 0.750000\n",
      "Epoch 75, CIFAR-10 Batch 3:  Loss:     0.6867 Validation Accuracy: 0.800000\n",
      "Epoch 75, CIFAR-10 Batch 4:  Loss:     1.1495 Validation Accuracy: 0.650000\n",
      "Epoch 75, CIFAR-10 Batch 5:  Loss:     1.0184 Validation Accuracy: 0.750000\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss:     1.1460 Validation Accuracy: 0.625000\n",
      "Epoch 76, CIFAR-10 Batch 2:  Loss:     0.7204 Validation Accuracy: 0.750000\n",
      "Epoch 76, CIFAR-10 Batch 3:  Loss:     0.6825 Validation Accuracy: 0.825000\n",
      "Epoch 76, CIFAR-10 Batch 4:  Loss:     1.1487 Validation Accuracy: 0.650000\n",
      "Epoch 76, CIFAR-10 Batch 5:  Loss:     1.0153 Validation Accuracy: 0.750000\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss:     1.1452 Validation Accuracy: 0.650000\n",
      "Epoch 77, CIFAR-10 Batch 2:  Loss:     0.7164 Validation Accuracy: 0.775000\n",
      "Epoch 77, CIFAR-10 Batch 3:  Loss:     0.6808 Validation Accuracy: 0.825000\n",
      "Epoch 77, CIFAR-10 Batch 4:  Loss:     1.1503 Validation Accuracy: 0.650000\n",
      "Epoch 77, CIFAR-10 Batch 5:  Loss:     1.0126 Validation Accuracy: 0.750000\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss:     1.1433 Validation Accuracy: 0.650000\n",
      "Epoch 78, CIFAR-10 Batch 2:  Loss:     0.7141 Validation Accuracy: 0.775000\n",
      "Epoch 78, CIFAR-10 Batch 3:  Loss:     0.6824 Validation Accuracy: 0.825000\n",
      "Epoch 78, CIFAR-10 Batch 4:  Loss:     1.1436 Validation Accuracy: 0.650000\n",
      "Epoch 78, CIFAR-10 Batch 5:  Loss:     1.0133 Validation Accuracy: 0.750000\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss:     1.1420 Validation Accuracy: 0.650000\n",
      "Epoch 79, CIFAR-10 Batch 2:  Loss:     0.7149 Validation Accuracy: 0.725000\n",
      "Epoch 79, CIFAR-10 Batch 3:  Loss:     0.6783 Validation Accuracy: 0.850000\n",
      "Epoch 79, CIFAR-10 Batch 4:  Loss:     1.1406 Validation Accuracy: 0.650000\n",
      "Epoch 79, CIFAR-10 Batch 5:  Loss:     1.0089 Validation Accuracy: 0.750000\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss:     1.1412 Validation Accuracy: 0.650000\n",
      "Epoch 80, CIFAR-10 Batch 2:  Loss:     0.7154 Validation Accuracy: 0.725000\n",
      "Epoch 80, CIFAR-10 Batch 3:  Loss:     0.6781 Validation Accuracy: 0.800000\n",
      "Epoch 80, CIFAR-10 Batch 4:  Loss:     1.1393 Validation Accuracy: 0.650000\n",
      "Epoch 80, CIFAR-10 Batch 5:  Loss:     1.0098 Validation Accuracy: 0.750000\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss:     1.1399 Validation Accuracy: 0.650000\n",
      "Epoch 81, CIFAR-10 Batch 2:  Loss:     0.7181 Validation Accuracy: 0.725000\n",
      "Epoch 81, CIFAR-10 Batch 3:  Loss:     0.6748 Validation Accuracy: 0.825000\n",
      "Epoch 81, CIFAR-10 Batch 4:  Loss:     1.1352 Validation Accuracy: 0.650000\n",
      "Epoch 81, CIFAR-10 Batch 5:  Loss:     1.0068 Validation Accuracy: 0.750000\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss:     1.1359 Validation Accuracy: 0.650000\n",
      "Epoch 82, CIFAR-10 Batch 2:  Loss:     0.7188 Validation Accuracy: 0.725000\n",
      "Epoch 82, CIFAR-10 Batch 3:  Loss:     0.6732 Validation Accuracy: 0.825000\n",
      "Epoch 82, CIFAR-10 Batch 4:  Loss:     1.1351 Validation Accuracy: 0.650000\n",
      "Epoch 82, CIFAR-10 Batch 5:  Loss:     1.0017 Validation Accuracy: 0.750000\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss:     1.1344 Validation Accuracy: 0.650000\n",
      "Epoch 83, CIFAR-10 Batch 2:  Loss:     0.7187 Validation Accuracy: 0.725000\n",
      "Epoch 83, CIFAR-10 Batch 3:  Loss:     0.6708 Validation Accuracy: 0.825000\n",
      "Epoch 83, CIFAR-10 Batch 4:  Loss:     1.1326 Validation Accuracy: 0.650000\n",
      "Epoch 83, CIFAR-10 Batch 5:  Loss:     0.9991 Validation Accuracy: 0.750000\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss:     1.1320 Validation Accuracy: 0.650000\n",
      "Epoch 84, CIFAR-10 Batch 2:  Loss:     0.7188 Validation Accuracy: 0.725000\n",
      "Epoch 84, CIFAR-10 Batch 3:  Loss:     0.6656 Validation Accuracy: 0.825000\n",
      "Epoch 84, CIFAR-10 Batch 4:  Loss:     1.1316 Validation Accuracy: 0.675000\n",
      "Epoch 84, CIFAR-10 Batch 5:  Loss:     0.9967 Validation Accuracy: 0.750000\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss:     1.1333 Validation Accuracy: 0.650000\n",
      "Epoch 85, CIFAR-10 Batch 2:  Loss:     0.7164 Validation Accuracy: 0.725000\n",
      "Epoch 85, CIFAR-10 Batch 3:  Loss:     0.6655 Validation Accuracy: 0.825000\n",
      "Epoch 85, CIFAR-10 Batch 4:  Loss:     1.1274 Validation Accuracy: 0.675000\n",
      "Epoch 85, CIFAR-10 Batch 5:  Loss:     0.9947 Validation Accuracy: 0.750000\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss:     1.1302 Validation Accuracy: 0.650000\n",
      "Epoch 86, CIFAR-10 Batch 2:  Loss:     0.7159 Validation Accuracy: 0.725000\n",
      "Epoch 86, CIFAR-10 Batch 3:  Loss:     0.6610 Validation Accuracy: 0.825000\n",
      "Epoch 86, CIFAR-10 Batch 4:  Loss:     1.1263 Validation Accuracy: 0.700000\n",
      "Epoch 86, CIFAR-10 Batch 5:  Loss:     0.9955 Validation Accuracy: 0.750000\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss:     1.1240 Validation Accuracy: 0.650000\n",
      "Epoch 87, CIFAR-10 Batch 2:  Loss:     0.7144 Validation Accuracy: 0.725000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87, CIFAR-10 Batch 3:  Loss:     0.6594 Validation Accuracy: 0.825000\n",
      "Epoch 87, CIFAR-10 Batch 4:  Loss:     1.1247 Validation Accuracy: 0.675000\n",
      "Epoch 87, CIFAR-10 Batch 5:  Loss:     0.9925 Validation Accuracy: 0.775000\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss:     1.1299 Validation Accuracy: 0.650000\n",
      "Epoch 88, CIFAR-10 Batch 2:  Loss:     0.7152 Validation Accuracy: 0.775000\n",
      "Epoch 88, CIFAR-10 Batch 3:  Loss:     0.6606 Validation Accuracy: 0.850000\n",
      "Epoch 88, CIFAR-10 Batch 4:  Loss:     1.1203 Validation Accuracy: 0.700000\n",
      "Epoch 88, CIFAR-10 Batch 5:  Loss:     0.9911 Validation Accuracy: 0.750000\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss:     1.1238 Validation Accuracy: 0.650000\n",
      "Epoch 89, CIFAR-10 Batch 2:  Loss:     0.7100 Validation Accuracy: 0.775000\n",
      "Epoch 89, CIFAR-10 Batch 3:  Loss:     0.6596 Validation Accuracy: 0.825000\n",
      "Epoch 89, CIFAR-10 Batch 4:  Loss:     1.1185 Validation Accuracy: 0.675000\n",
      "Epoch 89, CIFAR-10 Batch 5:  Loss:     0.9899 Validation Accuracy: 0.775000\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss:     1.1289 Validation Accuracy: 0.650000\n",
      "Epoch 90, CIFAR-10 Batch 2:  Loss:     0.7091 Validation Accuracy: 0.775000\n",
      "Epoch 90, CIFAR-10 Batch 3:  Loss:     0.6564 Validation Accuracy: 0.825000\n",
      "Epoch 90, CIFAR-10 Batch 4:  Loss:     1.1162 Validation Accuracy: 0.700000\n",
      "Epoch 90, CIFAR-10 Batch 5:  Loss:     0.9874 Validation Accuracy: 0.775000\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss:     1.1236 Validation Accuracy: 0.650000\n",
      "Epoch 91, CIFAR-10 Batch 2:  Loss:     0.7093 Validation Accuracy: 0.775000\n",
      "Epoch 91, CIFAR-10 Batch 3:  Loss:     0.6549 Validation Accuracy: 0.825000\n",
      "Epoch 91, CIFAR-10 Batch 4:  Loss:     1.1159 Validation Accuracy: 0.700000\n",
      "Epoch 91, CIFAR-10 Batch 5:  Loss:     0.9863 Validation Accuracy: 0.775000\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss:     1.1255 Validation Accuracy: 0.650000\n",
      "Epoch 92, CIFAR-10 Batch 2:  Loss:     0.7090 Validation Accuracy: 0.775000\n",
      "Epoch 92, CIFAR-10 Batch 3:  Loss:     0.6529 Validation Accuracy: 0.825000\n",
      "Epoch 92, CIFAR-10 Batch 4:  Loss:     1.1139 Validation Accuracy: 0.675000\n",
      "Epoch 92, CIFAR-10 Batch 5:  Loss:     0.9811 Validation Accuracy: 0.775000\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss:     1.1176 Validation Accuracy: 0.650000\n",
      "Epoch 93, CIFAR-10 Batch 2:  Loss:     0.7079 Validation Accuracy: 0.775000\n",
      "Epoch 93, CIFAR-10 Batch 3:  Loss:     0.6531 Validation Accuracy: 0.825000\n",
      "Epoch 93, CIFAR-10 Batch 4:  Loss:     1.1091 Validation Accuracy: 0.700000\n",
      "Epoch 93, CIFAR-10 Batch 5:  Loss:     0.9832 Validation Accuracy: 0.775000\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss:     1.1137 Validation Accuracy: 0.650000\n",
      "Epoch 94, CIFAR-10 Batch 2:  Loss:     0.7069 Validation Accuracy: 0.775000\n",
      "Epoch 94, CIFAR-10 Batch 3:  Loss:     0.6508 Validation Accuracy: 0.825000\n",
      "Epoch 94, CIFAR-10 Batch 4:  Loss:     1.1087 Validation Accuracy: 0.675000\n",
      "Epoch 94, CIFAR-10 Batch 5:  Loss:     0.9786 Validation Accuracy: 0.775000\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss:     1.1133 Validation Accuracy: 0.650000\n",
      "Epoch 95, CIFAR-10 Batch 2:  Loss:     0.7021 Validation Accuracy: 0.775000\n",
      "Epoch 95, CIFAR-10 Batch 3:  Loss:     0.6502 Validation Accuracy: 0.825000\n",
      "Epoch 95, CIFAR-10 Batch 4:  Loss:     1.1100 Validation Accuracy: 0.675000\n",
      "Epoch 95, CIFAR-10 Batch 5:  Loss:     0.9789 Validation Accuracy: 0.775000\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss:     1.1128 Validation Accuracy: 0.675000\n",
      "Epoch 96, CIFAR-10 Batch 2:  Loss:     0.7025 Validation Accuracy: 0.775000\n",
      "Epoch 96, CIFAR-10 Batch 3:  Loss:     0.6447 Validation Accuracy: 0.825000\n",
      "Epoch 96, CIFAR-10 Batch 4:  Loss:     1.1067 Validation Accuracy: 0.650000\n",
      "Epoch 96, CIFAR-10 Batch 5:  Loss:     0.9797 Validation Accuracy: 0.775000\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss:     1.1091 Validation Accuracy: 0.650000\n",
      "Epoch 97, CIFAR-10 Batch 2:  Loss:     0.7004 Validation Accuracy: 0.775000\n",
      "Epoch 97, CIFAR-10 Batch 3:  Loss:     0.6439 Validation Accuracy: 0.825000\n",
      "Epoch 97, CIFAR-10 Batch 4:  Loss:     1.1044 Validation Accuracy: 0.675000\n",
      "Epoch 97, CIFAR-10 Batch 5:  Loss:     0.9746 Validation Accuracy: 0.775000\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss:     1.1106 Validation Accuracy: 0.650000\n",
      "Epoch 98, CIFAR-10 Batch 2:  Loss:     0.7008 Validation Accuracy: 0.775000\n",
      "Epoch 98, CIFAR-10 Batch 3:  Loss:     0.6420 Validation Accuracy: 0.850000\n",
      "Epoch 98, CIFAR-10 Batch 4:  Loss:     1.1050 Validation Accuracy: 0.675000\n",
      "Epoch 98, CIFAR-10 Batch 5:  Loss:     0.9792 Validation Accuracy: 0.775000\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss:     1.1053 Validation Accuracy: 0.650000\n",
      "Epoch 99, CIFAR-10 Batch 2:  Loss:     0.7017 Validation Accuracy: 0.775000\n",
      "Epoch 99, CIFAR-10 Batch 3:  Loss:     0.6421 Validation Accuracy: 0.850000\n",
      "Epoch 99, CIFAR-10 Batch 4:  Loss:     1.1010 Validation Accuracy: 0.650000\n",
      "Epoch 99, CIFAR-10 Batch 5:  Loss:     0.9765 Validation Accuracy: 0.775000\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss:     1.1079 Validation Accuracy: 0.675000\n",
      "Epoch 100, CIFAR-10 Batch 2:  Loss:     0.6984 Validation Accuracy: 0.775000\n",
      "Epoch 100, CIFAR-10 Batch 3:  Loss:     0.6427 Validation Accuracy: 0.825000\n",
      "Epoch 100, CIFAR-10 Batch 4:  Loss:     1.0988 Validation Accuracy: 0.650000\n",
      "Epoch 100, CIFAR-10 Batch 5:  Loss:     0.9768 Validation Accuracy: 0.775000\n",
      "Epoch 101, CIFAR-10 Batch 1:  Loss:     1.1086 Validation Accuracy: 0.675000\n",
      "Epoch 101, CIFAR-10 Batch 2:  Loss:     0.6954 Validation Accuracy: 0.775000\n",
      "Epoch 101, CIFAR-10 Batch 3:  Loss:     0.6374 Validation Accuracy: 0.850000\n",
      "Epoch 101, CIFAR-10 Batch 4:  Loss:     1.0954 Validation Accuracy: 0.650000\n",
      "Epoch 101, CIFAR-10 Batch 5:  Loss:     0.9752 Validation Accuracy: 0.775000\n",
      "Epoch 102, CIFAR-10 Batch 1:  Loss:     1.1051 Validation Accuracy: 0.675000\n",
      "Epoch 102, CIFAR-10 Batch 2:  Loss:     0.6928 Validation Accuracy: 0.775000\n",
      "Epoch 102, CIFAR-10 Batch 3:  Loss:     0.6355 Validation Accuracy: 0.825000\n",
      "Epoch 102, CIFAR-10 Batch 4:  Loss:     1.0917 Validation Accuracy: 0.650000\n",
      "Epoch 102, CIFAR-10 Batch 5:  Loss:     0.9750 Validation Accuracy: 0.775000\n",
      "Epoch 103, CIFAR-10 Batch 1:  Loss:     1.1031 Validation Accuracy: 0.675000\n",
      "Epoch 103, CIFAR-10 Batch 2:  Loss:     0.6915 Validation Accuracy: 0.775000\n",
      "Epoch 103, CIFAR-10 Batch 3:  Loss:     0.6363 Validation Accuracy: 0.850000\n",
      "Epoch 103, CIFAR-10 Batch 4:  Loss:     1.0916 Validation Accuracy: 0.650000\n",
      "Epoch 103, CIFAR-10 Batch 5:  Loss:     0.9752 Validation Accuracy: 0.775000\n",
      "Epoch 104, CIFAR-10 Batch 1:  Loss:     1.1014 Validation Accuracy: 0.700000\n",
      "Epoch 104, CIFAR-10 Batch 2:  Loss:     0.6884 Validation Accuracy: 0.775000\n",
      "Epoch 104, CIFAR-10 Batch 3:  Loss:     0.6379 Validation Accuracy: 0.850000\n",
      "Epoch 104, CIFAR-10 Batch 4:  Loss:     1.0859 Validation Accuracy: 0.650000\n",
      "Epoch 104, CIFAR-10 Batch 5:  Loss:     0.9745 Validation Accuracy: 0.775000\n",
      "Epoch 105, CIFAR-10 Batch 1:  Loss:     1.0969 Validation Accuracy: 0.675000\n",
      "Epoch 105, CIFAR-10 Batch 2:  Loss:     0.6904 Validation Accuracy: 0.775000\n",
      "Epoch 105, CIFAR-10 Batch 3:  Loss:     0.6382 Validation Accuracy: 0.850000\n",
      "Epoch 105, CIFAR-10 Batch 4:  Loss:     1.0889 Validation Accuracy: 0.650000\n",
      "Epoch 105, CIFAR-10 Batch 5:  Loss:     0.9750 Validation Accuracy: 0.775000\n",
      "Epoch 106, CIFAR-10 Batch 1:  Loss:     1.0959 Validation Accuracy: 0.675000\n",
      "Epoch 106, CIFAR-10 Batch 2:  Loss:     0.6891 Validation Accuracy: 0.775000\n",
      "Epoch 106, CIFAR-10 Batch 3:  Loss:     0.6408 Validation Accuracy: 0.825000\n",
      "Epoch 106, CIFAR-10 Batch 4:  Loss:     1.0881 Validation Accuracy: 0.650000\n",
      "Epoch 106, CIFAR-10 Batch 5:  Loss:     0.9789 Validation Accuracy: 0.775000\n",
      "Epoch 107, CIFAR-10 Batch 1:  Loss:     1.0971 Validation Accuracy: 0.725000\n",
      "Epoch 107, CIFAR-10 Batch 2:  Loss:     0.6902 Validation Accuracy: 0.775000\n",
      "Epoch 107, CIFAR-10 Batch 3:  Loss:     0.6373 Validation Accuracy: 0.850000\n",
      "Epoch 107, CIFAR-10 Batch 4:  Loss:     1.0853 Validation Accuracy: 0.650000\n",
      "Epoch 107, CIFAR-10 Batch 5:  Loss:     0.9763 Validation Accuracy: 0.775000\n",
      "Epoch 108, CIFAR-10 Batch 1:  Loss:     1.0952 Validation Accuracy: 0.675000\n",
      "Epoch 108, CIFAR-10 Batch 2:  Loss:     0.6879 Validation Accuracy: 0.775000\n",
      "Epoch 108, CIFAR-10 Batch 3:  Loss:     0.6359 Validation Accuracy: 0.875000\n",
      "Epoch 108, CIFAR-10 Batch 4:  Loss:     1.0833 Validation Accuracy: 0.650000\n",
      "Epoch 108, CIFAR-10 Batch 5:  Loss:     0.9754 Validation Accuracy: 0.775000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 109, CIFAR-10 Batch 1:  Loss:     1.0956 Validation Accuracy: 0.675000\n",
      "Epoch 109, CIFAR-10 Batch 2:  Loss:     0.6918 Validation Accuracy: 0.775000\n",
      "Epoch 109, CIFAR-10 Batch 3:  Loss:     0.6349 Validation Accuracy: 0.875000\n",
      "Epoch 109, CIFAR-10 Batch 4:  Loss:     1.0793 Validation Accuracy: 0.650000\n",
      "Epoch 109, CIFAR-10 Batch 5:  Loss:     0.9740 Validation Accuracy: 0.775000\n",
      "Epoch 110, CIFAR-10 Batch 1:  Loss:     1.0942 Validation Accuracy: 0.700000\n",
      "Epoch 110, CIFAR-10 Batch 2:  Loss:     0.6923 Validation Accuracy: 0.775000\n",
      "Epoch 110, CIFAR-10 Batch 3:  Loss:     0.6334 Validation Accuracy: 0.875000\n",
      "Epoch 110, CIFAR-10 Batch 4:  Loss:     1.0787 Validation Accuracy: 0.650000\n",
      "Epoch 110, CIFAR-10 Batch 5:  Loss:     0.9726 Validation Accuracy: 0.775000\n",
      "Epoch 111, CIFAR-10 Batch 1:  Loss:     1.0890 Validation Accuracy: 0.675000\n",
      "Epoch 111, CIFAR-10 Batch 2:  Loss:     0.6886 Validation Accuracy: 0.775000\n",
      "Epoch 111, CIFAR-10 Batch 3:  Loss:     0.6341 Validation Accuracy: 0.875000\n",
      "Epoch 111, CIFAR-10 Batch 4:  Loss:     1.0737 Validation Accuracy: 0.650000\n",
      "Epoch 111, CIFAR-10 Batch 5:  Loss:     0.9717 Validation Accuracy: 0.775000\n",
      "Epoch 112, CIFAR-10 Batch 1:  Loss:     1.0891 Validation Accuracy: 0.700000\n",
      "Epoch 112, CIFAR-10 Batch 2:  Loss:     0.6930 Validation Accuracy: 0.775000\n",
      "Epoch 112, CIFAR-10 Batch 3:  Loss:     0.6336 Validation Accuracy: 0.875000\n",
      "Epoch 112, CIFAR-10 Batch 4:  Loss:     1.0727 Validation Accuracy: 0.650000\n",
      "Epoch 112, CIFAR-10 Batch 5:  Loss:     0.9677 Validation Accuracy: 0.775000\n",
      "Epoch 113, CIFAR-10 Batch 1:  Loss:     1.0900 Validation Accuracy: 0.700000\n",
      "Epoch 113, CIFAR-10 Batch 2:  Loss:     0.6914 Validation Accuracy: 0.775000\n",
      "Epoch 113, CIFAR-10 Batch 3:  Loss:     0.6360 Validation Accuracy: 0.875000\n",
      "Epoch 113, CIFAR-10 Batch 4:  Loss:     1.0721 Validation Accuracy: 0.650000\n",
      "Epoch 113, CIFAR-10 Batch 5:  Loss:     0.9668 Validation Accuracy: 0.775000\n",
      "Epoch 114, CIFAR-10 Batch 1:  Loss:     1.0881 Validation Accuracy: 0.700000\n",
      "Epoch 114, CIFAR-10 Batch 2:  Loss:     0.6905 Validation Accuracy: 0.775000\n",
      "Epoch 114, CIFAR-10 Batch 3:  Loss:     0.6337 Validation Accuracy: 0.875000\n",
      "Epoch 114, CIFAR-10 Batch 4:  Loss:     1.0680 Validation Accuracy: 0.650000\n",
      "Epoch 114, CIFAR-10 Batch 5:  Loss:     0.9657 Validation Accuracy: 0.775000\n",
      "Epoch 115, CIFAR-10 Batch 1:  Loss:     1.0841 Validation Accuracy: 0.700000\n",
      "Epoch 115, CIFAR-10 Batch 2:  Loss:     0.6924 Validation Accuracy: 0.775000\n",
      "Epoch 115, CIFAR-10 Batch 3:  Loss:     0.6332 Validation Accuracy: 0.875000\n",
      "Epoch 115, CIFAR-10 Batch 4:  Loss:     1.0668 Validation Accuracy: 0.650000\n",
      "Epoch 115, CIFAR-10 Batch 5:  Loss:     0.9671 Validation Accuracy: 0.775000\n",
      "Epoch 116, CIFAR-10 Batch 1:  Loss:     1.0859 Validation Accuracy: 0.700000\n",
      "Epoch 116, CIFAR-10 Batch 2:  Loss:     0.6893 Validation Accuracy: 0.775000\n",
      "Epoch 116, CIFAR-10 Batch 3:  Loss:     0.6342 Validation Accuracy: 0.850000\n",
      "Epoch 116, CIFAR-10 Batch 4:  Loss:     1.0692 Validation Accuracy: 0.650000\n",
      "Epoch 116, CIFAR-10 Batch 5:  Loss:     0.9664 Validation Accuracy: 0.775000\n",
      "Epoch 117, CIFAR-10 Batch 1:  Loss:     1.0846 Validation Accuracy: 0.700000\n",
      "Epoch 117, CIFAR-10 Batch 2:  Loss:     0.6909 Validation Accuracy: 0.775000\n",
      "Epoch 117, CIFAR-10 Batch 3:  Loss:     0.6292 Validation Accuracy: 0.875000\n",
      "Epoch 117, CIFAR-10 Batch 4:  Loss:     1.0641 Validation Accuracy: 0.650000\n",
      "Epoch 117, CIFAR-10 Batch 5:  Loss:     0.9620 Validation Accuracy: 0.775000\n",
      "Epoch 118, CIFAR-10 Batch 1:  Loss:     1.0812 Validation Accuracy: 0.700000\n",
      "Epoch 118, CIFAR-10 Batch 2:  Loss:     0.6934 Validation Accuracy: 0.775000\n",
      "Epoch 118, CIFAR-10 Batch 3:  Loss:     0.6302 Validation Accuracy: 0.875000\n",
      "Epoch 118, CIFAR-10 Batch 4:  Loss:     1.0652 Validation Accuracy: 0.650000\n",
      "Epoch 118, CIFAR-10 Batch 5:  Loss:     0.9671 Validation Accuracy: 0.775000\n",
      "Epoch 119, CIFAR-10 Batch 1:  Loss:     1.0804 Validation Accuracy: 0.700000\n",
      "Epoch 119, CIFAR-10 Batch 2:  Loss:     0.6920 Validation Accuracy: 0.775000\n",
      "Epoch 119, CIFAR-10 Batch 3:  Loss:     0.6321 Validation Accuracy: 0.875000\n",
      "Epoch 119, CIFAR-10 Batch 4:  Loss:     1.0657 Validation Accuracy: 0.650000\n",
      "Epoch 119, CIFAR-10 Batch 5:  Loss:     0.9618 Validation Accuracy: 0.750000\n",
      "Epoch 120, CIFAR-10 Batch 1:  Loss:     1.0804 Validation Accuracy: 0.700000\n",
      "Epoch 120, CIFAR-10 Batch 2:  Loss:     0.6931 Validation Accuracy: 0.775000\n",
      "Epoch 120, CIFAR-10 Batch 3:  Loss:     0.6272 Validation Accuracy: 0.900000\n",
      "Epoch 120, CIFAR-10 Batch 4:  Loss:     1.0658 Validation Accuracy: 0.650000\n",
      "Epoch 120, CIFAR-10 Batch 5:  Loss:     0.9566 Validation Accuracy: 0.775000\n",
      "Epoch 121, CIFAR-10 Batch 1:  Loss:     1.0814 Validation Accuracy: 0.725000\n",
      "Epoch 121, CIFAR-10 Batch 2:  Loss:     0.6900 Validation Accuracy: 0.775000\n",
      "Epoch 121, CIFAR-10 Batch 3:  Loss:     0.6270 Validation Accuracy: 0.900000\n",
      "Epoch 121, CIFAR-10 Batch 4:  Loss:     1.0664 Validation Accuracy: 0.650000\n",
      "Epoch 121, CIFAR-10 Batch 5:  Loss:     0.9583 Validation Accuracy: 0.775000\n",
      "Epoch 122, CIFAR-10 Batch 1:  Loss:     1.0758 Validation Accuracy: 0.725000\n",
      "Epoch 122, CIFAR-10 Batch 2:  Loss:     0.6923 Validation Accuracy: 0.775000\n",
      "Epoch 122, CIFAR-10 Batch 3:  Loss:     0.6238 Validation Accuracy: 0.875000\n",
      "Epoch 122, CIFAR-10 Batch 4:  Loss:     1.0658 Validation Accuracy: 0.650000\n",
      "Epoch 122, CIFAR-10 Batch 5:  Loss:     0.9582 Validation Accuracy: 0.750000\n",
      "Epoch 123, CIFAR-10 Batch 1:  Loss:     1.0755 Validation Accuracy: 0.725000\n",
      "Epoch 123, CIFAR-10 Batch 2:  Loss:     0.6898 Validation Accuracy: 0.775000\n",
      "Epoch 123, CIFAR-10 Batch 3:  Loss:     0.6191 Validation Accuracy: 0.900000\n",
      "Epoch 123, CIFAR-10 Batch 4:  Loss:     1.0681 Validation Accuracy: 0.650000\n",
      "Epoch 123, CIFAR-10 Batch 5:  Loss:     0.9572 Validation Accuracy: 0.775000\n",
      "Epoch 124, CIFAR-10 Batch 1:  Loss:     1.0740 Validation Accuracy: 0.700000\n",
      "Epoch 124, CIFAR-10 Batch 2:  Loss:     0.6903 Validation Accuracy: 0.775000\n",
      "Epoch 124, CIFAR-10 Batch 3:  Loss:     0.6197 Validation Accuracy: 0.875000\n",
      "Epoch 124, CIFAR-10 Batch 4:  Loss:     1.0645 Validation Accuracy: 0.700000\n",
      "Epoch 124, CIFAR-10 Batch 5:  Loss:     0.9570 Validation Accuracy: 0.775000\n",
      "Epoch 125, CIFAR-10 Batch 1:  Loss:     1.0733 Validation Accuracy: 0.725000\n",
      "Epoch 125, CIFAR-10 Batch 2:  Loss:     0.6897 Validation Accuracy: 0.775000\n",
      "Epoch 125, CIFAR-10 Batch 3:  Loss:     0.6208 Validation Accuracy: 0.875000\n",
      "Epoch 125, CIFAR-10 Batch 4:  Loss:     1.0609 Validation Accuracy: 0.675000\n",
      "Epoch 125, CIFAR-10 Batch 5:  Loss:     0.9621 Validation Accuracy: 0.775000\n",
      "Epoch 126, CIFAR-10 Batch 1:  Loss:     1.0690 Validation Accuracy: 0.725000\n",
      "Epoch 126, CIFAR-10 Batch 2:  Loss:     0.6861 Validation Accuracy: 0.775000\n",
      "Epoch 126, CIFAR-10 Batch 3:  Loss:     0.6187 Validation Accuracy: 0.875000\n",
      "Epoch 126, CIFAR-10 Batch 4:  Loss:     1.0634 Validation Accuracy: 0.675000\n",
      "Epoch 126, CIFAR-10 Batch 5:  Loss:     0.9570 Validation Accuracy: 0.775000\n",
      "Epoch 127, CIFAR-10 Batch 1:  Loss:     1.0655 Validation Accuracy: 0.700000\n",
      "Epoch 127, CIFAR-10 Batch 2:  Loss:     0.6857 Validation Accuracy: 0.750000\n",
      "Epoch 127, CIFAR-10 Batch 3:  Loss:     0.6202 Validation Accuracy: 0.850000\n",
      "Epoch 127, CIFAR-10 Batch 4:  Loss:     1.0606 Validation Accuracy: 0.675000\n",
      "Epoch 127, CIFAR-10 Batch 5:  Loss:     0.9591 Validation Accuracy: 0.775000\n",
      "Epoch 128, CIFAR-10 Batch 1:  Loss:     1.0646 Validation Accuracy: 0.700000\n",
      "Epoch 128, CIFAR-10 Batch 2:  Loss:     0.6838 Validation Accuracy: 0.750000\n",
      "Epoch 128, CIFAR-10 Batch 3:  Loss:     0.6188 Validation Accuracy: 0.850000\n",
      "Epoch 128, CIFAR-10 Batch 4:  Loss:     1.0612 Validation Accuracy: 0.700000\n",
      "Epoch 128, CIFAR-10 Batch 5:  Loss:     0.9559 Validation Accuracy: 0.775000\n",
      "Epoch 129, CIFAR-10 Batch 1:  Loss:     1.0658 Validation Accuracy: 0.700000\n",
      "Epoch 129, CIFAR-10 Batch 2:  Loss:     0.6828 Validation Accuracy: 0.775000\n",
      "Epoch 129, CIFAR-10 Batch 3:  Loss:     0.6125 Validation Accuracy: 0.900000\n",
      "Epoch 129, CIFAR-10 Batch 4:  Loss:     1.0642 Validation Accuracy: 0.650000\n",
      "Epoch 129, CIFAR-10 Batch 5:  Loss:     0.9547 Validation Accuracy: 0.775000\n",
      "Epoch 130, CIFAR-10 Batch 1:  Loss:     1.0614 Validation Accuracy: 0.700000\n",
      "Epoch 130, CIFAR-10 Batch 2:  Loss:     0.6802 Validation Accuracy: 0.775000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 130, CIFAR-10 Batch 3:  Loss:     0.6130 Validation Accuracy: 0.875000\n",
      "Epoch 130, CIFAR-10 Batch 4:  Loss:     1.0665 Validation Accuracy: 0.650000\n",
      "Epoch 130, CIFAR-10 Batch 5:  Loss:     0.9530 Validation Accuracy: 0.775000\n",
      "Epoch 131, CIFAR-10 Batch 1:  Loss:     1.0617 Validation Accuracy: 0.700000\n",
      "Epoch 131, CIFAR-10 Batch 2:  Loss:     0.6797 Validation Accuracy: 0.775000\n",
      "Epoch 131, CIFAR-10 Batch 3:  Loss:     0.6099 Validation Accuracy: 0.900000\n",
      "Epoch 131, CIFAR-10 Batch 4:  Loss:     1.0653 Validation Accuracy: 0.675000\n",
      "Epoch 131, CIFAR-10 Batch 5:  Loss:     0.9545 Validation Accuracy: 0.775000\n",
      "Epoch 132, CIFAR-10 Batch 1:  Loss:     1.0639 Validation Accuracy: 0.700000\n",
      "Epoch 132, CIFAR-10 Batch 2:  Loss:     0.6855 Validation Accuracy: 0.775000\n",
      "Epoch 132, CIFAR-10 Batch 3:  Loss:     0.6093 Validation Accuracy: 0.900000\n",
      "Epoch 132, CIFAR-10 Batch 4:  Loss:     1.0641 Validation Accuracy: 0.675000\n",
      "Epoch 132, CIFAR-10 Batch 5:  Loss:     0.9554 Validation Accuracy: 0.775000\n",
      "Epoch 133, CIFAR-10 Batch 1:  Loss:     1.0607 Validation Accuracy: 0.700000\n",
      "Epoch 133, CIFAR-10 Batch 2:  Loss:     0.6865 Validation Accuracy: 0.775000\n",
      "Epoch 133, CIFAR-10 Batch 3:  Loss:     0.6114 Validation Accuracy: 0.900000\n",
      "Epoch 133, CIFAR-10 Batch 4:  Loss:     1.0620 Validation Accuracy: 0.675000\n",
      "Epoch 133, CIFAR-10 Batch 5:  Loss:     0.9564 Validation Accuracy: 0.775000\n",
      "Epoch 134, CIFAR-10 Batch 1:  Loss:     1.0599 Validation Accuracy: 0.725000\n",
      "Epoch 134, CIFAR-10 Batch 2:  Loss:     0.6897 Validation Accuracy: 0.775000\n",
      "Epoch 134, CIFAR-10 Batch 3:  Loss:     0.6076 Validation Accuracy: 0.900000\n",
      "Epoch 134, CIFAR-10 Batch 4:  Loss:     1.0612 Validation Accuracy: 0.675000\n",
      "Epoch 134, CIFAR-10 Batch 5:  Loss:     0.9556 Validation Accuracy: 0.775000\n",
      "Epoch 135, CIFAR-10 Batch 1:  Loss:     1.0563 Validation Accuracy: 0.725000\n",
      "Epoch 135, CIFAR-10 Batch 2:  Loss:     0.6837 Validation Accuracy: 0.800000\n",
      "Epoch 135, CIFAR-10 Batch 3:  Loss:     0.6125 Validation Accuracy: 0.900000\n",
      "Epoch 135, CIFAR-10 Batch 4:  Loss:     1.0622 Validation Accuracy: 0.675000\n",
      "Epoch 135, CIFAR-10 Batch 5:  Loss:     0.9548 Validation Accuracy: 0.775000\n",
      "Epoch 136, CIFAR-10 Batch 1:  Loss:     1.0583 Validation Accuracy: 0.700000\n",
      "Epoch 136, CIFAR-10 Batch 2:  Loss:     0.6881 Validation Accuracy: 0.775000\n",
      "Epoch 136, CIFAR-10 Batch 3:  Loss:     0.6052 Validation Accuracy: 0.875000\n",
      "Epoch 136, CIFAR-10 Batch 4:  Loss:     1.0619 Validation Accuracy: 0.675000\n",
      "Epoch 136, CIFAR-10 Batch 5:  Loss:     0.9587 Validation Accuracy: 0.775000\n",
      "Epoch 137, CIFAR-10 Batch 1:  Loss:     1.0549 Validation Accuracy: 0.725000\n",
      "Epoch 137, CIFAR-10 Batch 2:  Loss:     0.6889 Validation Accuracy: 0.800000\n",
      "Epoch 137, CIFAR-10 Batch 3:  Loss:     0.6053 Validation Accuracy: 0.875000\n",
      "Epoch 137, CIFAR-10 Batch 4:  Loss:     1.0680 Validation Accuracy: 0.675000\n",
      "Epoch 137, CIFAR-10 Batch 5:  Loss:     0.9585 Validation Accuracy: 0.775000\n",
      "Epoch 138, CIFAR-10 Batch 1:  Loss:     1.0523 Validation Accuracy: 0.725000\n",
      "Epoch 138, CIFAR-10 Batch 2:  Loss:     0.6906 Validation Accuracy: 0.800000\n",
      "Epoch 138, CIFAR-10 Batch 3:  Loss:     0.6040 Validation Accuracy: 0.900000\n",
      "Epoch 138, CIFAR-10 Batch 4:  Loss:     1.0753 Validation Accuracy: 0.675000\n",
      "Epoch 138, CIFAR-10 Batch 5:  Loss:     0.9582 Validation Accuracy: 0.775000\n",
      "Epoch 139, CIFAR-10 Batch 1:  Loss:     1.0529 Validation Accuracy: 0.700000\n",
      "Epoch 139, CIFAR-10 Batch 2:  Loss:     0.6907 Validation Accuracy: 0.800000\n",
      "Epoch 139, CIFAR-10 Batch 3:  Loss:     0.6061 Validation Accuracy: 0.875000\n",
      "Epoch 139, CIFAR-10 Batch 4:  Loss:     1.0721 Validation Accuracy: 0.675000\n",
      "Epoch 139, CIFAR-10 Batch 5:  Loss:     0.9570 Validation Accuracy: 0.750000\n",
      "Epoch 140, CIFAR-10 Batch 1:  Loss:     1.0474 Validation Accuracy: 0.700000\n",
      "Epoch 140, CIFAR-10 Batch 2:  Loss:     0.6955 Validation Accuracy: 0.800000\n",
      "Epoch 140, CIFAR-10 Batch 3:  Loss:     0.6036 Validation Accuracy: 0.875000\n",
      "Epoch 140, CIFAR-10 Batch 4:  Loss:     1.0735 Validation Accuracy: 0.675000\n",
      "Epoch 140, CIFAR-10 Batch 5:  Loss:     0.9573 Validation Accuracy: 0.775000\n",
      "Epoch 141, CIFAR-10 Batch 1:  Loss:     1.0517 Validation Accuracy: 0.725000\n",
      "Epoch 141, CIFAR-10 Batch 2:  Loss:     0.6899 Validation Accuracy: 0.800000\n",
      "Epoch 141, CIFAR-10 Batch 3:  Loss:     0.6023 Validation Accuracy: 0.900000\n",
      "Epoch 141, CIFAR-10 Batch 4:  Loss:     1.0705 Validation Accuracy: 0.675000\n",
      "Epoch 141, CIFAR-10 Batch 5:  Loss:     0.9601 Validation Accuracy: 0.775000\n",
      "Epoch 142, CIFAR-10 Batch 1:  Loss:     1.0482 Validation Accuracy: 0.700000\n",
      "Epoch 142, CIFAR-10 Batch 2:  Loss:     0.6924 Validation Accuracy: 0.775000\n",
      "Epoch 142, CIFAR-10 Batch 3:  Loss:     0.6004 Validation Accuracy: 0.875000\n",
      "Epoch 142, CIFAR-10 Batch 4:  Loss:     1.0718 Validation Accuracy: 0.675000\n",
      "Epoch 142, CIFAR-10 Batch 5:  Loss:     0.9596 Validation Accuracy: 0.775000\n",
      "Epoch 143, CIFAR-10 Batch 1:  Loss:     1.0454 Validation Accuracy: 0.725000\n",
      "Epoch 143, CIFAR-10 Batch 2:  Loss:     0.6902 Validation Accuracy: 0.800000\n",
      "Epoch 143, CIFAR-10 Batch 3:  Loss:     0.5987 Validation Accuracy: 0.900000\n",
      "Epoch 143, CIFAR-10 Batch 4:  Loss:     1.0731 Validation Accuracy: 0.675000\n",
      "Epoch 143, CIFAR-10 Batch 5:  Loss:     0.9587 Validation Accuracy: 0.775000\n",
      "Epoch 144, CIFAR-10 Batch 1:  Loss:     1.0460 Validation Accuracy: 0.725000\n",
      "Epoch 144, CIFAR-10 Batch 2:  Loss:     0.6904 Validation Accuracy: 0.800000\n",
      "Epoch 144, CIFAR-10 Batch 3:  Loss:     0.6000 Validation Accuracy: 0.900000\n",
      "Epoch 144, CIFAR-10 Batch 4:  Loss:     1.0694 Validation Accuracy: 0.675000\n",
      "Epoch 144, CIFAR-10 Batch 5:  Loss:     0.9610 Validation Accuracy: 0.775000\n",
      "Epoch 145, CIFAR-10 Batch 1:  Loss:     1.0452 Validation Accuracy: 0.725000\n",
      "Epoch 145, CIFAR-10 Batch 2:  Loss:     0.6918 Validation Accuracy: 0.800000\n",
      "Epoch 145, CIFAR-10 Batch 3:  Loss:     0.5963 Validation Accuracy: 0.900000\n",
      "Epoch 145, CIFAR-10 Batch 4:  Loss:     1.0741 Validation Accuracy: 0.675000\n",
      "Epoch 145, CIFAR-10 Batch 5:  Loss:     0.9613 Validation Accuracy: 0.775000\n",
      "Epoch 146, CIFAR-10 Batch 1:  Loss:     1.0459 Validation Accuracy: 0.725000\n",
      "Epoch 146, CIFAR-10 Batch 2:  Loss:     0.6934 Validation Accuracy: 0.800000\n",
      "Epoch 146, CIFAR-10 Batch 3:  Loss:     0.5934 Validation Accuracy: 0.900000\n",
      "Epoch 146, CIFAR-10 Batch 4:  Loss:     1.0733 Validation Accuracy: 0.675000\n",
      "Epoch 146, CIFAR-10 Batch 5:  Loss:     0.9585 Validation Accuracy: 0.750000\n",
      "Epoch 147, CIFAR-10 Batch 1:  Loss:     1.0450 Validation Accuracy: 0.725000\n",
      "Epoch 147, CIFAR-10 Batch 2:  Loss:     0.6900 Validation Accuracy: 0.800000\n",
      "Epoch 147, CIFAR-10 Batch 3:  Loss:     0.5909 Validation Accuracy: 0.900000\n",
      "Epoch 147, CIFAR-10 Batch 4:  Loss:     1.0729 Validation Accuracy: 0.675000\n",
      "Epoch 147, CIFAR-10 Batch 5:  Loss:     0.9578 Validation Accuracy: 0.775000\n",
      "Epoch 148, CIFAR-10 Batch 1:  Loss:     1.0447 Validation Accuracy: 0.725000\n",
      "Epoch 148, CIFAR-10 Batch 2:  Loss:     0.6868 Validation Accuracy: 0.800000\n",
      "Epoch 148, CIFAR-10 Batch 3:  Loss:     0.5894 Validation Accuracy: 0.900000\n",
      "Epoch 148, CIFAR-10 Batch 4:  Loss:     1.0668 Validation Accuracy: 0.675000\n",
      "Epoch 148, CIFAR-10 Batch 5:  Loss:     0.9604 Validation Accuracy: 0.775000\n",
      "Epoch 149, CIFAR-10 Batch 1:  Loss:     1.0451 Validation Accuracy: 0.725000\n",
      "Epoch 149, CIFAR-10 Batch 2:  Loss:     0.6905 Validation Accuracy: 0.775000\n",
      "Epoch 149, CIFAR-10 Batch 3:  Loss:     0.5866 Validation Accuracy: 0.900000\n",
      "Epoch 149, CIFAR-10 Batch 4:  Loss:     1.0619 Validation Accuracy: 0.675000\n",
      "Epoch 149, CIFAR-10 Batch 5:  Loss:     0.9598 Validation Accuracy: 0.775000\n",
      "Epoch 150, CIFAR-10 Batch 1:  Loss:     1.0455 Validation Accuracy: 0.725000\n",
      "Epoch 150, CIFAR-10 Batch 2:  Loss:     0.6816 Validation Accuracy: 0.800000\n",
      "Epoch 150, CIFAR-10 Batch 3:  Loss:     0.5865 Validation Accuracy: 0.900000\n",
      "Epoch 150, CIFAR-10 Batch 4:  Loss:     1.0626 Validation Accuracy: 0.675000\n",
      "Epoch 150, CIFAR-10 Batch 5:  Loss:     0.9600 Validation Accuracy: 0.750000\n",
      "Epoch 151, CIFAR-10 Batch 1:  Loss:     1.0431 Validation Accuracy: 0.725000\n",
      "Epoch 151, CIFAR-10 Batch 2:  Loss:     0.6797 Validation Accuracy: 0.800000\n",
      "Epoch 151, CIFAR-10 Batch 3:  Loss:     0.5836 Validation Accuracy: 0.900000\n",
      "Epoch 151, CIFAR-10 Batch 4:  Loss:     1.0579 Validation Accuracy: 0.675000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 151, CIFAR-10 Batch 5:  Loss:     0.9572 Validation Accuracy: 0.775000\n",
      "Epoch 152, CIFAR-10 Batch 1:  Loss:     1.0387 Validation Accuracy: 0.725000\n",
      "Epoch 152, CIFAR-10 Batch 2:  Loss:     0.6796 Validation Accuracy: 0.800000\n",
      "Epoch 152, CIFAR-10 Batch 3:  Loss:     0.5829 Validation Accuracy: 0.900000\n",
      "Epoch 152, CIFAR-10 Batch 4:  Loss:     1.0569 Validation Accuracy: 0.675000\n",
      "Epoch 152, CIFAR-10 Batch 5:  Loss:     0.9531 Validation Accuracy: 0.775000\n",
      "Epoch 153, CIFAR-10 Batch 1:  Loss:     1.0418 Validation Accuracy: 0.725000\n",
      "Epoch 153, CIFAR-10 Batch 2:  Loss:     0.6771 Validation Accuracy: 0.800000\n",
      "Epoch 153, CIFAR-10 Batch 3:  Loss:     0.5797 Validation Accuracy: 0.900000\n",
      "Epoch 153, CIFAR-10 Batch 4:  Loss:     1.0568 Validation Accuracy: 0.675000\n",
      "Epoch 153, CIFAR-10 Batch 5:  Loss:     0.9527 Validation Accuracy: 0.800000\n",
      "Epoch 154, CIFAR-10 Batch 1:  Loss:     1.0429 Validation Accuracy: 0.725000\n",
      "Epoch 154, CIFAR-10 Batch 2:  Loss:     0.6727 Validation Accuracy: 0.800000\n",
      "Epoch 154, CIFAR-10 Batch 3:  Loss:     0.5808 Validation Accuracy: 0.900000\n",
      "Epoch 154, CIFAR-10 Batch 4:  Loss:     1.0503 Validation Accuracy: 0.700000\n",
      "Epoch 154, CIFAR-10 Batch 5:  Loss:     0.9490 Validation Accuracy: 0.800000\n",
      "Epoch 155, CIFAR-10 Batch 1:  Loss:     1.0440 Validation Accuracy: 0.725000\n",
      "Epoch 155, CIFAR-10 Batch 2:  Loss:     0.6766 Validation Accuracy: 0.800000\n",
      "Epoch 155, CIFAR-10 Batch 3:  Loss:     0.5796 Validation Accuracy: 0.900000\n",
      "Epoch 155, CIFAR-10 Batch 4:  Loss:     1.0492 Validation Accuracy: 0.700000\n",
      "Epoch 155, CIFAR-10 Batch 5:  Loss:     0.9522 Validation Accuracy: 0.800000\n",
      "Epoch 156, CIFAR-10 Batch 1:  Loss:     1.0428 Validation Accuracy: 0.725000\n",
      "Epoch 156, CIFAR-10 Batch 2:  Loss:     0.6789 Validation Accuracy: 0.800000\n",
      "Epoch 156, CIFAR-10 Batch 3:  Loss:     0.5807 Validation Accuracy: 0.900000\n",
      "Epoch 156, CIFAR-10 Batch 4:  Loss:     1.0453 Validation Accuracy: 0.700000\n",
      "Epoch 156, CIFAR-10 Batch 5:  Loss:     0.9471 Validation Accuracy: 0.800000\n",
      "Epoch 157, CIFAR-10 Batch 1:  Loss:     1.0373 Validation Accuracy: 0.725000\n",
      "Epoch 157, CIFAR-10 Batch 2:  Loss:     0.6745 Validation Accuracy: 0.800000\n",
      "Epoch 157, CIFAR-10 Batch 3:  Loss:     0.5773 Validation Accuracy: 0.900000\n",
      "Epoch 157, CIFAR-10 Batch 4:  Loss:     1.0442 Validation Accuracy: 0.700000\n",
      "Epoch 157, CIFAR-10 Batch 5:  Loss:     0.9476 Validation Accuracy: 0.800000\n",
      "Epoch 158, CIFAR-10 Batch 1:  Loss:     1.0400 Validation Accuracy: 0.725000\n",
      "Epoch 158, CIFAR-10 Batch 2:  Loss:     0.6680 Validation Accuracy: 0.800000\n",
      "Epoch 158, CIFAR-10 Batch 3:  Loss:     0.5768 Validation Accuracy: 0.900000\n",
      "Epoch 158, CIFAR-10 Batch 4:  Loss:     1.0459 Validation Accuracy: 0.700000\n",
      "Epoch 158, CIFAR-10 Batch 5:  Loss:     0.9485 Validation Accuracy: 0.800000\n",
      "Epoch 159, CIFAR-10 Batch 1:  Loss:     1.0430 Validation Accuracy: 0.725000\n",
      "Epoch 159, CIFAR-10 Batch 2:  Loss:     0.6755 Validation Accuracy: 0.800000\n",
      "Epoch 159, CIFAR-10 Batch 3:  Loss:     0.5771 Validation Accuracy: 0.900000\n",
      "Epoch 159, CIFAR-10 Batch 4:  Loss:     1.0437 Validation Accuracy: 0.700000\n",
      "Epoch 159, CIFAR-10 Batch 5:  Loss:     0.9500 Validation Accuracy: 0.800000\n",
      "Epoch 160, CIFAR-10 Batch 1:  Loss:     1.0413 Validation Accuracy: 0.725000\n",
      "Epoch 160, CIFAR-10 Batch 2:  Loss:     0.6652 Validation Accuracy: 0.800000\n",
      "Epoch 160, CIFAR-10 Batch 3:  Loss:     0.5764 Validation Accuracy: 0.900000\n",
      "Epoch 160, CIFAR-10 Batch 4:  Loss:     1.0396 Validation Accuracy: 0.700000\n",
      "Epoch 160, CIFAR-10 Batch 5:  Loss:     0.9486 Validation Accuracy: 0.775000\n",
      "Epoch 161, CIFAR-10 Batch 1:  Loss:     1.0401 Validation Accuracy: 0.725000\n",
      "Epoch 161, CIFAR-10 Batch 2:  Loss:     0.6662 Validation Accuracy: 0.800000\n",
      "Epoch 161, CIFAR-10 Batch 3:  Loss:     0.5742 Validation Accuracy: 0.900000\n",
      "Epoch 161, CIFAR-10 Batch 4:  Loss:     1.0368 Validation Accuracy: 0.700000\n",
      "Epoch 161, CIFAR-10 Batch 5:  Loss:     0.9448 Validation Accuracy: 0.800000\n",
      "Epoch 162, CIFAR-10 Batch 1:  Loss:     1.0373 Validation Accuracy: 0.725000\n",
      "Epoch 162, CIFAR-10 Batch 2:  Loss:     0.6651 Validation Accuracy: 0.800000\n",
      "Epoch 162, CIFAR-10 Batch 3:  Loss:     0.5790 Validation Accuracy: 0.875000\n",
      "Epoch 162, CIFAR-10 Batch 4:  Loss:     1.0375 Validation Accuracy: 0.700000\n",
      "Epoch 162, CIFAR-10 Batch 5:  Loss:     0.9408 Validation Accuracy: 0.800000\n",
      "Epoch 163, CIFAR-10 Batch 1:  Loss:     1.0381 Validation Accuracy: 0.725000\n",
      "Epoch 163, CIFAR-10 Batch 2:  Loss:     0.6725 Validation Accuracy: 0.800000\n",
      "Epoch 163, CIFAR-10 Batch 3:  Loss:     0.5750 Validation Accuracy: 0.900000\n",
      "Epoch 163, CIFAR-10 Batch 4:  Loss:     1.0354 Validation Accuracy: 0.700000\n",
      "Epoch 163, CIFAR-10 Batch 5:  Loss:     0.9405 Validation Accuracy: 0.800000\n",
      "Epoch 164, CIFAR-10 Batch 1:  Loss:     1.0324 Validation Accuracy: 0.725000\n",
      "Epoch 164, CIFAR-10 Batch 2:  Loss:     0.6702 Validation Accuracy: 0.800000\n",
      "Epoch 164, CIFAR-10 Batch 3:  Loss:     0.5748 Validation Accuracy: 0.900000\n",
      "Epoch 164, CIFAR-10 Batch 4:  Loss:     1.0353 Validation Accuracy: 0.700000\n",
      "Epoch 164, CIFAR-10 Batch 5:  Loss:     0.9384 Validation Accuracy: 0.800000\n",
      "Epoch 165, CIFAR-10 Batch 1:  Loss:     1.0311 Validation Accuracy: 0.725000\n",
      "Epoch 165, CIFAR-10 Batch 2:  Loss:     0.6729 Validation Accuracy: 0.800000\n",
      "Epoch 165, CIFAR-10 Batch 3:  Loss:     0.5726 Validation Accuracy: 0.900000\n",
      "Epoch 165, CIFAR-10 Batch 4:  Loss:     1.0352 Validation Accuracy: 0.700000\n",
      "Epoch 165, CIFAR-10 Batch 5:  Loss:     0.9423 Validation Accuracy: 0.800000\n",
      "Epoch 166, CIFAR-10 Batch 1:  Loss:     1.0294 Validation Accuracy: 0.725000\n",
      "Epoch 166, CIFAR-10 Batch 2:  Loss:     0.6652 Validation Accuracy: 0.825000\n",
      "Epoch 166, CIFAR-10 Batch 3:  Loss:     0.5750 Validation Accuracy: 0.900000\n",
      "Epoch 166, CIFAR-10 Batch 4:  Loss:     1.0316 Validation Accuracy: 0.700000\n",
      "Epoch 166, CIFAR-10 Batch 5:  Loss:     0.9414 Validation Accuracy: 0.800000\n",
      "Epoch 167, CIFAR-10 Batch 1:  Loss:     1.0277 Validation Accuracy: 0.725000\n",
      "Epoch 167, CIFAR-10 Batch 2:  Loss:     0.6707 Validation Accuracy: 0.800000\n",
      "Epoch 167, CIFAR-10 Batch 3:  Loss:     0.5739 Validation Accuracy: 0.900000\n",
      "Epoch 167, CIFAR-10 Batch 4:  Loss:     1.0338 Validation Accuracy: 0.700000\n",
      "Epoch 167, CIFAR-10 Batch 5:  Loss:     0.9436 Validation Accuracy: 0.800000\n",
      "Epoch 168, CIFAR-10 Batch 1:  Loss:     1.0288 Validation Accuracy: 0.725000\n",
      "Epoch 168, CIFAR-10 Batch 2:  Loss:     0.6632 Validation Accuracy: 0.800000\n",
      "Epoch 168, CIFAR-10 Batch 3:  Loss:     0.5722 Validation Accuracy: 0.900000\n",
      "Epoch 168, CIFAR-10 Batch 4:  Loss:     1.0337 Validation Accuracy: 0.700000\n",
      "Epoch 168, CIFAR-10 Batch 5:  Loss:     0.9355 Validation Accuracy: 0.800000\n",
      "Epoch 169, CIFAR-10 Batch 1:  Loss:     1.0289 Validation Accuracy: 0.725000\n",
      "Epoch 169, CIFAR-10 Batch 2:  Loss:     0.6650 Validation Accuracy: 0.800000\n",
      "Epoch 169, CIFAR-10 Batch 3:  Loss:     0.5743 Validation Accuracy: 0.900000\n",
      "Epoch 169, CIFAR-10 Batch 4:  Loss:     1.0329 Validation Accuracy: 0.700000\n",
      "Epoch 169, CIFAR-10 Batch 5:  Loss:     0.9377 Validation Accuracy: 0.800000\n",
      "Epoch 170, CIFAR-10 Batch 1:  Loss:     1.0233 Validation Accuracy: 0.725000\n",
      "Epoch 170, CIFAR-10 Batch 2:  Loss:     0.6651 Validation Accuracy: 0.800000\n",
      "Epoch 170, CIFAR-10 Batch 3:  Loss:     0.5717 Validation Accuracy: 0.900000\n",
      "Epoch 170, CIFAR-10 Batch 4:  Loss:     1.0329 Validation Accuracy: 0.700000\n",
      "Epoch 170, CIFAR-10 Batch 5:  Loss:     0.9370 Validation Accuracy: 0.800000\n",
      "Epoch 171, CIFAR-10 Batch 1:  Loss:     1.0257 Validation Accuracy: 0.725000\n",
      "Epoch 171, CIFAR-10 Batch 2:  Loss:     0.6675 Validation Accuracy: 0.800000\n",
      "Epoch 171, CIFAR-10 Batch 3:  Loss:     0.5770 Validation Accuracy: 0.875000\n",
      "Epoch 171, CIFAR-10 Batch 4:  Loss:     1.0277 Validation Accuracy: 0.700000\n",
      "Epoch 171, CIFAR-10 Batch 5:  Loss:     0.9316 Validation Accuracy: 0.800000\n",
      "Epoch 172, CIFAR-10 Batch 1:  Loss:     1.0179 Validation Accuracy: 0.725000\n",
      "Epoch 172, CIFAR-10 Batch 2:  Loss:     0.6612 Validation Accuracy: 0.800000\n",
      "Epoch 172, CIFAR-10 Batch 3:  Loss:     0.5786 Validation Accuracy: 0.900000\n",
      "Epoch 172, CIFAR-10 Batch 4:  Loss:     1.0306 Validation Accuracy: 0.700000\n",
      "Epoch 172, CIFAR-10 Batch 5:  Loss:     0.9321 Validation Accuracy: 0.800000\n",
      "Epoch 173, CIFAR-10 Batch 1:  Loss:     1.0200 Validation Accuracy: 0.725000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 173, CIFAR-10 Batch 2:  Loss:     0.6636 Validation Accuracy: 0.800000\n",
      "Epoch 173, CIFAR-10 Batch 3:  Loss:     0.5745 Validation Accuracy: 0.900000\n",
      "Epoch 173, CIFAR-10 Batch 4:  Loss:     1.0330 Validation Accuracy: 0.700000\n",
      "Epoch 173, CIFAR-10 Batch 5:  Loss:     0.9293 Validation Accuracy: 0.800000\n",
      "Epoch 174, CIFAR-10 Batch 1:  Loss:     1.0184 Validation Accuracy: 0.725000\n",
      "Epoch 174, CIFAR-10 Batch 2:  Loss:     0.6632 Validation Accuracy: 0.800000\n",
      "Epoch 174, CIFAR-10 Batch 3:  Loss:     0.5752 Validation Accuracy: 0.900000\n",
      "Epoch 174, CIFAR-10 Batch 4:  Loss:     1.0316 Validation Accuracy: 0.700000\n",
      "Epoch 174, CIFAR-10 Batch 5:  Loss:     0.9328 Validation Accuracy: 0.800000\n",
      "Epoch 175, CIFAR-10 Batch 1:  Loss:     1.0194 Validation Accuracy: 0.725000\n",
      "Epoch 175, CIFAR-10 Batch 2:  Loss:     0.6671 Validation Accuracy: 0.800000\n",
      "Epoch 175, CIFAR-10 Batch 3:  Loss:     0.5739 Validation Accuracy: 0.900000\n",
      "Epoch 175, CIFAR-10 Batch 4:  Loss:     1.0272 Validation Accuracy: 0.700000\n",
      "Epoch 175, CIFAR-10 Batch 5:  Loss:     0.9296 Validation Accuracy: 0.800000\n",
      "Epoch 176, CIFAR-10 Batch 1:  Loss:     1.0185 Validation Accuracy: 0.725000\n",
      "Epoch 176, CIFAR-10 Batch 2:  Loss:     0.6680 Validation Accuracy: 0.800000\n",
      "Epoch 176, CIFAR-10 Batch 3:  Loss:     0.5780 Validation Accuracy: 0.900000\n",
      "Epoch 176, CIFAR-10 Batch 4:  Loss:     1.0311 Validation Accuracy: 0.700000\n",
      "Epoch 176, CIFAR-10 Batch 5:  Loss:     0.9225 Validation Accuracy: 0.800000\n",
      "Epoch 177, CIFAR-10 Batch 1:  Loss:     1.0203 Validation Accuracy: 0.725000\n",
      "Epoch 177, CIFAR-10 Batch 2:  Loss:     0.6644 Validation Accuracy: 0.800000\n",
      "Epoch 177, CIFAR-10 Batch 3:  Loss:     0.5815 Validation Accuracy: 0.900000\n",
      "Epoch 177, CIFAR-10 Batch 4:  Loss:     1.0256 Validation Accuracy: 0.700000\n",
      "Epoch 177, CIFAR-10 Batch 5:  Loss:     0.9299 Validation Accuracy: 0.775000\n",
      "Epoch 178, CIFAR-10 Batch 1:  Loss:     1.0204 Validation Accuracy: 0.725000\n",
      "Epoch 178, CIFAR-10 Batch 2:  Loss:     0.6567 Validation Accuracy: 0.825000\n",
      "Epoch 178, CIFAR-10 Batch 3:  Loss:     0.5854 Validation Accuracy: 0.900000\n",
      "Epoch 178, CIFAR-10 Batch 4:  Loss:     1.0228 Validation Accuracy: 0.700000\n",
      "Epoch 178, CIFAR-10 Batch 5:  Loss:     0.9296 Validation Accuracy: 0.775000\n",
      "Epoch 179, CIFAR-10 Batch 1:  Loss:     1.0220 Validation Accuracy: 0.700000\n",
      "Epoch 179, CIFAR-10 Batch 2:  Loss:     0.6575 Validation Accuracy: 0.850000\n",
      "Epoch 179, CIFAR-10 Batch 3:  Loss:     0.5850 Validation Accuracy: 0.900000\n",
      "Epoch 179, CIFAR-10 Batch 4:  Loss:     1.0261 Validation Accuracy: 0.700000\n",
      "Epoch 179, CIFAR-10 Batch 5:  Loss:     0.9331 Validation Accuracy: 0.775000\n",
      "Epoch 180, CIFAR-10 Batch 1:  Loss:     1.0175 Validation Accuracy: 0.725000\n",
      "Epoch 180, CIFAR-10 Batch 2:  Loss:     0.6552 Validation Accuracy: 0.850000\n",
      "Epoch 180, CIFAR-10 Batch 3:  Loss:     0.5871 Validation Accuracy: 0.900000\n",
      "Epoch 180, CIFAR-10 Batch 4:  Loss:     1.0263 Validation Accuracy: 0.700000\n",
      "Epoch 180, CIFAR-10 Batch 5:  Loss:     0.9317 Validation Accuracy: 0.775000\n",
      "Epoch 181, CIFAR-10 Batch 1:  Loss:     1.0193 Validation Accuracy: 0.725000\n",
      "Epoch 181, CIFAR-10 Batch 2:  Loss:     0.6547 Validation Accuracy: 0.850000\n",
      "Epoch 181, CIFAR-10 Batch 3:  Loss:     0.5844 Validation Accuracy: 0.900000\n",
      "Epoch 181, CIFAR-10 Batch 4:  Loss:     1.0211 Validation Accuracy: 0.700000\n",
      "Epoch 181, CIFAR-10 Batch 5:  Loss:     0.9307 Validation Accuracy: 0.775000\n",
      "Epoch 182, CIFAR-10 Batch 1:  Loss:     1.0220 Validation Accuracy: 0.700000\n",
      "Epoch 182, CIFAR-10 Batch 2:  Loss:     0.6496 Validation Accuracy: 0.850000\n",
      "Epoch 182, CIFAR-10 Batch 3:  Loss:     0.5840 Validation Accuracy: 0.900000\n",
      "Epoch 182, CIFAR-10 Batch 4:  Loss:     1.0264 Validation Accuracy: 0.700000\n",
      "Epoch 182, CIFAR-10 Batch 5:  Loss:     0.9291 Validation Accuracy: 0.800000\n",
      "Epoch 183, CIFAR-10 Batch 1:  Loss:     1.0243 Validation Accuracy: 0.700000\n",
      "Epoch 183, CIFAR-10 Batch 2:  Loss:     0.6525 Validation Accuracy: 0.850000\n",
      "Epoch 183, CIFAR-10 Batch 3:  Loss:     0.5872 Validation Accuracy: 0.900000\n",
      "Epoch 183, CIFAR-10 Batch 4:  Loss:     1.0239 Validation Accuracy: 0.700000\n",
      "Epoch 183, CIFAR-10 Batch 5:  Loss:     0.9281 Validation Accuracy: 0.775000\n",
      "Epoch 184, CIFAR-10 Batch 1:  Loss:     1.0231 Validation Accuracy: 0.700000\n",
      "Epoch 184, CIFAR-10 Batch 2:  Loss:     0.6512 Validation Accuracy: 0.850000\n",
      "Epoch 184, CIFAR-10 Batch 3:  Loss:     0.5911 Validation Accuracy: 0.875000\n",
      "Epoch 184, CIFAR-10 Batch 4:  Loss:     1.0265 Validation Accuracy: 0.675000\n",
      "Epoch 184, CIFAR-10 Batch 5:  Loss:     0.9335 Validation Accuracy: 0.775000\n",
      "Epoch 185, CIFAR-10 Batch 1:  Loss:     1.0228 Validation Accuracy: 0.700000\n",
      "Epoch 185, CIFAR-10 Batch 2:  Loss:     0.6445 Validation Accuracy: 0.850000\n",
      "Epoch 185, CIFAR-10 Batch 3:  Loss:     0.5910 Validation Accuracy: 0.875000\n",
      "Epoch 185, CIFAR-10 Batch 4:  Loss:     1.0249 Validation Accuracy: 0.675000\n",
      "Epoch 185, CIFAR-10 Batch 5:  Loss:     0.9294 Validation Accuracy: 0.775000\n",
      "Epoch 186, CIFAR-10 Batch 1:  Loss:     1.0246 Validation Accuracy: 0.725000\n",
      "Epoch 186, CIFAR-10 Batch 2:  Loss:     0.6395 Validation Accuracy: 0.850000\n",
      "Epoch 186, CIFAR-10 Batch 3:  Loss:     0.5874 Validation Accuracy: 0.875000\n",
      "Epoch 186, CIFAR-10 Batch 4:  Loss:     1.0212 Validation Accuracy: 0.675000\n",
      "Epoch 186, CIFAR-10 Batch 5:  Loss:     0.9332 Validation Accuracy: 0.775000\n",
      "Epoch 187, CIFAR-10 Batch 1:  Loss:     1.0280 Validation Accuracy: 0.700000\n",
      "Epoch 187, CIFAR-10 Batch 2:  Loss:     0.6343 Validation Accuracy: 0.850000\n",
      "Epoch 187, CIFAR-10 Batch 3:  Loss:     0.5860 Validation Accuracy: 0.875000\n",
      "Epoch 187, CIFAR-10 Batch 4:  Loss:     1.0211 Validation Accuracy: 0.675000\n",
      "Epoch 187, CIFAR-10 Batch 5:  Loss:     0.9302 Validation Accuracy: 0.775000\n",
      "Epoch 188, CIFAR-10 Batch 1:  Loss:     1.0331 Validation Accuracy: 0.700000\n",
      "Epoch 188, CIFAR-10 Batch 2:  Loss:     0.6351 Validation Accuracy: 0.850000\n",
      "Epoch 188, CIFAR-10 Batch 3:  Loss:     0.5892 Validation Accuracy: 0.875000\n",
      "Epoch 188, CIFAR-10 Batch 4:  Loss:     1.0213 Validation Accuracy: 0.675000\n",
      "Epoch 188, CIFAR-10 Batch 5:  Loss:     0.9223 Validation Accuracy: 0.800000\n",
      "Epoch 189, CIFAR-10 Batch 1:  Loss:     1.0321 Validation Accuracy: 0.700000\n",
      "Epoch 189, CIFAR-10 Batch 2:  Loss:     0.6341 Validation Accuracy: 0.850000\n",
      "Epoch 189, CIFAR-10 Batch 3:  Loss:     0.5878 Validation Accuracy: 0.875000\n",
      "Epoch 189, CIFAR-10 Batch 4:  Loss:     1.0138 Validation Accuracy: 0.675000\n",
      "Epoch 189, CIFAR-10 Batch 5:  Loss:     0.9268 Validation Accuracy: 0.800000\n",
      "Epoch 190, CIFAR-10 Batch 1:  Loss:     1.0264 Validation Accuracy: 0.725000\n",
      "Epoch 190, CIFAR-10 Batch 2:  Loss:     0.6323 Validation Accuracy: 0.850000\n",
      "Epoch 190, CIFAR-10 Batch 3:  Loss:     0.5848 Validation Accuracy: 0.875000\n",
      "Epoch 190, CIFAR-10 Batch 4:  Loss:     1.0092 Validation Accuracy: 0.675000\n",
      "Epoch 190, CIFAR-10 Batch 5:  Loss:     0.9278 Validation Accuracy: 0.775000\n",
      "Epoch 191, CIFAR-10 Batch 1:  Loss:     1.0280 Validation Accuracy: 0.725000\n",
      "Epoch 191, CIFAR-10 Batch 2:  Loss:     0.6311 Validation Accuracy: 0.850000\n",
      "Epoch 191, CIFAR-10 Batch 3:  Loss:     0.5860 Validation Accuracy: 0.875000\n",
      "Epoch 191, CIFAR-10 Batch 4:  Loss:     1.0120 Validation Accuracy: 0.675000\n",
      "Epoch 191, CIFAR-10 Batch 5:  Loss:     0.9217 Validation Accuracy: 0.775000\n",
      "Epoch 192, CIFAR-10 Batch 1:  Loss:     1.0324 Validation Accuracy: 0.700000\n",
      "Epoch 192, CIFAR-10 Batch 2:  Loss:     0.6316 Validation Accuracy: 0.850000\n",
      "Epoch 192, CIFAR-10 Batch 3:  Loss:     0.5841 Validation Accuracy: 0.875000\n",
      "Epoch 192, CIFAR-10 Batch 4:  Loss:     1.0065 Validation Accuracy: 0.675000\n",
      "Epoch 192, CIFAR-10 Batch 5:  Loss:     0.9245 Validation Accuracy: 0.775000\n",
      "Epoch 193, CIFAR-10 Batch 1:  Loss:     1.0287 Validation Accuracy: 0.700000\n",
      "Epoch 193, CIFAR-10 Batch 2:  Loss:     0.6266 Validation Accuracy: 0.850000\n",
      "Epoch 193, CIFAR-10 Batch 3:  Loss:     0.5826 Validation Accuracy: 0.875000\n",
      "Epoch 193, CIFAR-10 Batch 4:  Loss:     1.0020 Validation Accuracy: 0.700000\n",
      "Epoch 193, CIFAR-10 Batch 5:  Loss:     0.9194 Validation Accuracy: 0.775000\n",
      "Epoch 194, CIFAR-10 Batch 1:  Loss:     1.0327 Validation Accuracy: 0.700000\n",
      "Epoch 194, CIFAR-10 Batch 2:  Loss:     0.6225 Validation Accuracy: 0.850000\n",
      "Epoch 194, CIFAR-10 Batch 3:  Loss:     0.5844 Validation Accuracy: 0.875000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 194, CIFAR-10 Batch 4:  Loss:     1.0041 Validation Accuracy: 0.700000\n",
      "Epoch 194, CIFAR-10 Batch 5:  Loss:     0.9187 Validation Accuracy: 0.775000\n",
      "Epoch 195, CIFAR-10 Batch 1:  Loss:     1.0284 Validation Accuracy: 0.700000\n",
      "Epoch 195, CIFAR-10 Batch 2:  Loss:     0.6266 Validation Accuracy: 0.850000\n",
      "Epoch 195, CIFAR-10 Batch 3:  Loss:     0.5856 Validation Accuracy: 0.875000\n",
      "Epoch 195, CIFAR-10 Batch 4:  Loss:     1.0069 Validation Accuracy: 0.700000\n",
      "Epoch 195, CIFAR-10 Batch 5:  Loss:     0.9224 Validation Accuracy: 0.800000\n",
      "Epoch 196, CIFAR-10 Batch 1:  Loss:     1.0227 Validation Accuracy: 0.725000\n",
      "Epoch 196, CIFAR-10 Batch 2:  Loss:     0.6254 Validation Accuracy: 0.850000\n",
      "Epoch 196, CIFAR-10 Batch 3:  Loss:     0.5862 Validation Accuracy: 0.875000\n",
      "Epoch 196, CIFAR-10 Batch 4:  Loss:     1.0052 Validation Accuracy: 0.725000\n",
      "Epoch 196, CIFAR-10 Batch 5:  Loss:     0.9192 Validation Accuracy: 0.800000\n",
      "Epoch 197, CIFAR-10 Batch 1:  Loss:     1.0289 Validation Accuracy: 0.700000\n",
      "Epoch 197, CIFAR-10 Batch 2:  Loss:     0.6295 Validation Accuracy: 0.850000\n",
      "Epoch 197, CIFAR-10 Batch 3:  Loss:     0.5830 Validation Accuracy: 0.875000\n",
      "Epoch 197, CIFAR-10 Batch 4:  Loss:     1.0042 Validation Accuracy: 0.725000\n",
      "Epoch 197, CIFAR-10 Batch 5:  Loss:     0.9174 Validation Accuracy: 0.775000\n",
      "Epoch 198, CIFAR-10 Batch 1:  Loss:     1.0261 Validation Accuracy: 0.700000\n",
      "Epoch 198, CIFAR-10 Batch 2:  Loss:     0.6291 Validation Accuracy: 0.850000\n",
      "Epoch 198, CIFAR-10 Batch 3:  Loss:     0.5828 Validation Accuracy: 0.875000\n",
      "Epoch 198, CIFAR-10 Batch 4:  Loss:     1.0013 Validation Accuracy: 0.725000\n",
      "Epoch 198, CIFAR-10 Batch 5:  Loss:     0.9178 Validation Accuracy: 0.775000\n",
      "Epoch 199, CIFAR-10 Batch 1:  Loss:     1.0268 Validation Accuracy: 0.700000\n",
      "Epoch 199, CIFAR-10 Batch 2:  Loss:     0.6292 Validation Accuracy: 0.850000\n",
      "Epoch 199, CIFAR-10 Batch 3:  Loss:     0.5815 Validation Accuracy: 0.875000\n",
      "Epoch 199, CIFAR-10 Batch 4:  Loss:     0.9990 Validation Accuracy: 0.725000\n",
      "Epoch 199, CIFAR-10 Batch 5:  Loss:     0.9145 Validation Accuracy: 0.775000\n",
      "Epoch 200, CIFAR-10 Batch 1:  Loss:     1.0276 Validation Accuracy: 0.700000\n",
      "Epoch 200, CIFAR-10 Batch 2:  Loss:     0.6312 Validation Accuracy: 0.850000\n",
      "Epoch 200, CIFAR-10 Batch 3:  Loss:     0.5815 Validation Accuracy: 0.875000\n",
      "Epoch 200, CIFAR-10 Batch 4:  Loss:     0.9991 Validation Accuracy: 0.700000\n",
      "Epoch 200, CIFAR-10 Batch 5:  Loss:     0.9156 Validation Accuracy: 0.775000\n",
      "Epoch 201, CIFAR-10 Batch 1:  Loss:     1.0214 Validation Accuracy: 0.700000\n",
      "Epoch 201, CIFAR-10 Batch 2:  Loss:     0.6289 Validation Accuracy: 0.850000\n",
      "Epoch 201, CIFAR-10 Batch 3:  Loss:     0.5823 Validation Accuracy: 0.875000\n",
      "Epoch 201, CIFAR-10 Batch 4:  Loss:     1.0011 Validation Accuracy: 0.700000\n",
      "Epoch 201, CIFAR-10 Batch 5:  Loss:     0.9160 Validation Accuracy: 0.775000\n",
      "Epoch 202, CIFAR-10 Batch 1:  Loss:     1.0248 Validation Accuracy: 0.700000\n",
      "Epoch 202, CIFAR-10 Batch 2:  Loss:     0.6291 Validation Accuracy: 0.850000\n",
      "Epoch 202, CIFAR-10 Batch 3:  Loss:     0.5825 Validation Accuracy: 0.875000\n",
      "Epoch 202, CIFAR-10 Batch 4:  Loss:     0.9949 Validation Accuracy: 0.725000\n",
      "Epoch 202, CIFAR-10 Batch 5:  Loss:     0.9142 Validation Accuracy: 0.775000\n",
      "Epoch 203, CIFAR-10 Batch 1:  Loss:     1.0216 Validation Accuracy: 0.700000\n",
      "Epoch 203, CIFAR-10 Batch 2:  Loss:     0.6274 Validation Accuracy: 0.850000\n",
      "Epoch 203, CIFAR-10 Batch 3:  Loss:     0.5812 Validation Accuracy: 0.875000\n",
      "Epoch 203, CIFAR-10 Batch 4:  Loss:     0.9941 Validation Accuracy: 0.725000\n",
      "Epoch 203, CIFAR-10 Batch 5:  Loss:     0.9141 Validation Accuracy: 0.800000\n",
      "Epoch 204, CIFAR-10 Batch 1:  Loss:     1.0186 Validation Accuracy: 0.700000\n",
      "Epoch 204, CIFAR-10 Batch 2:  Loss:     0.6268 Validation Accuracy: 0.825000\n",
      "Epoch 204, CIFAR-10 Batch 3:  Loss:     0.5804 Validation Accuracy: 0.850000\n",
      "Epoch 204, CIFAR-10 Batch 4:  Loss:     0.9949 Validation Accuracy: 0.725000\n",
      "Epoch 204, CIFAR-10 Batch 5:  Loss:     0.9115 Validation Accuracy: 0.775000\n",
      "Epoch 205, CIFAR-10 Batch 1:  Loss:     1.0177 Validation Accuracy: 0.700000\n",
      "Epoch 205, CIFAR-10 Batch 2:  Loss:     0.6283 Validation Accuracy: 0.850000\n",
      "Epoch 205, CIFAR-10 Batch 3:  Loss:     0.5765 Validation Accuracy: 0.875000\n",
      "Epoch 205, CIFAR-10 Batch 4:  Loss:     0.9930 Validation Accuracy: 0.725000\n",
      "Epoch 205, CIFAR-10 Batch 5:  Loss:     0.9101 Validation Accuracy: 0.775000\n",
      "Epoch 206, CIFAR-10 Batch 1:  Loss:     1.0141 Validation Accuracy: 0.725000\n",
      "Epoch 206, CIFAR-10 Batch 2:  Loss:     0.6291 Validation Accuracy: 0.850000\n",
      "Epoch 206, CIFAR-10 Batch 3:  Loss:     0.5773 Validation Accuracy: 0.850000\n",
      "Epoch 206, CIFAR-10 Batch 4:  Loss:     0.9912 Validation Accuracy: 0.725000\n",
      "Epoch 206, CIFAR-10 Batch 5:  Loss:     0.9104 Validation Accuracy: 0.800000\n",
      "Epoch 207, CIFAR-10 Batch 1:  Loss:     1.0196 Validation Accuracy: 0.700000\n",
      "Epoch 207, CIFAR-10 Batch 2:  Loss:     0.6286 Validation Accuracy: 0.825000\n",
      "Epoch 207, CIFAR-10 Batch 3:  Loss:     0.5763 Validation Accuracy: 0.850000\n",
      "Epoch 207, CIFAR-10 Batch 4:  Loss:     0.9918 Validation Accuracy: 0.725000\n",
      "Epoch 207, CIFAR-10 Batch 5:  Loss:     0.9089 Validation Accuracy: 0.800000\n",
      "Epoch 208, CIFAR-10 Batch 1:  Loss:     1.0139 Validation Accuracy: 0.725000\n",
      "Epoch 208, CIFAR-10 Batch 2:  Loss:     0.6272 Validation Accuracy: 0.825000\n",
      "Epoch 208, CIFAR-10 Batch 3:  Loss:     0.5764 Validation Accuracy: 0.875000\n",
      "Epoch 208, CIFAR-10 Batch 4:  Loss:     0.9881 Validation Accuracy: 0.725000\n",
      "Epoch 208, CIFAR-10 Batch 5:  Loss:     0.9125 Validation Accuracy: 0.800000\n",
      "Epoch 209, CIFAR-10 Batch 1:  Loss:     1.0208 Validation Accuracy: 0.725000\n",
      "Epoch 209, CIFAR-10 Batch 2:  Loss:     0.6333 Validation Accuracy: 0.825000\n",
      "Epoch 209, CIFAR-10 Batch 3:  Loss:     0.5752 Validation Accuracy: 0.850000\n",
      "Epoch 209, CIFAR-10 Batch 4:  Loss:     0.9859 Validation Accuracy: 0.725000\n",
      "Epoch 209, CIFAR-10 Batch 5:  Loss:     0.9093 Validation Accuracy: 0.775000\n",
      "Epoch 210, CIFAR-10 Batch 1:  Loss:     1.0179 Validation Accuracy: 0.725000\n",
      "Epoch 210, CIFAR-10 Batch 2:  Loss:     0.6301 Validation Accuracy: 0.825000\n",
      "Epoch 210, CIFAR-10 Batch 3:  Loss:     0.5716 Validation Accuracy: 0.875000\n",
      "Epoch 210, CIFAR-10 Batch 4:  Loss:     0.9912 Validation Accuracy: 0.725000\n",
      "Epoch 210, CIFAR-10 Batch 5:  Loss:     0.9087 Validation Accuracy: 0.775000\n",
      "Epoch 211, CIFAR-10 Batch 1:  Loss:     1.0167 Validation Accuracy: 0.725000\n",
      "Epoch 211, CIFAR-10 Batch 2:  Loss:     0.6275 Validation Accuracy: 0.825000\n",
      "Epoch 211, CIFAR-10 Batch 3:  Loss:     0.5746 Validation Accuracy: 0.875000\n",
      "Epoch 211, CIFAR-10 Batch 4:  Loss:     0.9908 Validation Accuracy: 0.725000\n",
      "Epoch 211, CIFAR-10 Batch 5:  Loss:     0.9063 Validation Accuracy: 0.775000\n",
      "Epoch 212, CIFAR-10 Batch 1:  Loss:     1.0147 Validation Accuracy: 0.700000\n",
      "Epoch 212, CIFAR-10 Batch 2:  Loss:     0.6226 Validation Accuracy: 0.850000\n",
      "Epoch 212, CIFAR-10 Batch 3:  Loss:     0.5751 Validation Accuracy: 0.850000\n",
      "Epoch 212, CIFAR-10 Batch 4:  Loss:     0.9930 Validation Accuracy: 0.725000\n",
      "Epoch 212, CIFAR-10 Batch 5:  Loss:     0.9086 Validation Accuracy: 0.775000\n",
      "Epoch 213, CIFAR-10 Batch 1:  Loss:     1.0108 Validation Accuracy: 0.700000\n",
      "Epoch 213, CIFAR-10 Batch 2:  Loss:     0.6315 Validation Accuracy: 0.800000\n",
      "Epoch 213, CIFAR-10 Batch 3:  Loss:     0.5728 Validation Accuracy: 0.850000\n",
      "Epoch 213, CIFAR-10 Batch 4:  Loss:     0.9887 Validation Accuracy: 0.700000\n",
      "Epoch 213, CIFAR-10 Batch 5:  Loss:     0.9048 Validation Accuracy: 0.750000\n",
      "Epoch 214, CIFAR-10 Batch 1:  Loss:     1.0062 Validation Accuracy: 0.700000\n",
      "Epoch 214, CIFAR-10 Batch 2:  Loss:     0.6225 Validation Accuracy: 0.850000\n",
      "Epoch 214, CIFAR-10 Batch 3:  Loss:     0.5721 Validation Accuracy: 0.850000\n",
      "Epoch 214, CIFAR-10 Batch 4:  Loss:     0.9917 Validation Accuracy: 0.700000\n",
      "Epoch 214, CIFAR-10 Batch 5:  Loss:     0.9068 Validation Accuracy: 0.775000\n",
      "Epoch 215, CIFAR-10 Batch 1:  Loss:     1.0166 Validation Accuracy: 0.675000\n",
      "Epoch 215, CIFAR-10 Batch 2:  Loss:     0.6233 Validation Accuracy: 0.850000\n",
      "Epoch 215, CIFAR-10 Batch 3:  Loss:     0.5737 Validation Accuracy: 0.850000\n",
      "Epoch 215, CIFAR-10 Batch 4:  Loss:     0.9860 Validation Accuracy: 0.725000\n",
      "Epoch 215, CIFAR-10 Batch 5:  Loss:     0.9058 Validation Accuracy: 0.775000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 216, CIFAR-10 Batch 1:  Loss:     1.0052 Validation Accuracy: 0.700000\n",
      "Epoch 216, CIFAR-10 Batch 2:  Loss:     0.6255 Validation Accuracy: 0.850000\n",
      "Epoch 216, CIFAR-10 Batch 3:  Loss:     0.5788 Validation Accuracy: 0.850000\n",
      "Epoch 216, CIFAR-10 Batch 4:  Loss:     0.9854 Validation Accuracy: 0.700000\n",
      "Epoch 216, CIFAR-10 Batch 5:  Loss:     0.9073 Validation Accuracy: 0.775000\n",
      "Epoch 217, CIFAR-10 Batch 1:  Loss:     1.0036 Validation Accuracy: 0.700000\n",
      "Epoch 217, CIFAR-10 Batch 2:  Loss:     0.6241 Validation Accuracy: 0.850000\n",
      "Epoch 217, CIFAR-10 Batch 3:  Loss:     0.5737 Validation Accuracy: 0.850000\n",
      "Epoch 217, CIFAR-10 Batch 4:  Loss:     0.9855 Validation Accuracy: 0.700000\n",
      "Epoch 217, CIFAR-10 Batch 5:  Loss:     0.9083 Validation Accuracy: 0.775000\n",
      "Epoch 218, CIFAR-10 Batch 1:  Loss:     1.0060 Validation Accuracy: 0.675000\n",
      "Epoch 218, CIFAR-10 Batch 2:  Loss:     0.6234 Validation Accuracy: 0.850000\n",
      "Epoch 218, CIFAR-10 Batch 3:  Loss:     0.5724 Validation Accuracy: 0.850000\n",
      "Epoch 218, CIFAR-10 Batch 4:  Loss:     0.9865 Validation Accuracy: 0.725000\n",
      "Epoch 218, CIFAR-10 Batch 5:  Loss:     0.9092 Validation Accuracy: 0.750000\n",
      "Epoch 219, CIFAR-10 Batch 1:  Loss:     1.0070 Validation Accuracy: 0.675000\n",
      "Epoch 219, CIFAR-10 Batch 2:  Loss:     0.6245 Validation Accuracy: 0.825000\n",
      "Epoch 219, CIFAR-10 Batch 3:  Loss:     0.5677 Validation Accuracy: 0.875000\n",
      "Epoch 219, CIFAR-10 Batch 4:  Loss:     0.9821 Validation Accuracy: 0.700000\n",
      "Epoch 219, CIFAR-10 Batch 5:  Loss:     0.9052 Validation Accuracy: 0.775000\n",
      "Epoch 220, CIFAR-10 Batch 1:  Loss:     1.0042 Validation Accuracy: 0.675000\n",
      "Epoch 220, CIFAR-10 Batch 2:  Loss:     0.6229 Validation Accuracy: 0.850000\n",
      "Epoch 220, CIFAR-10 Batch 3:  Loss:     0.5705 Validation Accuracy: 0.875000\n",
      "Epoch 220, CIFAR-10 Batch 4:  Loss:     0.9898 Validation Accuracy: 0.700000\n",
      "Epoch 220, CIFAR-10 Batch 5:  Loss:     0.9084 Validation Accuracy: 0.775000\n",
      "Epoch 221, CIFAR-10 Batch 1:  Loss:     1.0055 Validation Accuracy: 0.675000\n",
      "Epoch 221, CIFAR-10 Batch 2:  Loss:     0.6316 Validation Accuracy: 0.825000\n",
      "Epoch 221, CIFAR-10 Batch 3:  Loss:     0.5681 Validation Accuracy: 0.875000\n",
      "Epoch 221, CIFAR-10 Batch 4:  Loss:     0.9911 Validation Accuracy: 0.700000\n",
      "Epoch 221, CIFAR-10 Batch 5:  Loss:     0.9077 Validation Accuracy: 0.750000\n",
      "Epoch 222, CIFAR-10 Batch 1:  Loss:     1.0008 Validation Accuracy: 0.700000\n",
      "Epoch 222, CIFAR-10 Batch 2:  Loss:     0.6301 Validation Accuracy: 0.850000\n",
      "Epoch 222, CIFAR-10 Batch 3:  Loss:     0.5683 Validation Accuracy: 0.850000\n",
      "Epoch 222, CIFAR-10 Batch 4:  Loss:     0.9858 Validation Accuracy: 0.725000\n",
      "Epoch 222, CIFAR-10 Batch 5:  Loss:     0.9074 Validation Accuracy: 0.750000\n",
      "Epoch 223, CIFAR-10 Batch 1:  Loss:     1.0028 Validation Accuracy: 0.700000\n",
      "Epoch 223, CIFAR-10 Batch 2:  Loss:     0.6231 Validation Accuracy: 0.850000\n",
      "Epoch 223, CIFAR-10 Batch 3:  Loss:     0.5700 Validation Accuracy: 0.850000\n",
      "Epoch 223, CIFAR-10 Batch 4:  Loss:     0.9860 Validation Accuracy: 0.700000\n",
      "Epoch 223, CIFAR-10 Batch 5:  Loss:     0.9080 Validation Accuracy: 0.750000\n",
      "Epoch 224, CIFAR-10 Batch 1:  Loss:     1.0002 Validation Accuracy: 0.700000\n",
      "Epoch 224, CIFAR-10 Batch 2:  Loss:     0.6240 Validation Accuracy: 0.850000\n",
      "Epoch 224, CIFAR-10 Batch 3:  Loss:     0.5664 Validation Accuracy: 0.875000\n",
      "Epoch 224, CIFAR-10 Batch 4:  Loss:     0.9832 Validation Accuracy: 0.700000\n",
      "Epoch 224, CIFAR-10 Batch 5:  Loss:     0.9064 Validation Accuracy: 0.750000\n",
      "Epoch 225, CIFAR-10 Batch 1:  Loss:     1.0033 Validation Accuracy: 0.675000\n",
      "Epoch 225, CIFAR-10 Batch 2:  Loss:     0.6226 Validation Accuracy: 0.850000\n",
      "Epoch 225, CIFAR-10 Batch 3:  Loss:     0.5628 Validation Accuracy: 0.850000\n",
      "Epoch 225, CIFAR-10 Batch 4:  Loss:     0.9894 Validation Accuracy: 0.725000\n",
      "Epoch 225, CIFAR-10 Batch 5:  Loss:     0.9099 Validation Accuracy: 0.750000\n",
      "Epoch 226, CIFAR-10 Batch 1:  Loss:     1.0033 Validation Accuracy: 0.700000\n",
      "Epoch 226, CIFAR-10 Batch 2:  Loss:     0.6271 Validation Accuracy: 0.850000\n",
      "Epoch 226, CIFAR-10 Batch 3:  Loss:     0.5699 Validation Accuracy: 0.875000\n",
      "Epoch 226, CIFAR-10 Batch 4:  Loss:     0.9832 Validation Accuracy: 0.700000\n",
      "Epoch 226, CIFAR-10 Batch 5:  Loss:     0.9065 Validation Accuracy: 0.750000\n",
      "Epoch 227, CIFAR-10 Batch 1:  Loss:     1.0008 Validation Accuracy: 0.675000\n",
      "Epoch 227, CIFAR-10 Batch 2:  Loss:     0.6253 Validation Accuracy: 0.850000\n",
      "Epoch 227, CIFAR-10 Batch 3:  Loss:     0.5704 Validation Accuracy: 0.875000\n",
      "Epoch 227, CIFAR-10 Batch 4:  Loss:     0.9832 Validation Accuracy: 0.700000\n",
      "Epoch 227, CIFAR-10 Batch 5:  Loss:     0.9035 Validation Accuracy: 0.750000\n",
      "Epoch 228, CIFAR-10 Batch 1:  Loss:     1.0008 Validation Accuracy: 0.650000\n",
      "Epoch 228, CIFAR-10 Batch 2:  Loss:     0.6267 Validation Accuracy: 0.850000\n",
      "Epoch 228, CIFAR-10 Batch 3:  Loss:     0.5628 Validation Accuracy: 0.875000\n",
      "Epoch 228, CIFAR-10 Batch 4:  Loss:     0.9839 Validation Accuracy: 0.700000\n",
      "Epoch 228, CIFAR-10 Batch 5:  Loss:     0.9073 Validation Accuracy: 0.750000\n",
      "Epoch 229, CIFAR-10 Batch 1:  Loss:     0.9979 Validation Accuracy: 0.700000\n",
      "Epoch 229, CIFAR-10 Batch 2:  Loss:     0.6290 Validation Accuracy: 0.800000\n",
      "Epoch 229, CIFAR-10 Batch 3:  Loss:     0.5642 Validation Accuracy: 0.875000\n",
      "Epoch 229, CIFAR-10 Batch 4:  Loss:     0.9918 Validation Accuracy: 0.675000\n",
      "Epoch 229, CIFAR-10 Batch 5:  Loss:     0.9070 Validation Accuracy: 0.750000\n",
      "Epoch 230, CIFAR-10 Batch 1:  Loss:     0.9996 Validation Accuracy: 0.700000\n",
      "Epoch 230, CIFAR-10 Batch 2:  Loss:     0.6285 Validation Accuracy: 0.825000\n",
      "Epoch 230, CIFAR-10 Batch 3:  Loss:     0.5646 Validation Accuracy: 0.875000\n",
      "Epoch 230, CIFAR-10 Batch 4:  Loss:     0.9911 Validation Accuracy: 0.725000\n",
      "Epoch 230, CIFAR-10 Batch 5:  Loss:     0.9093 Validation Accuracy: 0.750000\n",
      "Epoch 231, CIFAR-10 Batch 1:  Loss:     0.9986 Validation Accuracy: 0.675000\n",
      "Epoch 231, CIFAR-10 Batch 2:  Loss:     0.6247 Validation Accuracy: 0.875000\n",
      "Epoch 231, CIFAR-10 Batch 3:  Loss:     0.5652 Validation Accuracy: 0.875000\n",
      "Epoch 231, CIFAR-10 Batch 4:  Loss:     0.9910 Validation Accuracy: 0.700000\n",
      "Epoch 231, CIFAR-10 Batch 5:  Loss:     0.9094 Validation Accuracy: 0.750000\n",
      "Epoch 232, CIFAR-10 Batch 1:  Loss:     0.9942 Validation Accuracy: 0.675000\n",
      "Epoch 232, CIFAR-10 Batch 2:  Loss:     0.6222 Validation Accuracy: 0.825000\n",
      "Epoch 232, CIFAR-10 Batch 3:  Loss:     0.5703 Validation Accuracy: 0.875000\n",
      "Epoch 232, CIFAR-10 Batch 4:  Loss:     0.9892 Validation Accuracy: 0.700000\n",
      "Epoch 232, CIFAR-10 Batch 5:  Loss:     0.9046 Validation Accuracy: 0.750000\n",
      "Epoch 233, CIFAR-10 Batch 1:  Loss:     0.9946 Validation Accuracy: 0.675000\n",
      "Epoch 233, CIFAR-10 Batch 2:  Loss:     0.6223 Validation Accuracy: 0.875000\n",
      "Epoch 233, CIFAR-10 Batch 3:  Loss:     0.5655 Validation Accuracy: 0.875000\n",
      "Epoch 233, CIFAR-10 Batch 4:  Loss:     0.9889 Validation Accuracy: 0.700000\n",
      "Epoch 233, CIFAR-10 Batch 5:  Loss:     0.9137 Validation Accuracy: 0.775000\n",
      "Epoch 234, CIFAR-10 Batch 1:  Loss:     0.9936 Validation Accuracy: 0.700000\n",
      "Epoch 234, CIFAR-10 Batch 2:  Loss:     0.6334 Validation Accuracy: 0.825000\n",
      "Epoch 234, CIFAR-10 Batch 3:  Loss:     0.5645 Validation Accuracy: 0.900000\n",
      "Epoch 234, CIFAR-10 Batch 4:  Loss:     0.9841 Validation Accuracy: 0.725000\n",
      "Epoch 234, CIFAR-10 Batch 5:  Loss:     0.9069 Validation Accuracy: 0.750000\n",
      "Epoch 235, CIFAR-10 Batch 1:  Loss:     1.0077 Validation Accuracy: 0.700000\n",
      "Epoch 235, CIFAR-10 Batch 2:  Loss:     0.6289 Validation Accuracy: 0.800000\n",
      "Epoch 235, CIFAR-10 Batch 3:  Loss:     0.5710 Validation Accuracy: 0.900000\n",
      "Epoch 235, CIFAR-10 Batch 4:  Loss:     0.9958 Validation Accuracy: 0.700000\n",
      "Epoch 235, CIFAR-10 Batch 5:  Loss:     0.9067 Validation Accuracy: 0.775000\n",
      "Epoch 236, CIFAR-10 Batch 1:  Loss:     0.9976 Validation Accuracy: 0.675000\n",
      "Epoch 236, CIFAR-10 Batch 2:  Loss:     0.6253 Validation Accuracy: 0.825000\n",
      "Epoch 236, CIFAR-10 Batch 3:  Loss:     0.5682 Validation Accuracy: 0.900000\n",
      "Epoch 236, CIFAR-10 Batch 4:  Loss:     0.9938 Validation Accuracy: 0.725000\n",
      "Epoch 236, CIFAR-10 Batch 5:  Loss:     0.9101 Validation Accuracy: 0.775000\n",
      "Epoch 237, CIFAR-10 Batch 1:  Loss:     0.9933 Validation Accuracy: 0.675000\n",
      "Epoch 237, CIFAR-10 Batch 2:  Loss:     0.6219 Validation Accuracy: 0.850000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 237, CIFAR-10 Batch 3:  Loss:     0.5706 Validation Accuracy: 0.900000\n",
      "Epoch 237, CIFAR-10 Batch 4:  Loss:     0.9878 Validation Accuracy: 0.700000\n",
      "Epoch 237, CIFAR-10 Batch 5:  Loss:     0.9046 Validation Accuracy: 0.775000\n",
      "Epoch 238, CIFAR-10 Batch 1:  Loss:     0.9988 Validation Accuracy: 0.700000\n",
      "Epoch 238, CIFAR-10 Batch 2:  Loss:     0.6270 Validation Accuracy: 0.850000\n",
      "Epoch 238, CIFAR-10 Batch 3:  Loss:     0.5733 Validation Accuracy: 0.875000\n",
      "Epoch 238, CIFAR-10 Batch 4:  Loss:     0.9889 Validation Accuracy: 0.725000\n",
      "Epoch 238, CIFAR-10 Batch 5:  Loss:     0.9048 Validation Accuracy: 0.775000\n",
      "Epoch 239, CIFAR-10 Batch 1:  Loss:     0.9987 Validation Accuracy: 0.700000\n",
      "Epoch 239, CIFAR-10 Batch 2:  Loss:     0.6258 Validation Accuracy: 0.850000\n",
      "Epoch 239, CIFAR-10 Batch 3:  Loss:     0.5684 Validation Accuracy: 0.875000\n",
      "Epoch 239, CIFAR-10 Batch 4:  Loss:     0.9870 Validation Accuracy: 0.725000\n",
      "Epoch 239, CIFAR-10 Batch 5:  Loss:     0.9069 Validation Accuracy: 0.750000\n",
      "Epoch 240, CIFAR-10 Batch 1:  Loss:     0.9948 Validation Accuracy: 0.700000\n",
      "Epoch 240, CIFAR-10 Batch 2:  Loss:     0.6207 Validation Accuracy: 0.850000\n",
      "Epoch 240, CIFAR-10 Batch 3:  Loss:     0.5664 Validation Accuracy: 0.900000\n",
      "Epoch 240, CIFAR-10 Batch 4:  Loss:     0.9825 Validation Accuracy: 0.725000\n",
      "Epoch 240, CIFAR-10 Batch 5:  Loss:     0.8994 Validation Accuracy: 0.800000\n",
      "Epoch 241, CIFAR-10 Batch 1:  Loss:     0.9935 Validation Accuracy: 0.725000\n",
      "Epoch 241, CIFAR-10 Batch 2:  Loss:     0.6158 Validation Accuracy: 0.850000\n",
      "Epoch 241, CIFAR-10 Batch 3:  Loss:     0.5656 Validation Accuracy: 0.875000\n",
      "Epoch 241, CIFAR-10 Batch 4:  Loss:     0.9900 Validation Accuracy: 0.725000\n",
      "Epoch 241, CIFAR-10 Batch 5:  Loss:     0.9037 Validation Accuracy: 0.800000\n",
      "Epoch 242, CIFAR-10 Batch 1:  Loss:     0.9915 Validation Accuracy: 0.700000\n",
      "Epoch 242, CIFAR-10 Batch 2:  Loss:     0.6244 Validation Accuracy: 0.850000\n",
      "Epoch 242, CIFAR-10 Batch 3:  Loss:     0.5627 Validation Accuracy: 0.900000\n",
      "Epoch 242, CIFAR-10 Batch 4:  Loss:     0.9826 Validation Accuracy: 0.700000\n",
      "Epoch 242, CIFAR-10 Batch 5:  Loss:     0.9019 Validation Accuracy: 0.800000\n",
      "Epoch 243, CIFAR-10 Batch 1:  Loss:     0.9913 Validation Accuracy: 0.700000\n",
      "Epoch 243, CIFAR-10 Batch 2:  Loss:     0.6223 Validation Accuracy: 0.850000\n",
      "Epoch 243, CIFAR-10 Batch 3:  Loss:     0.5683 Validation Accuracy: 0.875000\n",
      "Epoch 243, CIFAR-10 Batch 4:  Loss:     0.9822 Validation Accuracy: 0.700000\n",
      "Epoch 243, CIFAR-10 Batch 5:  Loss:     0.9001 Validation Accuracy: 0.775000\n",
      "Epoch 244, CIFAR-10 Batch 1:  Loss:     0.9940 Validation Accuracy: 0.700000\n",
      "Epoch 244, CIFAR-10 Batch 2:  Loss:     0.6237 Validation Accuracy: 0.850000\n",
      "Epoch 244, CIFAR-10 Batch 3:  Loss:     0.5651 Validation Accuracy: 0.900000\n",
      "Epoch 244, CIFAR-10 Batch 4:  Loss:     0.9854 Validation Accuracy: 0.700000\n",
      "Epoch 244, CIFAR-10 Batch 5:  Loss:     0.8981 Validation Accuracy: 0.775000\n",
      "Epoch 245, CIFAR-10 Batch 1:  Loss:     0.9871 Validation Accuracy: 0.700000\n",
      "Epoch 245, CIFAR-10 Batch 2:  Loss:     0.6154 Validation Accuracy: 0.850000\n",
      "Epoch 245, CIFAR-10 Batch 3:  Loss:     0.5674 Validation Accuracy: 0.875000\n",
      "Epoch 245, CIFAR-10 Batch 4:  Loss:     0.9837 Validation Accuracy: 0.700000\n",
      "Epoch 245, CIFAR-10 Batch 5:  Loss:     0.8980 Validation Accuracy: 0.775000\n",
      "Epoch 246, CIFAR-10 Batch 1:  Loss:     0.9983 Validation Accuracy: 0.700000\n",
      "Epoch 246, CIFAR-10 Batch 2:  Loss:     0.6210 Validation Accuracy: 0.850000\n",
      "Epoch 246, CIFAR-10 Batch 3:  Loss:     0.5700 Validation Accuracy: 0.875000\n",
      "Epoch 246, CIFAR-10 Batch 4:  Loss:     0.9897 Validation Accuracy: 0.700000\n",
      "Epoch 246, CIFAR-10 Batch 5:  Loss:     0.9006 Validation Accuracy: 0.800000\n",
      "Epoch 247, CIFAR-10 Batch 1:  Loss:     0.9937 Validation Accuracy: 0.700000\n",
      "Epoch 247, CIFAR-10 Batch 2:  Loss:     0.6202 Validation Accuracy: 0.875000\n",
      "Epoch 247, CIFAR-10 Batch 3:  Loss:     0.5630 Validation Accuracy: 0.875000\n",
      "Epoch 247, CIFAR-10 Batch 4:  Loss:     0.9852 Validation Accuracy: 0.700000\n",
      "Epoch 247, CIFAR-10 Batch 5:  Loss:     0.8970 Validation Accuracy: 0.800000\n",
      "Epoch 248, CIFAR-10 Batch 1:  Loss:     1.0029 Validation Accuracy: 0.700000\n",
      "Epoch 248, CIFAR-10 Batch 2:  Loss:     0.6226 Validation Accuracy: 0.850000\n",
      "Epoch 248, CIFAR-10 Batch 3:  Loss:     0.5646 Validation Accuracy: 0.900000\n",
      "Epoch 248, CIFAR-10 Batch 4:  Loss:     0.9843 Validation Accuracy: 0.700000\n",
      "Epoch 248, CIFAR-10 Batch 5:  Loss:     0.8965 Validation Accuracy: 0.800000\n",
      "Epoch 249, CIFAR-10 Batch 1:  Loss:     0.9961 Validation Accuracy: 0.700000\n",
      "Epoch 249, CIFAR-10 Batch 2:  Loss:     0.6235 Validation Accuracy: 0.850000\n",
      "Epoch 249, CIFAR-10 Batch 3:  Loss:     0.5643 Validation Accuracy: 0.900000\n",
      "Epoch 249, CIFAR-10 Batch 4:  Loss:     0.9847 Validation Accuracy: 0.700000\n",
      "Epoch 249, CIFAR-10 Batch 5:  Loss:     0.8951 Validation Accuracy: 0.800000\n",
      "Epoch 250, CIFAR-10 Batch 1:  Loss:     0.9957 Validation Accuracy: 0.675000\n",
      "Epoch 250, CIFAR-10 Batch 2:  Loss:     0.6239 Validation Accuracy: 0.850000\n",
      "Epoch 250, CIFAR-10 Batch 3:  Loss:     0.5698 Validation Accuracy: 0.875000\n",
      "Epoch 250, CIFAR-10 Batch 4:  Loss:     0.9795 Validation Accuracy: 0.700000\n",
      "Epoch 250, CIFAR-10 Batch 5:  Loss:     0.8996 Validation Accuracy: 0.775000\n",
      "Epoch 251, CIFAR-10 Batch 1:  Loss:     0.9991 Validation Accuracy: 0.675000\n",
      "Epoch 251, CIFAR-10 Batch 2:  Loss:     0.6216 Validation Accuracy: 0.850000\n",
      "Epoch 251, CIFAR-10 Batch 3:  Loss:     0.5675 Validation Accuracy: 0.875000\n",
      "Epoch 251, CIFAR-10 Batch 4:  Loss:     0.9842 Validation Accuracy: 0.725000\n",
      "Epoch 251, CIFAR-10 Batch 5:  Loss:     0.8961 Validation Accuracy: 0.800000\n",
      "Epoch 252, CIFAR-10 Batch 1:  Loss:     0.9990 Validation Accuracy: 0.700000\n",
      "Epoch 252, CIFAR-10 Batch 2:  Loss:     0.6196 Validation Accuracy: 0.850000\n",
      "Epoch 252, CIFAR-10 Batch 3:  Loss:     0.5593 Validation Accuracy: 0.875000\n",
      "Epoch 252, CIFAR-10 Batch 4:  Loss:     0.9812 Validation Accuracy: 0.725000\n",
      "Epoch 252, CIFAR-10 Batch 5:  Loss:     0.8945 Validation Accuracy: 0.800000\n",
      "Epoch 253, CIFAR-10 Batch 1:  Loss:     1.0002 Validation Accuracy: 0.675000\n",
      "Epoch 253, CIFAR-10 Batch 2:  Loss:     0.6156 Validation Accuracy: 0.850000\n",
      "Epoch 253, CIFAR-10 Batch 3:  Loss:     0.5604 Validation Accuracy: 0.900000\n",
      "Epoch 253, CIFAR-10 Batch 4:  Loss:     0.9860 Validation Accuracy: 0.725000\n",
      "Epoch 253, CIFAR-10 Batch 5:  Loss:     0.8933 Validation Accuracy: 0.800000\n",
      "Epoch 254, CIFAR-10 Batch 1:  Loss:     0.9950 Validation Accuracy: 0.675000\n",
      "Epoch 254, CIFAR-10 Batch 2:  Loss:     0.6206 Validation Accuracy: 0.850000\n",
      "Epoch 254, CIFAR-10 Batch 3:  Loss:     0.5610 Validation Accuracy: 0.875000\n",
      "Epoch 254, CIFAR-10 Batch 4:  Loss:     0.9811 Validation Accuracy: 0.725000\n",
      "Epoch 254, CIFAR-10 Batch 5:  Loss:     0.8974 Validation Accuracy: 0.825000\n",
      "Epoch 255, CIFAR-10 Batch 1:  Loss:     0.9970 Validation Accuracy: 0.700000\n",
      "Epoch 255, CIFAR-10 Batch 2:  Loss:     0.6119 Validation Accuracy: 0.825000\n",
      "Epoch 255, CIFAR-10 Batch 3:  Loss:     0.5617 Validation Accuracy: 0.900000\n",
      "Epoch 255, CIFAR-10 Batch 4:  Loss:     0.9843 Validation Accuracy: 0.725000\n",
      "Epoch 255, CIFAR-10 Batch 5:  Loss:     0.8986 Validation Accuracy: 0.800000\n",
      "Epoch 256, CIFAR-10 Batch 1:  Loss:     0.9966 Validation Accuracy: 0.675000\n",
      "Epoch 256, CIFAR-10 Batch 2:  Loss:     0.6192 Validation Accuracy: 0.850000\n",
      "Epoch 256, CIFAR-10 Batch 3:  Loss:     0.5615 Validation Accuracy: 0.875000\n",
      "Epoch 256, CIFAR-10 Batch 4:  Loss:     0.9843 Validation Accuracy: 0.725000\n",
      "Epoch 256, CIFAR-10 Batch 5:  Loss:     0.8963 Validation Accuracy: 0.800000\n",
      "Epoch 257, CIFAR-10 Batch 1:  Loss:     0.9987 Validation Accuracy: 0.675000\n",
      "Epoch 257, CIFAR-10 Batch 2:  Loss:     0.6134 Validation Accuracy: 0.850000\n",
      "Epoch 257, CIFAR-10 Batch 3:  Loss:     0.5618 Validation Accuracy: 0.875000\n",
      "Epoch 257, CIFAR-10 Batch 4:  Loss:     0.9751 Validation Accuracy: 0.725000\n",
      "Epoch 257, CIFAR-10 Batch 5:  Loss:     0.8963 Validation Accuracy: 0.800000\n",
      "Epoch 258, CIFAR-10 Batch 1:  Loss:     0.9981 Validation Accuracy: 0.700000\n",
      "Epoch 258, CIFAR-10 Batch 2:  Loss:     0.6099 Validation Accuracy: 0.850000\n",
      "Epoch 258, CIFAR-10 Batch 3:  Loss:     0.5605 Validation Accuracy: 0.875000\n",
      "Epoch 258, CIFAR-10 Batch 4:  Loss:     0.9847 Validation Accuracy: 0.725000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 258, CIFAR-10 Batch 5:  Loss:     0.8956 Validation Accuracy: 0.800000\n",
      "Epoch 259, CIFAR-10 Batch 1:  Loss:     0.9964 Validation Accuracy: 0.700000\n",
      "Epoch 259, CIFAR-10 Batch 2:  Loss:     0.6135 Validation Accuracy: 0.850000\n",
      "Epoch 259, CIFAR-10 Batch 3:  Loss:     0.5570 Validation Accuracy: 0.900000\n",
      "Epoch 259, CIFAR-10 Batch 4:  Loss:     0.9810 Validation Accuracy: 0.725000\n",
      "Epoch 259, CIFAR-10 Batch 5:  Loss:     0.8923 Validation Accuracy: 0.800000\n",
      "Epoch 260, CIFAR-10 Batch 1:  Loss:     1.0043 Validation Accuracy: 0.675000\n",
      "Epoch 260, CIFAR-10 Batch 2:  Loss:     0.6113 Validation Accuracy: 0.850000\n",
      "Epoch 260, CIFAR-10 Batch 3:  Loss:     0.5620 Validation Accuracy: 0.900000\n",
      "Epoch 260, CIFAR-10 Batch 4:  Loss:     0.9831 Validation Accuracy: 0.700000\n",
      "Epoch 260, CIFAR-10 Batch 5:  Loss:     0.8918 Validation Accuracy: 0.800000\n",
      "Epoch 261, CIFAR-10 Batch 1:  Loss:     1.0007 Validation Accuracy: 0.700000\n",
      "Epoch 261, CIFAR-10 Batch 2:  Loss:     0.6136 Validation Accuracy: 0.850000\n",
      "Epoch 261, CIFAR-10 Batch 3:  Loss:     0.5637 Validation Accuracy: 0.875000\n",
      "Epoch 261, CIFAR-10 Batch 4:  Loss:     0.9800 Validation Accuracy: 0.725000\n",
      "Epoch 261, CIFAR-10 Batch 5:  Loss:     0.8944 Validation Accuracy: 0.800000\n",
      "Epoch 262, CIFAR-10 Batch 1:  Loss:     0.9850 Validation Accuracy: 0.700000\n",
      "Epoch 262, CIFAR-10 Batch 2:  Loss:     0.6133 Validation Accuracy: 0.850000\n",
      "Epoch 262, CIFAR-10 Batch 3:  Loss:     0.5591 Validation Accuracy: 0.900000\n",
      "Epoch 262, CIFAR-10 Batch 4:  Loss:     0.9788 Validation Accuracy: 0.725000\n",
      "Epoch 262, CIFAR-10 Batch 5:  Loss:     0.8937 Validation Accuracy: 0.775000\n",
      "Epoch 263, CIFAR-10 Batch 1:  Loss:     0.9925 Validation Accuracy: 0.700000\n",
      "Epoch 263, CIFAR-10 Batch 2:  Loss:     0.6148 Validation Accuracy: 0.825000\n",
      "Epoch 263, CIFAR-10 Batch 3:  Loss:     0.5643 Validation Accuracy: 0.875000\n",
      "Epoch 263, CIFAR-10 Batch 4:  Loss:     0.9787 Validation Accuracy: 0.725000\n",
      "Epoch 263, CIFAR-10 Batch 5:  Loss:     0.8944 Validation Accuracy: 0.775000\n",
      "Epoch 264, CIFAR-10 Batch 1:  Loss:     0.9953 Validation Accuracy: 0.675000\n",
      "Epoch 264, CIFAR-10 Batch 2:  Loss:     0.6155 Validation Accuracy: 0.850000\n",
      "Epoch 264, CIFAR-10 Batch 3:  Loss:     0.5619 Validation Accuracy: 0.900000\n",
      "Epoch 264, CIFAR-10 Batch 4:  Loss:     0.9816 Validation Accuracy: 0.725000\n",
      "Epoch 264, CIFAR-10 Batch 5:  Loss:     0.8936 Validation Accuracy: 0.775000\n",
      "Epoch 265, CIFAR-10 Batch 1:  Loss:     0.9881 Validation Accuracy: 0.700000\n",
      "Epoch 265, CIFAR-10 Batch 2:  Loss:     0.6254 Validation Accuracy: 0.825000\n",
      "Epoch 265, CIFAR-10 Batch 3:  Loss:     0.5615 Validation Accuracy: 0.875000\n",
      "Epoch 265, CIFAR-10 Batch 4:  Loss:     0.9748 Validation Accuracy: 0.725000\n",
      "Epoch 265, CIFAR-10 Batch 5:  Loss:     0.8989 Validation Accuracy: 0.750000\n",
      "Epoch 266, CIFAR-10 Batch 1:  Loss:     1.0013 Validation Accuracy: 0.700000\n",
      "Epoch 266, CIFAR-10 Batch 2:  Loss:     0.6187 Validation Accuracy: 0.850000\n",
      "Epoch 266, CIFAR-10 Batch 3:  Loss:     0.5660 Validation Accuracy: 0.900000\n",
      "Epoch 266, CIFAR-10 Batch 4:  Loss:     0.9806 Validation Accuracy: 0.725000\n",
      "Epoch 266, CIFAR-10 Batch 5:  Loss:     0.8966 Validation Accuracy: 0.775000\n",
      "Epoch 267, CIFAR-10 Batch 1:  Loss:     0.9862 Validation Accuracy: 0.700000\n",
      "Epoch 267, CIFAR-10 Batch 2:  Loss:     0.6171 Validation Accuracy: 0.850000\n",
      "Epoch 267, CIFAR-10 Batch 3:  Loss:     0.5610 Validation Accuracy: 0.900000\n",
      "Epoch 267, CIFAR-10 Batch 4:  Loss:     0.9789 Validation Accuracy: 0.725000\n",
      "Epoch 267, CIFAR-10 Batch 5:  Loss:     0.8948 Validation Accuracy: 0.775000\n",
      "Epoch 268, CIFAR-10 Batch 1:  Loss:     0.9929 Validation Accuracy: 0.700000\n",
      "Epoch 268, CIFAR-10 Batch 2:  Loss:     0.6240 Validation Accuracy: 0.850000\n",
      "Epoch 268, CIFAR-10 Batch 3:  Loss:     0.5653 Validation Accuracy: 0.875000\n",
      "Epoch 268, CIFAR-10 Batch 4:  Loss:     0.9788 Validation Accuracy: 0.725000\n",
      "Epoch 268, CIFAR-10 Batch 5:  Loss:     0.8948 Validation Accuracy: 0.800000\n",
      "Epoch 269, CIFAR-10 Batch 1:  Loss:     0.9957 Validation Accuracy: 0.675000\n",
      "Epoch 269, CIFAR-10 Batch 2:  Loss:     0.6203 Validation Accuracy: 0.850000\n",
      "Epoch 269, CIFAR-10 Batch 3:  Loss:     0.5614 Validation Accuracy: 0.900000\n",
      "Epoch 269, CIFAR-10 Batch 4:  Loss:     0.9825 Validation Accuracy: 0.725000\n",
      "Epoch 269, CIFAR-10 Batch 5:  Loss:     0.8971 Validation Accuracy: 0.800000\n",
      "Epoch 270, CIFAR-10 Batch 1:  Loss:     0.9957 Validation Accuracy: 0.700000\n",
      "Epoch 270, CIFAR-10 Batch 2:  Loss:     0.6094 Validation Accuracy: 0.850000\n",
      "Epoch 270, CIFAR-10 Batch 3:  Loss:     0.5595 Validation Accuracy: 0.900000\n",
      "Epoch 270, CIFAR-10 Batch 4:  Loss:     0.9819 Validation Accuracy: 0.725000\n",
      "Epoch 270, CIFAR-10 Batch 5:  Loss:     0.8944 Validation Accuracy: 0.800000\n",
      "Epoch 271, CIFAR-10 Batch 1:  Loss:     0.9942 Validation Accuracy: 0.700000\n",
      "Epoch 271, CIFAR-10 Batch 2:  Loss:     0.6102 Validation Accuracy: 0.850000\n",
      "Epoch 271, CIFAR-10 Batch 3:  Loss:     0.5591 Validation Accuracy: 0.900000\n",
      "Epoch 271, CIFAR-10 Batch 4:  Loss:     0.9857 Validation Accuracy: 0.750000\n",
      "Epoch 271, CIFAR-10 Batch 5:  Loss:     0.8954 Validation Accuracy: 0.775000\n",
      "Epoch 272, CIFAR-10 Batch 1:  Loss:     0.9903 Validation Accuracy: 0.700000\n",
      "Epoch 272, CIFAR-10 Batch 2:  Loss:     0.6166 Validation Accuracy: 0.825000\n",
      "Epoch 272, CIFAR-10 Batch 3:  Loss:     0.5549 Validation Accuracy: 0.900000\n",
      "Epoch 272, CIFAR-10 Batch 4:  Loss:     0.9756 Validation Accuracy: 0.750000\n",
      "Epoch 272, CIFAR-10 Batch 5:  Loss:     0.8953 Validation Accuracy: 0.775000\n",
      "Epoch 273, CIFAR-10 Batch 1:  Loss:     0.9895 Validation Accuracy: 0.700000\n",
      "Epoch 273, CIFAR-10 Batch 2:  Loss:     0.6145 Validation Accuracy: 0.850000\n",
      "Epoch 273, CIFAR-10 Batch 3:  Loss:     0.5572 Validation Accuracy: 0.875000\n",
      "Epoch 273, CIFAR-10 Batch 4:  Loss:     0.9792 Validation Accuracy: 0.750000\n",
      "Epoch 273, CIFAR-10 Batch 5:  Loss:     0.8956 Validation Accuracy: 0.800000\n",
      "Epoch 274, CIFAR-10 Batch 1:  Loss:     0.9947 Validation Accuracy: 0.700000\n",
      "Epoch 274, CIFAR-10 Batch 2:  Loss:     0.6229 Validation Accuracy: 0.850000\n",
      "Epoch 274, CIFAR-10 Batch 3:  Loss:     0.5576 Validation Accuracy: 0.900000\n",
      "Epoch 274, CIFAR-10 Batch 4:  Loss:     0.9776 Validation Accuracy: 0.725000\n",
      "Epoch 274, CIFAR-10 Batch 5:  Loss:     0.8952 Validation Accuracy: 0.775000\n",
      "Epoch 275, CIFAR-10 Batch 1:  Loss:     0.9863 Validation Accuracy: 0.700000\n",
      "Epoch 275, CIFAR-10 Batch 2:  Loss:     0.6128 Validation Accuracy: 0.850000\n",
      "Epoch 275, CIFAR-10 Batch 3:  Loss:     0.5584 Validation Accuracy: 0.900000\n",
      "Epoch 275, CIFAR-10 Batch 4:  Loss:     0.9790 Validation Accuracy: 0.725000\n",
      "Epoch 275, CIFAR-10 Batch 5:  Loss:     0.8939 Validation Accuracy: 0.775000\n",
      "Epoch 276, CIFAR-10 Batch 1:  Loss:     0.9877 Validation Accuracy: 0.700000\n",
      "Epoch 276, CIFAR-10 Batch 2:  Loss:     0.6063 Validation Accuracy: 0.850000\n",
      "Epoch 276, CIFAR-10 Batch 3:  Loss:     0.5557 Validation Accuracy: 0.900000\n",
      "Epoch 276, CIFAR-10 Batch 4:  Loss:     0.9767 Validation Accuracy: 0.725000\n",
      "Epoch 276, CIFAR-10 Batch 5:  Loss:     0.8957 Validation Accuracy: 0.775000\n",
      "Epoch 277, CIFAR-10 Batch 1:  Loss:     0.9863 Validation Accuracy: 0.675000\n",
      "Epoch 277, CIFAR-10 Batch 2:  Loss:     0.6108 Validation Accuracy: 0.825000\n",
      "Epoch 277, CIFAR-10 Batch 3:  Loss:     0.5585 Validation Accuracy: 0.900000\n",
      "Epoch 277, CIFAR-10 Batch 4:  Loss:     0.9781 Validation Accuracy: 0.700000\n",
      "Epoch 277, CIFAR-10 Batch 5:  Loss:     0.8941 Validation Accuracy: 0.775000\n",
      "Epoch 278, CIFAR-10 Batch 1:  Loss:     0.9845 Validation Accuracy: 0.700000\n",
      "Epoch 278, CIFAR-10 Batch 2:  Loss:     0.6089 Validation Accuracy: 0.850000\n",
      "Epoch 278, CIFAR-10 Batch 3:  Loss:     0.5613 Validation Accuracy: 0.875000\n",
      "Epoch 278, CIFAR-10 Batch 4:  Loss:     0.9788 Validation Accuracy: 0.700000\n",
      "Epoch 278, CIFAR-10 Batch 5:  Loss:     0.8987 Validation Accuracy: 0.800000\n",
      "Epoch 279, CIFAR-10 Batch 1:  Loss:     0.9923 Validation Accuracy: 0.700000\n",
      "Epoch 279, CIFAR-10 Batch 2:  Loss:     0.6036 Validation Accuracy: 0.850000\n",
      "Epoch 279, CIFAR-10 Batch 3:  Loss:     0.5568 Validation Accuracy: 0.900000\n",
      "Epoch 279, CIFAR-10 Batch 4:  Loss:     0.9828 Validation Accuracy: 0.725000\n",
      "Epoch 279, CIFAR-10 Batch 5:  Loss:     0.8998 Validation Accuracy: 0.775000\n",
      "Epoch 280, CIFAR-10 Batch 1:  Loss:     0.9854 Validation Accuracy: 0.700000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 280, CIFAR-10 Batch 2:  Loss:     0.6048 Validation Accuracy: 0.850000\n",
      "Epoch 280, CIFAR-10 Batch 3:  Loss:     0.5553 Validation Accuracy: 0.900000\n",
      "Epoch 280, CIFAR-10 Batch 4:  Loss:     0.9858 Validation Accuracy: 0.700000\n",
      "Epoch 280, CIFAR-10 Batch 5:  Loss:     0.9010 Validation Accuracy: 0.775000\n",
      "Epoch 281, CIFAR-10 Batch 1:  Loss:     0.9831 Validation Accuracy: 0.675000\n",
      "Epoch 281, CIFAR-10 Batch 2:  Loss:     0.6140 Validation Accuracy: 0.850000\n",
      "Epoch 281, CIFAR-10 Batch 3:  Loss:     0.5585 Validation Accuracy: 0.900000\n",
      "Epoch 281, CIFAR-10 Batch 4:  Loss:     0.9771 Validation Accuracy: 0.725000\n",
      "Epoch 281, CIFAR-10 Batch 5:  Loss:     0.8997 Validation Accuracy: 0.775000\n",
      "Epoch 282, CIFAR-10 Batch 1:  Loss:     0.9838 Validation Accuracy: 0.725000\n",
      "Epoch 282, CIFAR-10 Batch 2:  Loss:     0.6154 Validation Accuracy: 0.850000\n",
      "Epoch 282, CIFAR-10 Batch 3:  Loss:     0.5602 Validation Accuracy: 0.875000\n",
      "Epoch 282, CIFAR-10 Batch 4:  Loss:     0.9785 Validation Accuracy: 0.700000\n",
      "Epoch 282, CIFAR-10 Batch 5:  Loss:     0.9016 Validation Accuracy: 0.775000\n",
      "Epoch 283, CIFAR-10 Batch 1:  Loss:     0.9795 Validation Accuracy: 0.700000\n",
      "Epoch 283, CIFAR-10 Batch 2:  Loss:     0.6102 Validation Accuracy: 0.850000\n",
      "Epoch 283, CIFAR-10 Batch 3:  Loss:     0.5594 Validation Accuracy: 0.900000\n",
      "Epoch 283, CIFAR-10 Batch 4:  Loss:     0.9818 Validation Accuracy: 0.700000\n",
      "Epoch 283, CIFAR-10 Batch 5:  Loss:     0.8968 Validation Accuracy: 0.775000\n",
      "Epoch 284, CIFAR-10 Batch 1:  Loss:     0.9836 Validation Accuracy: 0.725000\n",
      "Epoch 284, CIFAR-10 Batch 2:  Loss:     0.6065 Validation Accuracy: 0.850000\n",
      "Epoch 284, CIFAR-10 Batch 3:  Loss:     0.5648 Validation Accuracy: 0.875000\n",
      "Epoch 284, CIFAR-10 Batch 4:  Loss:     0.9832 Validation Accuracy: 0.700000\n",
      "Epoch 284, CIFAR-10 Batch 5:  Loss:     0.9021 Validation Accuracy: 0.775000\n",
      "Epoch 285, CIFAR-10 Batch 1:  Loss:     0.9842 Validation Accuracy: 0.700000\n",
      "Epoch 285, CIFAR-10 Batch 2:  Loss:     0.6095 Validation Accuracy: 0.850000\n",
      "Epoch 285, CIFAR-10 Batch 3:  Loss:     0.5569 Validation Accuracy: 0.900000\n",
      "Epoch 285, CIFAR-10 Batch 4:  Loss:     0.9800 Validation Accuracy: 0.700000\n",
      "Epoch 285, CIFAR-10 Batch 5:  Loss:     0.9026 Validation Accuracy: 0.775000\n",
      "Epoch 286, CIFAR-10 Batch 1:  Loss:     0.9821 Validation Accuracy: 0.700000\n",
      "Epoch 286, CIFAR-10 Batch 2:  Loss:     0.6054 Validation Accuracy: 0.850000\n",
      "Epoch 286, CIFAR-10 Batch 3:  Loss:     0.5535 Validation Accuracy: 0.900000\n",
      "Epoch 286, CIFAR-10 Batch 4:  Loss:     0.9761 Validation Accuracy: 0.700000\n",
      "Epoch 286, CIFAR-10 Batch 5:  Loss:     0.9000 Validation Accuracy: 0.775000\n",
      "Epoch 287, CIFAR-10 Batch 1:  Loss:     0.9831 Validation Accuracy: 0.725000\n",
      "Epoch 287, CIFAR-10 Batch 2:  Loss:     0.6052 Validation Accuracy: 0.850000\n",
      "Epoch 287, CIFAR-10 Batch 3:  Loss:     0.5558 Validation Accuracy: 0.900000\n",
      "Epoch 287, CIFAR-10 Batch 4:  Loss:     0.9716 Validation Accuracy: 0.700000\n",
      "Epoch 287, CIFAR-10 Batch 5:  Loss:     0.8967 Validation Accuracy: 0.800000\n",
      "Epoch 288, CIFAR-10 Batch 1:  Loss:     0.9827 Validation Accuracy: 0.725000\n",
      "Epoch 288, CIFAR-10 Batch 2:  Loss:     0.6026 Validation Accuracy: 0.850000\n",
      "Epoch 288, CIFAR-10 Batch 3:  Loss:     0.5601 Validation Accuracy: 0.900000\n",
      "Epoch 288, CIFAR-10 Batch 4:  Loss:     0.9792 Validation Accuracy: 0.700000\n",
      "Epoch 288, CIFAR-10 Batch 5:  Loss:     0.8969 Validation Accuracy: 0.775000\n",
      "Epoch 289, CIFAR-10 Batch 1:  Loss:     0.9827 Validation Accuracy: 0.675000\n",
      "Epoch 289, CIFAR-10 Batch 2:  Loss:     0.6019 Validation Accuracy: 0.850000\n",
      "Epoch 289, CIFAR-10 Batch 3:  Loss:     0.5529 Validation Accuracy: 0.900000\n",
      "Epoch 289, CIFAR-10 Batch 4:  Loss:     0.9694 Validation Accuracy: 0.700000\n",
      "Epoch 289, CIFAR-10 Batch 5:  Loss:     0.9015 Validation Accuracy: 0.775000\n",
      "Epoch 290, CIFAR-10 Batch 1:  Loss:     0.9751 Validation Accuracy: 0.725000\n",
      "Epoch 290, CIFAR-10 Batch 2:  Loss:     0.6007 Validation Accuracy: 0.850000\n",
      "Epoch 290, CIFAR-10 Batch 3:  Loss:     0.5545 Validation Accuracy: 0.900000\n",
      "Epoch 290, CIFAR-10 Batch 4:  Loss:     0.9746 Validation Accuracy: 0.700000\n",
      "Epoch 290, CIFAR-10 Batch 5:  Loss:     0.8984 Validation Accuracy: 0.800000\n",
      "Epoch 291, CIFAR-10 Batch 1:  Loss:     0.9799 Validation Accuracy: 0.725000\n",
      "Epoch 291, CIFAR-10 Batch 2:  Loss:     0.5988 Validation Accuracy: 0.850000\n",
      "Epoch 291, CIFAR-10 Batch 3:  Loss:     0.5559 Validation Accuracy: 0.900000\n",
      "Epoch 291, CIFAR-10 Batch 4:  Loss:     0.9707 Validation Accuracy: 0.725000\n",
      "Epoch 291, CIFAR-10 Batch 5:  Loss:     0.8971 Validation Accuracy: 0.775000\n",
      "Epoch 292, CIFAR-10 Batch 1:  Loss:     0.9853 Validation Accuracy: 0.700000\n",
      "Epoch 292, CIFAR-10 Batch 2:  Loss:     0.5996 Validation Accuracy: 0.850000\n",
      "Epoch 292, CIFAR-10 Batch 3:  Loss:     0.5600 Validation Accuracy: 0.900000\n",
      "Epoch 292, CIFAR-10 Batch 4:  Loss:     0.9734 Validation Accuracy: 0.700000\n",
      "Epoch 292, CIFAR-10 Batch 5:  Loss:     0.8981 Validation Accuracy: 0.775000\n",
      "Epoch 293, CIFAR-10 Batch 1:  Loss:     0.9885 Validation Accuracy: 0.700000\n",
      "Epoch 293, CIFAR-10 Batch 2:  Loss:     0.6100 Validation Accuracy: 0.850000\n",
      "Epoch 293, CIFAR-10 Batch 3:  Loss:     0.5529 Validation Accuracy: 0.900000\n",
      "Epoch 293, CIFAR-10 Batch 4:  Loss:     0.9743 Validation Accuracy: 0.725000\n",
      "Epoch 293, CIFAR-10 Batch 5:  Loss:     0.8945 Validation Accuracy: 0.800000\n",
      "Epoch 294, CIFAR-10 Batch 1:  Loss:     0.9765 Validation Accuracy: 0.675000\n",
      "Epoch 294, CIFAR-10 Batch 2:  Loss:     0.5928 Validation Accuracy: 0.850000\n",
      "Epoch 294, CIFAR-10 Batch 3:  Loss:     0.5552 Validation Accuracy: 0.900000\n",
      "Epoch 294, CIFAR-10 Batch 4:  Loss:     0.9723 Validation Accuracy: 0.700000\n",
      "Epoch 294, CIFAR-10 Batch 5:  Loss:     0.8959 Validation Accuracy: 0.775000\n",
      "Epoch 295, CIFAR-10 Batch 1:  Loss:     0.9755 Validation Accuracy: 0.725000\n",
      "Epoch 295, CIFAR-10 Batch 2:  Loss:     0.5983 Validation Accuracy: 0.850000\n",
      "Epoch 295, CIFAR-10 Batch 3:  Loss:     0.5600 Validation Accuracy: 0.875000\n",
      "Epoch 295, CIFAR-10 Batch 4:  Loss:     0.9701 Validation Accuracy: 0.700000\n",
      "Epoch 295, CIFAR-10 Batch 5:  Loss:     0.8960 Validation Accuracy: 0.775000\n",
      "Epoch 296, CIFAR-10 Batch 1:  Loss:     0.9777 Validation Accuracy: 0.700000\n",
      "Epoch 296, CIFAR-10 Batch 2:  Loss:     0.6068 Validation Accuracy: 0.850000\n",
      "Epoch 296, CIFAR-10 Batch 3:  Loss:     0.5564 Validation Accuracy: 0.900000\n",
      "Epoch 296, CIFAR-10 Batch 4:  Loss:     0.9728 Validation Accuracy: 0.700000\n",
      "Epoch 296, CIFAR-10 Batch 5:  Loss:     0.8999 Validation Accuracy: 0.775000\n",
      "Epoch 297, CIFAR-10 Batch 1:  Loss:     0.9782 Validation Accuracy: 0.675000\n",
      "Epoch 297, CIFAR-10 Batch 2:  Loss:     0.6033 Validation Accuracy: 0.850000\n",
      "Epoch 297, CIFAR-10 Batch 3:  Loss:     0.5540 Validation Accuracy: 0.900000\n",
      "Epoch 297, CIFAR-10 Batch 4:  Loss:     0.9697 Validation Accuracy: 0.700000\n",
      "Epoch 297, CIFAR-10 Batch 5:  Loss:     0.8983 Validation Accuracy: 0.775000\n",
      "Epoch 298, CIFAR-10 Batch 1:  Loss:     0.9884 Validation Accuracy: 0.675000\n",
      "Epoch 298, CIFAR-10 Batch 2:  Loss:     0.6078 Validation Accuracy: 0.825000\n",
      "Epoch 298, CIFAR-10 Batch 3:  Loss:     0.5578 Validation Accuracy: 0.900000\n",
      "Epoch 298, CIFAR-10 Batch 4:  Loss:     0.9737 Validation Accuracy: 0.700000\n",
      "Epoch 298, CIFAR-10 Batch 5:  Loss:     0.8982 Validation Accuracy: 0.775000\n",
      "Epoch 299, CIFAR-10 Batch 1:  Loss:     0.9790 Validation Accuracy: 0.675000\n",
      "Epoch 299, CIFAR-10 Batch 2:  Loss:     0.5947 Validation Accuracy: 0.850000\n",
      "Epoch 299, CIFAR-10 Batch 3:  Loss:     0.5537 Validation Accuracy: 0.900000\n",
      "Epoch 299, CIFAR-10 Batch 4:  Loss:     0.9727 Validation Accuracy: 0.675000\n",
      "Epoch 299, CIFAR-10 Batch 5:  Loss:     0.8904 Validation Accuracy: 0.775000\n",
      "Epoch 300, CIFAR-10 Batch 1:  Loss:     0.9784 Validation Accuracy: 0.675000\n",
      "Epoch 300, CIFAR-10 Batch 2:  Loss:     0.5974 Validation Accuracy: 0.850000\n",
      "Epoch 300, CIFAR-10 Batch 3:  Loss:     0.5541 Validation Accuracy: 0.900000\n",
      "Epoch 300, CIFAR-10 Batch 4:  Loss:     0.9683 Validation Accuracy: 0.675000\n",
      "Epoch 300, CIFAR-10 Batch 5:  Loss:     0.8901 Validation Accuracy: 0.775000\n",
      "Epoch 301, CIFAR-10 Batch 1:  Loss:     0.9804 Validation Accuracy: 0.675000\n",
      "Epoch 301, CIFAR-10 Batch 2:  Loss:     0.5949 Validation Accuracy: 0.850000\n",
      "Epoch 301, CIFAR-10 Batch 3:  Loss:     0.5557 Validation Accuracy: 0.900000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 301, CIFAR-10 Batch 4:  Loss:     0.9766 Validation Accuracy: 0.700000\n",
      "Epoch 301, CIFAR-10 Batch 5:  Loss:     0.8908 Validation Accuracy: 0.775000\n",
      "Epoch 302, CIFAR-10 Batch 1:  Loss:     0.9785 Validation Accuracy: 0.700000\n",
      "Epoch 302, CIFAR-10 Batch 2:  Loss:     0.5950 Validation Accuracy: 0.850000\n",
      "Epoch 302, CIFAR-10 Batch 3:  Loss:     0.5589 Validation Accuracy: 0.900000\n",
      "Epoch 302, CIFAR-10 Batch 4:  Loss:     0.9679 Validation Accuracy: 0.700000\n",
      "Epoch 302, CIFAR-10 Batch 5:  Loss:     0.8895 Validation Accuracy: 0.775000\n",
      "Epoch 303, CIFAR-10 Batch 1:  Loss:     0.9762 Validation Accuracy: 0.700000\n",
      "Epoch 303, CIFAR-10 Batch 2:  Loss:     0.5951 Validation Accuracy: 0.850000\n",
      "Epoch 303, CIFAR-10 Batch 3:  Loss:     0.5563 Validation Accuracy: 0.900000\n",
      "Epoch 303, CIFAR-10 Batch 4:  Loss:     0.9691 Validation Accuracy: 0.675000\n",
      "Epoch 303, CIFAR-10 Batch 5:  Loss:     0.8903 Validation Accuracy: 0.775000\n",
      "Epoch 304, CIFAR-10 Batch 1:  Loss:     0.9733 Validation Accuracy: 0.675000\n",
      "Epoch 304, CIFAR-10 Batch 2:  Loss:     0.5970 Validation Accuracy: 0.850000\n",
      "Epoch 304, CIFAR-10 Batch 3:  Loss:     0.5547 Validation Accuracy: 0.900000\n",
      "Epoch 304, CIFAR-10 Batch 4:  Loss:     0.9673 Validation Accuracy: 0.725000\n",
      "Epoch 304, CIFAR-10 Batch 5:  Loss:     0.8889 Validation Accuracy: 0.775000\n",
      "Epoch 305, CIFAR-10 Batch 1:  Loss:     0.9690 Validation Accuracy: 0.675000\n",
      "Epoch 305, CIFAR-10 Batch 2:  Loss:     0.5918 Validation Accuracy: 0.850000\n",
      "Epoch 305, CIFAR-10 Batch 3:  Loss:     0.5566 Validation Accuracy: 0.900000\n",
      "Epoch 305, CIFAR-10 Batch 4:  Loss:     0.9675 Validation Accuracy: 0.675000\n",
      "Epoch 305, CIFAR-10 Batch 5:  Loss:     0.9101 Validation Accuracy: 0.775000\n",
      "Epoch 306, CIFAR-10 Batch 1:  Loss:     0.9713 Validation Accuracy: 0.675000\n",
      "Epoch 306, CIFAR-10 Batch 2:  Loss:     0.5984 Validation Accuracy: 0.825000\n",
      "Epoch 306, CIFAR-10 Batch 3:  Loss:     0.5553 Validation Accuracy: 0.900000\n",
      "Epoch 306, CIFAR-10 Batch 4:  Loss:     0.9639 Validation Accuracy: 0.700000\n",
      "Epoch 306, CIFAR-10 Batch 5:  Loss:     0.9157 Validation Accuracy: 0.750000\n",
      "Epoch 307, CIFAR-10 Batch 1:  Loss:     0.9680 Validation Accuracy: 0.700000\n",
      "Epoch 307, CIFAR-10 Batch 2:  Loss:     0.6006 Validation Accuracy: 0.850000\n",
      "Epoch 307, CIFAR-10 Batch 3:  Loss:     0.5511 Validation Accuracy: 0.900000\n",
      "Epoch 307, CIFAR-10 Batch 4:  Loss:     0.9633 Validation Accuracy: 0.700000\n",
      "Epoch 307, CIFAR-10 Batch 5:  Loss:     0.9090 Validation Accuracy: 0.750000\n",
      "Epoch 308, CIFAR-10 Batch 1:  Loss:     0.9723 Validation Accuracy: 0.675000\n",
      "Epoch 308, CIFAR-10 Batch 2:  Loss:     0.5931 Validation Accuracy: 0.850000\n",
      "Epoch 308, CIFAR-10 Batch 3:  Loss:     0.5543 Validation Accuracy: 0.900000\n",
      "Epoch 308, CIFAR-10 Batch 4:  Loss:     0.9587 Validation Accuracy: 0.700000\n",
      "Epoch 308, CIFAR-10 Batch 5:  Loss:     0.9065 Validation Accuracy: 0.775000\n",
      "Epoch 309, CIFAR-10 Batch 1:  Loss:     0.9671 Validation Accuracy: 0.700000\n",
      "Epoch 309, CIFAR-10 Batch 2:  Loss:     0.5954 Validation Accuracy: 0.850000\n",
      "Epoch 309, CIFAR-10 Batch 3:  Loss:     0.5523 Validation Accuracy: 0.900000\n",
      "Epoch 309, CIFAR-10 Batch 4:  Loss:     0.9606 Validation Accuracy: 0.700000\n",
      "Epoch 309, CIFAR-10 Batch 5:  Loss:     0.9074 Validation Accuracy: 0.775000\n",
      "Epoch 310, CIFAR-10 Batch 1:  Loss:     0.9671 Validation Accuracy: 0.700000\n",
      "Epoch 310, CIFAR-10 Batch 2:  Loss:     0.5975 Validation Accuracy: 0.825000\n",
      "Epoch 310, CIFAR-10 Batch 3:  Loss:     0.5550 Validation Accuracy: 0.900000\n",
      "Epoch 310, CIFAR-10 Batch 4:  Loss:     0.9646 Validation Accuracy: 0.700000\n",
      "Epoch 310, CIFAR-10 Batch 5:  Loss:     0.9036 Validation Accuracy: 0.750000\n",
      "Epoch 311, CIFAR-10 Batch 1:  Loss:     0.9694 Validation Accuracy: 0.700000\n",
      "Epoch 311, CIFAR-10 Batch 2:  Loss:     0.5916 Validation Accuracy: 0.850000\n",
      "Epoch 311, CIFAR-10 Batch 3:  Loss:     0.5575 Validation Accuracy: 0.900000\n",
      "Epoch 311, CIFAR-10 Batch 4:  Loss:     0.9646 Validation Accuracy: 0.700000\n",
      "Epoch 311, CIFAR-10 Batch 5:  Loss:     0.9018 Validation Accuracy: 0.775000\n",
      "Epoch 312, CIFAR-10 Batch 1:  Loss:     0.9646 Validation Accuracy: 0.700000\n",
      "Epoch 312, CIFAR-10 Batch 2:  Loss:     0.5880 Validation Accuracy: 0.850000\n",
      "Epoch 312, CIFAR-10 Batch 3:  Loss:     0.5539 Validation Accuracy: 0.875000\n",
      "Epoch 312, CIFAR-10 Batch 4:  Loss:     0.9604 Validation Accuracy: 0.700000\n",
      "Epoch 312, CIFAR-10 Batch 5:  Loss:     0.9054 Validation Accuracy: 0.750000\n",
      "Epoch 313, CIFAR-10 Batch 1:  Loss:     0.9612 Validation Accuracy: 0.675000\n",
      "Epoch 313, CIFAR-10 Batch 2:  Loss:     0.6009 Validation Accuracy: 0.825000\n",
      "Epoch 313, CIFAR-10 Batch 3:  Loss:     0.5544 Validation Accuracy: 0.900000\n",
      "Epoch 313, CIFAR-10 Batch 4:  Loss:     0.9599 Validation Accuracy: 0.700000\n",
      "Epoch 313, CIFAR-10 Batch 5:  Loss:     0.9012 Validation Accuracy: 0.750000\n",
      "Epoch 314, CIFAR-10 Batch 1:  Loss:     0.9691 Validation Accuracy: 0.700000\n",
      "Epoch 314, CIFAR-10 Batch 2:  Loss:     0.5907 Validation Accuracy: 0.850000\n",
      "Epoch 314, CIFAR-10 Batch 3:  Loss:     0.5585 Validation Accuracy: 0.900000\n",
      "Epoch 314, CIFAR-10 Batch 4:  Loss:     0.9571 Validation Accuracy: 0.700000\n",
      "Epoch 314, CIFAR-10 Batch 5:  Loss:     0.9020 Validation Accuracy: 0.750000\n",
      "Epoch 315, CIFAR-10 Batch 1:  Loss:     0.9620 Validation Accuracy: 0.700000\n",
      "Epoch 315, CIFAR-10 Batch 2:  Loss:     0.5985 Validation Accuracy: 0.875000\n",
      "Epoch 315, CIFAR-10 Batch 3:  Loss:     0.5546 Validation Accuracy: 0.900000\n",
      "Epoch 315, CIFAR-10 Batch 4:  Loss:     0.9634 Validation Accuracy: 0.700000\n",
      "Epoch 315, CIFAR-10 Batch 5:  Loss:     0.8999 Validation Accuracy: 0.775000\n",
      "Epoch 316, CIFAR-10 Batch 1:  Loss:     0.9604 Validation Accuracy: 0.700000\n",
      "Epoch 316, CIFAR-10 Batch 2:  Loss:     0.5855 Validation Accuracy: 0.850000\n",
      "Epoch 316, CIFAR-10 Batch 3:  Loss:     0.5566 Validation Accuracy: 0.900000\n",
      "Epoch 316, CIFAR-10 Batch 4:  Loss:     0.9531 Validation Accuracy: 0.700000\n",
      "Epoch 316, CIFAR-10 Batch 5:  Loss:     0.9020 Validation Accuracy: 0.775000\n",
      "Epoch 317, CIFAR-10 Batch 1:  Loss:     0.9665 Validation Accuracy: 0.700000\n",
      "Epoch 317, CIFAR-10 Batch 2:  Loss:     0.5936 Validation Accuracy: 0.850000\n",
      "Epoch 317, CIFAR-10 Batch 3:  Loss:     0.5520 Validation Accuracy: 0.900000\n",
      "Epoch 317, CIFAR-10 Batch 4:  Loss:     0.9612 Validation Accuracy: 0.700000\n",
      "Epoch 317, CIFAR-10 Batch 5:  Loss:     0.8933 Validation Accuracy: 0.775000\n",
      "Epoch 318, CIFAR-10 Batch 1:  Loss:     0.9636 Validation Accuracy: 0.700000\n",
      "Epoch 318, CIFAR-10 Batch 2:  Loss:     0.5915 Validation Accuracy: 0.850000\n",
      "Epoch 318, CIFAR-10 Batch 3:  Loss:     0.5585 Validation Accuracy: 0.875000\n",
      "Epoch 318, CIFAR-10 Batch 4:  Loss:     0.9566 Validation Accuracy: 0.700000\n",
      "Epoch 318, CIFAR-10 Batch 5:  Loss:     0.8913 Validation Accuracy: 0.750000\n",
      "Epoch 319, CIFAR-10 Batch 1:  Loss:     0.9641 Validation Accuracy: 0.700000\n",
      "Epoch 319, CIFAR-10 Batch 2:  Loss:     0.5925 Validation Accuracy: 0.850000\n",
      "Epoch 319, CIFAR-10 Batch 3:  Loss:     0.5571 Validation Accuracy: 0.900000\n",
      "Epoch 319, CIFAR-10 Batch 4:  Loss:     0.9590 Validation Accuracy: 0.700000\n",
      "Epoch 319, CIFAR-10 Batch 5:  Loss:     0.8944 Validation Accuracy: 0.750000\n",
      "Epoch 320, CIFAR-10 Batch 1:  Loss:     0.9610 Validation Accuracy: 0.700000\n",
      "Epoch 320, CIFAR-10 Batch 2:  Loss:     0.5920 Validation Accuracy: 0.850000\n",
      "Epoch 320, CIFAR-10 Batch 3:  Loss:     0.5613 Validation Accuracy: 0.900000\n",
      "Epoch 320, CIFAR-10 Batch 4:  Loss:     0.9589 Validation Accuracy: 0.700000\n",
      "Epoch 320, CIFAR-10 Batch 5:  Loss:     0.8943 Validation Accuracy: 0.750000\n",
      "Epoch 321, CIFAR-10 Batch 1:  Loss:     0.9598 Validation Accuracy: 0.700000\n",
      "Epoch 321, CIFAR-10 Batch 2:  Loss:     0.5933 Validation Accuracy: 0.850000\n",
      "Epoch 321, CIFAR-10 Batch 3:  Loss:     0.5609 Validation Accuracy: 0.900000\n",
      "Epoch 321, CIFAR-10 Batch 4:  Loss:     0.9567 Validation Accuracy: 0.725000\n",
      "Epoch 321, CIFAR-10 Batch 5:  Loss:     0.8964 Validation Accuracy: 0.750000\n",
      "Epoch 322, CIFAR-10 Batch 1:  Loss:     0.9567 Validation Accuracy: 0.700000\n",
      "Epoch 322, CIFAR-10 Batch 2:  Loss:     0.5941 Validation Accuracy: 0.850000\n",
      "Epoch 322, CIFAR-10 Batch 3:  Loss:     0.5584 Validation Accuracy: 0.875000\n",
      "Epoch 322, CIFAR-10 Batch 4:  Loss:     0.9557 Validation Accuracy: 0.700000\n",
      "Epoch 322, CIFAR-10 Batch 5:  Loss:     0.8984 Validation Accuracy: 0.750000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 323, CIFAR-10 Batch 1:  Loss:     0.9648 Validation Accuracy: 0.675000\n",
      "Epoch 323, CIFAR-10 Batch 2:  Loss:     0.5944 Validation Accuracy: 0.850000\n",
      "Epoch 323, CIFAR-10 Batch 3:  Loss:     0.5545 Validation Accuracy: 0.900000\n",
      "Epoch 323, CIFAR-10 Batch 4:  Loss:     0.9577 Validation Accuracy: 0.700000\n",
      "Epoch 323, CIFAR-10 Batch 5:  Loss:     0.8970 Validation Accuracy: 0.750000\n",
      "Epoch 324, CIFAR-10 Batch 1:  Loss:     0.9640 Validation Accuracy: 0.700000\n",
      "Epoch 324, CIFAR-10 Batch 2:  Loss:     0.5888 Validation Accuracy: 0.850000\n",
      "Epoch 324, CIFAR-10 Batch 3:  Loss:     0.5523 Validation Accuracy: 0.900000\n",
      "Epoch 324, CIFAR-10 Batch 4:  Loss:     0.9587 Validation Accuracy: 0.700000\n",
      "Epoch 324, CIFAR-10 Batch 5:  Loss:     0.8948 Validation Accuracy: 0.775000\n",
      "Epoch 325, CIFAR-10 Batch 1:  Loss:     0.9667 Validation Accuracy: 0.700000\n",
      "Epoch 325, CIFAR-10 Batch 2:  Loss:     0.5873 Validation Accuracy: 0.850000\n",
      "Epoch 325, CIFAR-10 Batch 3:  Loss:     0.5569 Validation Accuracy: 0.900000\n",
      "Epoch 325, CIFAR-10 Batch 4:  Loss:     0.9550 Validation Accuracy: 0.700000\n",
      "Epoch 325, CIFAR-10 Batch 5:  Loss:     0.8991 Validation Accuracy: 0.750000\n",
      "Epoch 326, CIFAR-10 Batch 1:  Loss:     0.9702 Validation Accuracy: 0.700000\n",
      "Epoch 326, CIFAR-10 Batch 2:  Loss:     0.5883 Validation Accuracy: 0.850000\n",
      "Epoch 326, CIFAR-10 Batch 3:  Loss:     0.5577 Validation Accuracy: 0.900000\n",
      "Epoch 326, CIFAR-10 Batch 4:  Loss:     0.9513 Validation Accuracy: 0.700000\n",
      "Epoch 326, CIFAR-10 Batch 5:  Loss:     0.9014 Validation Accuracy: 0.775000\n",
      "Epoch 327, CIFAR-10 Batch 1:  Loss:     0.9754 Validation Accuracy: 0.675000\n",
      "Epoch 327, CIFAR-10 Batch 2:  Loss:     0.5851 Validation Accuracy: 0.850000\n",
      "Epoch 327, CIFAR-10 Batch 3:  Loss:     0.5543 Validation Accuracy: 0.900000\n",
      "Epoch 327, CIFAR-10 Batch 4:  Loss:     0.9584 Validation Accuracy: 0.700000\n",
      "Epoch 327, CIFAR-10 Batch 5:  Loss:     0.8944 Validation Accuracy: 0.775000\n",
      "Epoch 328, CIFAR-10 Batch 1:  Loss:     0.9534 Validation Accuracy: 0.700000\n",
      "Epoch 328, CIFAR-10 Batch 2:  Loss:     0.5864 Validation Accuracy: 0.850000\n",
      "Epoch 328, CIFAR-10 Batch 3:  Loss:     0.5590 Validation Accuracy: 0.875000\n",
      "Epoch 328, CIFAR-10 Batch 4:  Loss:     0.9587 Validation Accuracy: 0.700000\n",
      "Epoch 328, CIFAR-10 Batch 5:  Loss:     0.8953 Validation Accuracy: 0.750000\n",
      "Epoch 329, CIFAR-10 Batch 1:  Loss:     0.9605 Validation Accuracy: 0.700000\n",
      "Epoch 329, CIFAR-10 Batch 2:  Loss:     0.5872 Validation Accuracy: 0.850000\n",
      "Epoch 329, CIFAR-10 Batch 3:  Loss:     0.5532 Validation Accuracy: 0.900000\n",
      "Epoch 329, CIFAR-10 Batch 4:  Loss:     0.9592 Validation Accuracy: 0.725000\n",
      "Epoch 329, CIFAR-10 Batch 5:  Loss:     0.8993 Validation Accuracy: 0.750000\n",
      "Epoch 330, CIFAR-10 Batch 1:  Loss:     0.9590 Validation Accuracy: 0.725000\n",
      "Epoch 330, CIFAR-10 Batch 2:  Loss:     0.5848 Validation Accuracy: 0.850000\n",
      "Epoch 330, CIFAR-10 Batch 3:  Loss:     0.5525 Validation Accuracy: 0.900000\n",
      "Epoch 330, CIFAR-10 Batch 4:  Loss:     0.9582 Validation Accuracy: 0.725000\n",
      "Epoch 330, CIFAR-10 Batch 5:  Loss:     0.9049 Validation Accuracy: 0.750000\n",
      "Epoch 331, CIFAR-10 Batch 1:  Loss:     0.9596 Validation Accuracy: 0.700000\n",
      "Epoch 331, CIFAR-10 Batch 2:  Loss:     0.5862 Validation Accuracy: 0.850000\n",
      "Epoch 331, CIFAR-10 Batch 3:  Loss:     0.5593 Validation Accuracy: 0.900000\n",
      "Epoch 331, CIFAR-10 Batch 4:  Loss:     0.9647 Validation Accuracy: 0.700000\n",
      "Epoch 331, CIFAR-10 Batch 5:  Loss:     0.8970 Validation Accuracy: 0.750000\n",
      "Epoch 332, CIFAR-10 Batch 1:  Loss:     0.9604 Validation Accuracy: 0.725000\n",
      "Epoch 332, CIFAR-10 Batch 2:  Loss:     0.5875 Validation Accuracy: 0.850000\n",
      "Epoch 332, CIFAR-10 Batch 3:  Loss:     0.5535 Validation Accuracy: 0.900000\n",
      "Epoch 332, CIFAR-10 Batch 4:  Loss:     0.9605 Validation Accuracy: 0.725000\n",
      "Epoch 332, CIFAR-10 Batch 5:  Loss:     0.8965 Validation Accuracy: 0.750000\n",
      "Epoch 333, CIFAR-10 Batch 1:  Loss:     0.9711 Validation Accuracy: 0.725000\n",
      "Epoch 333, CIFAR-10 Batch 2:  Loss:     0.5889 Validation Accuracy: 0.850000\n",
      "Epoch 333, CIFAR-10 Batch 3:  Loss:     0.5577 Validation Accuracy: 0.900000\n",
      "Epoch 333, CIFAR-10 Batch 4:  Loss:     0.9638 Validation Accuracy: 0.725000\n",
      "Epoch 333, CIFAR-10 Batch 5:  Loss:     0.9003 Validation Accuracy: 0.750000\n",
      "Epoch 334, CIFAR-10 Batch 1:  Loss:     0.9702 Validation Accuracy: 0.700000\n",
      "Epoch 334, CIFAR-10 Batch 2:  Loss:     0.5874 Validation Accuracy: 0.850000\n",
      "Epoch 334, CIFAR-10 Batch 3:  Loss:     0.5575 Validation Accuracy: 0.900000\n",
      "Epoch 334, CIFAR-10 Batch 4:  Loss:     0.9624 Validation Accuracy: 0.700000\n",
      "Epoch 334, CIFAR-10 Batch 5:  Loss:     0.8935 Validation Accuracy: 0.750000\n",
      "Epoch 335, CIFAR-10 Batch 1:  Loss:     0.9753 Validation Accuracy: 0.725000\n",
      "Epoch 335, CIFAR-10 Batch 2:  Loss:     0.5943 Validation Accuracy: 0.850000\n",
      "Epoch 335, CIFAR-10 Batch 3:  Loss:     0.5650 Validation Accuracy: 0.900000\n",
      "Epoch 335, CIFAR-10 Batch 4:  Loss:     0.9633 Validation Accuracy: 0.725000\n",
      "Epoch 335, CIFAR-10 Batch 5:  Loss:     0.8906 Validation Accuracy: 0.775000\n",
      "Epoch 336, CIFAR-10 Batch 1:  Loss:     0.9602 Validation Accuracy: 0.725000\n",
      "Epoch 336, CIFAR-10 Batch 2:  Loss:     0.5860 Validation Accuracy: 0.850000\n",
      "Epoch 336, CIFAR-10 Batch 3:  Loss:     0.5598 Validation Accuracy: 0.900000\n",
      "Epoch 336, CIFAR-10 Batch 4:  Loss:     0.9663 Validation Accuracy: 0.700000\n",
      "Epoch 336, CIFAR-10 Batch 5:  Loss:     0.8978 Validation Accuracy: 0.750000\n",
      "Epoch 337, CIFAR-10 Batch 1:  Loss:     0.9631 Validation Accuracy: 0.700000\n",
      "Epoch 337, CIFAR-10 Batch 2:  Loss:     0.5829 Validation Accuracy: 0.875000\n",
      "Epoch 337, CIFAR-10 Batch 3:  Loss:     0.5613 Validation Accuracy: 0.875000\n",
      "Epoch 337, CIFAR-10 Batch 4:  Loss:     0.9650 Validation Accuracy: 0.700000\n",
      "Epoch 337, CIFAR-10 Batch 5:  Loss:     0.8944 Validation Accuracy: 0.750000\n",
      "Epoch 338, CIFAR-10 Batch 1:  Loss:     0.9709 Validation Accuracy: 0.725000\n",
      "Epoch 338, CIFAR-10 Batch 2:  Loss:     0.5802 Validation Accuracy: 0.850000\n",
      "Epoch 338, CIFAR-10 Batch 3:  Loss:     0.5608 Validation Accuracy: 0.900000\n",
      "Epoch 338, CIFAR-10 Batch 4:  Loss:     0.9649 Validation Accuracy: 0.675000\n",
      "Epoch 338, CIFAR-10 Batch 5:  Loss:     0.9012 Validation Accuracy: 0.750000\n",
      "Epoch 339, CIFAR-10 Batch 1:  Loss:     0.9595 Validation Accuracy: 0.725000\n",
      "Epoch 339, CIFAR-10 Batch 2:  Loss:     0.5888 Validation Accuracy: 0.875000\n",
      "Epoch 339, CIFAR-10 Batch 3:  Loss:     0.5566 Validation Accuracy: 0.900000\n",
      "Epoch 339, CIFAR-10 Batch 4:  Loss:     0.9633 Validation Accuracy: 0.725000\n",
      "Epoch 339, CIFAR-10 Batch 5:  Loss:     0.8960 Validation Accuracy: 0.750000\n",
      "Epoch 340, CIFAR-10 Batch 1:  Loss:     0.9645 Validation Accuracy: 0.700000\n",
      "Epoch 340, CIFAR-10 Batch 2:  Loss:     0.5886 Validation Accuracy: 0.875000\n",
      "Epoch 340, CIFAR-10 Batch 3:  Loss:     0.5582 Validation Accuracy: 0.900000\n",
      "Epoch 340, CIFAR-10 Batch 4:  Loss:     0.9724 Validation Accuracy: 0.725000\n",
      "Epoch 340, CIFAR-10 Batch 5:  Loss:     0.8944 Validation Accuracy: 0.750000\n",
      "Epoch 341, CIFAR-10 Batch 1:  Loss:     0.9624 Validation Accuracy: 0.700000\n",
      "Epoch 341, CIFAR-10 Batch 2:  Loss:     0.5805 Validation Accuracy: 0.875000\n",
      "Epoch 341, CIFAR-10 Batch 3:  Loss:     0.5554 Validation Accuracy: 0.900000\n",
      "Epoch 341, CIFAR-10 Batch 4:  Loss:     0.9692 Validation Accuracy: 0.700000\n",
      "Epoch 341, CIFAR-10 Batch 5:  Loss:     0.8964 Validation Accuracy: 0.750000\n",
      "Epoch 342, CIFAR-10 Batch 1:  Loss:     0.9627 Validation Accuracy: 0.725000\n",
      "Epoch 342, CIFAR-10 Batch 2:  Loss:     0.5896 Validation Accuracy: 0.875000\n",
      "Epoch 342, CIFAR-10 Batch 3:  Loss:     0.5534 Validation Accuracy: 0.900000\n",
      "Epoch 342, CIFAR-10 Batch 4:  Loss:     0.9734 Validation Accuracy: 0.700000\n",
      "Epoch 342, CIFAR-10 Batch 5:  Loss:     0.8899 Validation Accuracy: 0.750000\n",
      "Epoch 343, CIFAR-10 Batch 1:  Loss:     0.9591 Validation Accuracy: 0.700000\n",
      "Epoch 343, CIFAR-10 Batch 2:  Loss:     0.5897 Validation Accuracy: 0.875000\n",
      "Epoch 343, CIFAR-10 Batch 3:  Loss:     0.5584 Validation Accuracy: 0.875000\n",
      "Epoch 343, CIFAR-10 Batch 4:  Loss:     0.9834 Validation Accuracy: 0.725000\n",
      "Epoch 343, CIFAR-10 Batch 5:  Loss:     0.9018 Validation Accuracy: 0.775000\n",
      "Epoch 344, CIFAR-10 Batch 1:  Loss:     0.9581 Validation Accuracy: 0.700000\n",
      "Epoch 344, CIFAR-10 Batch 2:  Loss:     0.5959 Validation Accuracy: 0.875000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 344, CIFAR-10 Batch 3:  Loss:     0.5545 Validation Accuracy: 0.875000\n",
      "Epoch 344, CIFAR-10 Batch 4:  Loss:     0.9805 Validation Accuracy: 0.700000\n",
      "Epoch 344, CIFAR-10 Batch 5:  Loss:     0.8971 Validation Accuracy: 0.775000\n",
      "Epoch 345, CIFAR-10 Batch 1:  Loss:     0.9604 Validation Accuracy: 0.700000\n",
      "Epoch 345, CIFAR-10 Batch 2:  Loss:     0.5852 Validation Accuracy: 0.875000\n",
      "Epoch 345, CIFAR-10 Batch 3:  Loss:     0.5550 Validation Accuracy: 0.875000\n",
      "Epoch 345, CIFAR-10 Batch 4:  Loss:     0.9826 Validation Accuracy: 0.725000\n",
      "Epoch 345, CIFAR-10 Batch 5:  Loss:     0.8971 Validation Accuracy: 0.775000\n",
      "Epoch 346, CIFAR-10 Batch 1:  Loss:     0.9661 Validation Accuracy: 0.700000\n",
      "Epoch 346, CIFAR-10 Batch 2:  Loss:     0.5822 Validation Accuracy: 0.875000\n",
      "Epoch 346, CIFAR-10 Batch 3:  Loss:     0.5599 Validation Accuracy: 0.875000\n",
      "Epoch 346, CIFAR-10 Batch 4:  Loss:     0.9835 Validation Accuracy: 0.700000\n",
      "Epoch 346, CIFAR-10 Batch 5:  Loss:     0.8966 Validation Accuracy: 0.750000\n",
      "Epoch 347, CIFAR-10 Batch 1:  Loss:     0.9628 Validation Accuracy: 0.700000\n",
      "Epoch 347, CIFAR-10 Batch 2:  Loss:     0.5923 Validation Accuracy: 0.875000\n",
      "Epoch 347, CIFAR-10 Batch 3:  Loss:     0.5564 Validation Accuracy: 0.900000\n",
      "Epoch 347, CIFAR-10 Batch 4:  Loss:     0.9848 Validation Accuracy: 0.725000\n",
      "Epoch 347, CIFAR-10 Batch 5:  Loss:     0.8975 Validation Accuracy: 0.775000\n",
      "Epoch 348, CIFAR-10 Batch 1:  Loss:     0.9619 Validation Accuracy: 0.700000\n",
      "Epoch 348, CIFAR-10 Batch 2:  Loss:     0.5895 Validation Accuracy: 0.875000\n",
      "Epoch 348, CIFAR-10 Batch 3:  Loss:     0.5589 Validation Accuracy: 0.900000\n",
      "Epoch 348, CIFAR-10 Batch 4:  Loss:     0.9803 Validation Accuracy: 0.725000\n",
      "Epoch 348, CIFAR-10 Batch 5:  Loss:     0.9006 Validation Accuracy: 0.750000\n",
      "Epoch 349, CIFAR-10 Batch 1:  Loss:     0.9629 Validation Accuracy: 0.700000\n",
      "Epoch 349, CIFAR-10 Batch 2:  Loss:     0.5806 Validation Accuracy: 0.875000\n",
      "Epoch 349, CIFAR-10 Batch 3:  Loss:     0.5639 Validation Accuracy: 0.900000\n",
      "Epoch 349, CIFAR-10 Batch 4:  Loss:     0.9820 Validation Accuracy: 0.725000\n",
      "Epoch 349, CIFAR-10 Batch 5:  Loss:     0.9017 Validation Accuracy: 0.775000\n",
      "Epoch 350, CIFAR-10 Batch 1:  Loss:     0.9638 Validation Accuracy: 0.700000\n",
      "Epoch 350, CIFAR-10 Batch 2:  Loss:     0.5777 Validation Accuracy: 0.850000\n",
      "Epoch 350, CIFAR-10 Batch 3:  Loss:     0.5498 Validation Accuracy: 0.900000\n",
      "Epoch 350, CIFAR-10 Batch 4:  Loss:     0.9795 Validation Accuracy: 0.725000\n",
      "Epoch 350, CIFAR-10 Batch 5:  Loss:     0.9005 Validation Accuracy: 0.775000\n",
      "Epoch 351, CIFAR-10 Batch 1:  Loss:     0.9606 Validation Accuracy: 0.725000\n",
      "Epoch 351, CIFAR-10 Batch 2:  Loss:     0.5827 Validation Accuracy: 0.875000\n",
      "Epoch 351, CIFAR-10 Batch 3:  Loss:     0.5535 Validation Accuracy: 0.900000\n",
      "Epoch 351, CIFAR-10 Batch 4:  Loss:     0.9884 Validation Accuracy: 0.725000\n",
      "Epoch 351, CIFAR-10 Batch 5:  Loss:     0.8904 Validation Accuracy: 0.775000\n",
      "Epoch 352, CIFAR-10 Batch 1:  Loss:     0.9590 Validation Accuracy: 0.700000\n",
      "Epoch 352, CIFAR-10 Batch 2:  Loss:     0.5900 Validation Accuracy: 0.875000\n",
      "Epoch 352, CIFAR-10 Batch 3:  Loss:     0.5551 Validation Accuracy: 0.900000\n",
      "Epoch 352, CIFAR-10 Batch 4:  Loss:     0.9809 Validation Accuracy: 0.725000\n",
      "Epoch 352, CIFAR-10 Batch 5:  Loss:     0.9025 Validation Accuracy: 0.800000\n",
      "Epoch 353, CIFAR-10 Batch 1:  Loss:     0.9694 Validation Accuracy: 0.700000\n",
      "Epoch 353, CIFAR-10 Batch 2:  Loss:     0.5814 Validation Accuracy: 0.875000\n",
      "Epoch 353, CIFAR-10 Batch 3:  Loss:     0.5556 Validation Accuracy: 0.900000\n",
      "Epoch 353, CIFAR-10 Batch 4:  Loss:     0.9778 Validation Accuracy: 0.725000\n",
      "Epoch 353, CIFAR-10 Batch 5:  Loss:     0.8953 Validation Accuracy: 0.775000\n",
      "Epoch 354, CIFAR-10 Batch 1:  Loss:     0.9635 Validation Accuracy: 0.725000\n",
      "Epoch 354, CIFAR-10 Batch 2:  Loss:     0.5817 Validation Accuracy: 0.875000\n",
      "Epoch 354, CIFAR-10 Batch 3:  Loss:     0.5527 Validation Accuracy: 0.900000\n",
      "Epoch 354, CIFAR-10 Batch 4:  Loss:     0.9789 Validation Accuracy: 0.725000\n",
      "Epoch 354, CIFAR-10 Batch 5:  Loss:     0.9047 Validation Accuracy: 0.775000\n",
      "Epoch 355, CIFAR-10 Batch 1:  Loss:     0.9585 Validation Accuracy: 0.725000\n",
      "Epoch 355, CIFAR-10 Batch 2:  Loss:     0.5755 Validation Accuracy: 0.875000\n",
      "Epoch 355, CIFAR-10 Batch 3:  Loss:     0.5529 Validation Accuracy: 0.900000\n",
      "Epoch 355, CIFAR-10 Batch 4:  Loss:     0.9895 Validation Accuracy: 0.725000\n",
      "Epoch 355, CIFAR-10 Batch 5:  Loss:     0.9034 Validation Accuracy: 0.775000\n",
      "Epoch 356, CIFAR-10 Batch 1:  Loss:     0.9583 Validation Accuracy: 0.725000\n",
      "Epoch 356, CIFAR-10 Batch 2:  Loss:     0.5740 Validation Accuracy: 0.875000\n",
      "Epoch 356, CIFAR-10 Batch 3:  Loss:     0.5485 Validation Accuracy: 0.900000\n",
      "Epoch 356, CIFAR-10 Batch 4:  Loss:     0.9854 Validation Accuracy: 0.725000\n",
      "Epoch 356, CIFAR-10 Batch 5:  Loss:     0.8926 Validation Accuracy: 0.750000\n",
      "Epoch 357, CIFAR-10 Batch 1:  Loss:     0.9620 Validation Accuracy: 0.725000\n",
      "Epoch 357, CIFAR-10 Batch 2:  Loss:     0.5813 Validation Accuracy: 0.875000\n",
      "Epoch 357, CIFAR-10 Batch 3:  Loss:     0.5556 Validation Accuracy: 0.900000\n",
      "Epoch 357, CIFAR-10 Batch 4:  Loss:     0.9916 Validation Accuracy: 0.700000\n",
      "Epoch 357, CIFAR-10 Batch 5:  Loss:     0.8964 Validation Accuracy: 0.775000\n",
      "Epoch 358, CIFAR-10 Batch 1:  Loss:     0.9599 Validation Accuracy: 0.725000\n",
      "Epoch 358, CIFAR-10 Batch 2:  Loss:     0.5748 Validation Accuracy: 0.875000\n",
      "Epoch 358, CIFAR-10 Batch 3:  Loss:     0.5493 Validation Accuracy: 0.900000\n",
      "Epoch 358, CIFAR-10 Batch 4:  Loss:     0.9939 Validation Accuracy: 0.700000\n",
      "Epoch 358, CIFAR-10 Batch 5:  Loss:     0.8955 Validation Accuracy: 0.750000\n",
      "Epoch 359, CIFAR-10 Batch 1:  Loss:     0.9605 Validation Accuracy: 0.725000\n",
      "Epoch 359, CIFAR-10 Batch 2:  Loss:     0.5767 Validation Accuracy: 0.875000\n",
      "Epoch 359, CIFAR-10 Batch 3:  Loss:     0.5489 Validation Accuracy: 0.900000\n",
      "Epoch 359, CIFAR-10 Batch 4:  Loss:     0.9973 Validation Accuracy: 0.725000\n",
      "Epoch 359, CIFAR-10 Batch 5:  Loss:     0.8891 Validation Accuracy: 0.800000\n",
      "Epoch 360, CIFAR-10 Batch 1:  Loss:     0.9651 Validation Accuracy: 0.700000\n",
      "Epoch 360, CIFAR-10 Batch 2:  Loss:     0.5668 Validation Accuracy: 0.875000\n",
      "Epoch 360, CIFAR-10 Batch 3:  Loss:     0.5528 Validation Accuracy: 0.900000\n",
      "Epoch 360, CIFAR-10 Batch 4:  Loss:     0.9901 Validation Accuracy: 0.725000\n",
      "Epoch 360, CIFAR-10 Batch 5:  Loss:     0.8924 Validation Accuracy: 0.750000\n",
      "Epoch 361, CIFAR-10 Batch 1:  Loss:     0.9588 Validation Accuracy: 0.700000\n",
      "Epoch 361, CIFAR-10 Batch 2:  Loss:     0.5735 Validation Accuracy: 0.875000\n",
      "Epoch 361, CIFAR-10 Batch 3:  Loss:     0.5532 Validation Accuracy: 0.900000\n",
      "Epoch 361, CIFAR-10 Batch 4:  Loss:     0.9951 Validation Accuracy: 0.700000\n",
      "Epoch 361, CIFAR-10 Batch 5:  Loss:     0.8865 Validation Accuracy: 0.800000\n",
      "Epoch 362, CIFAR-10 Batch 1:  Loss:     0.9619 Validation Accuracy: 0.725000\n",
      "Epoch 362, CIFAR-10 Batch 2:  Loss:     0.5676 Validation Accuracy: 0.850000\n",
      "Epoch 362, CIFAR-10 Batch 3:  Loss:     0.5584 Validation Accuracy: 0.900000\n",
      "Epoch 362, CIFAR-10 Batch 4:  Loss:     0.9904 Validation Accuracy: 0.725000\n",
      "Epoch 362, CIFAR-10 Batch 5:  Loss:     0.8930 Validation Accuracy: 0.800000\n",
      "Epoch 363, CIFAR-10 Batch 1:  Loss:     0.9637 Validation Accuracy: 0.725000\n",
      "Epoch 363, CIFAR-10 Batch 2:  Loss:     0.5734 Validation Accuracy: 0.850000\n",
      "Epoch 363, CIFAR-10 Batch 3:  Loss:     0.5512 Validation Accuracy: 0.900000\n",
      "Epoch 363, CIFAR-10 Batch 4:  Loss:     0.9951 Validation Accuracy: 0.700000\n",
      "Epoch 363, CIFAR-10 Batch 5:  Loss:     0.8987 Validation Accuracy: 0.775000\n",
      "Epoch 364, CIFAR-10 Batch 1:  Loss:     0.9607 Validation Accuracy: 0.700000\n",
      "Epoch 364, CIFAR-10 Batch 2:  Loss:     0.5725 Validation Accuracy: 0.850000\n",
      "Epoch 364, CIFAR-10 Batch 3:  Loss:     0.5481 Validation Accuracy: 0.900000\n",
      "Epoch 364, CIFAR-10 Batch 4:  Loss:     0.9981 Validation Accuracy: 0.700000\n",
      "Epoch 364, CIFAR-10 Batch 5:  Loss:     0.9061 Validation Accuracy: 0.800000\n",
      "Epoch 365, CIFAR-10 Batch 1:  Loss:     0.9543 Validation Accuracy: 0.725000\n",
      "Epoch 365, CIFAR-10 Batch 2:  Loss:     0.5707 Validation Accuracy: 0.850000\n",
      "Epoch 365, CIFAR-10 Batch 3:  Loss:     0.5486 Validation Accuracy: 0.900000\n",
      "Epoch 365, CIFAR-10 Batch 4:  Loss:     0.9904 Validation Accuracy: 0.725000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 365, CIFAR-10 Batch 5:  Loss:     0.8903 Validation Accuracy: 0.775000\n",
      "Epoch 366, CIFAR-10 Batch 1:  Loss:     0.9639 Validation Accuracy: 0.725000\n",
      "Epoch 366, CIFAR-10 Batch 2:  Loss:     0.5715 Validation Accuracy: 0.850000\n",
      "Epoch 366, CIFAR-10 Batch 3:  Loss:     0.5524 Validation Accuracy: 0.900000\n",
      "Epoch 366, CIFAR-10 Batch 4:  Loss:     0.9845 Validation Accuracy: 0.725000\n",
      "Epoch 366, CIFAR-10 Batch 5:  Loss:     0.8932 Validation Accuracy: 0.800000\n",
      "Epoch 367, CIFAR-10 Batch 1:  Loss:     0.9652 Validation Accuracy: 0.725000\n",
      "Epoch 367, CIFAR-10 Batch 2:  Loss:     0.5724 Validation Accuracy: 0.850000\n",
      "Epoch 367, CIFAR-10 Batch 3:  Loss:     0.5590 Validation Accuracy: 0.900000\n",
      "Epoch 367, CIFAR-10 Batch 4:  Loss:     0.9929 Validation Accuracy: 0.725000\n",
      "Epoch 367, CIFAR-10 Batch 5:  Loss:     0.8905 Validation Accuracy: 0.775000\n",
      "Epoch 368, CIFAR-10 Batch 1:  Loss:     0.9646 Validation Accuracy: 0.725000\n",
      "Epoch 368, CIFAR-10 Batch 2:  Loss:     0.5717 Validation Accuracy: 0.850000\n",
      "Epoch 368, CIFAR-10 Batch 3:  Loss:     0.5590 Validation Accuracy: 0.900000\n",
      "Epoch 368, CIFAR-10 Batch 4:  Loss:     0.9859 Validation Accuracy: 0.725000\n",
      "Epoch 368, CIFAR-10 Batch 5:  Loss:     0.8967 Validation Accuracy: 0.775000\n",
      "Epoch 369, CIFAR-10 Batch 1:  Loss:     0.9627 Validation Accuracy: 0.725000\n",
      "Epoch 369, CIFAR-10 Batch 2:  Loss:     0.5781 Validation Accuracy: 0.875000\n",
      "Epoch 369, CIFAR-10 Batch 3:  Loss:     0.5564 Validation Accuracy: 0.900000\n",
      "Epoch 369, CIFAR-10 Batch 4:  Loss:     0.9857 Validation Accuracy: 0.725000\n",
      "Epoch 369, CIFAR-10 Batch 5:  Loss:     0.8896 Validation Accuracy: 0.775000\n",
      "Epoch 370, CIFAR-10 Batch 1:  Loss:     0.9636 Validation Accuracy: 0.750000\n",
      "Epoch 370, CIFAR-10 Batch 2:  Loss:     0.5715 Validation Accuracy: 0.850000\n",
      "Epoch 370, CIFAR-10 Batch 3:  Loss:     0.5545 Validation Accuracy: 0.900000\n",
      "Epoch 370, CIFAR-10 Batch 4:  Loss:     0.9760 Validation Accuracy: 0.725000\n",
      "Epoch 370, CIFAR-10 Batch 5:  Loss:     0.8880 Validation Accuracy: 0.775000\n",
      "Epoch 371, CIFAR-10 Batch 1:  Loss:     0.9653 Validation Accuracy: 0.725000\n",
      "Epoch 371, CIFAR-10 Batch 2:  Loss:     0.5722 Validation Accuracy: 0.850000\n",
      "Epoch 371, CIFAR-10 Batch 3:  Loss:     0.5572 Validation Accuracy: 0.900000\n",
      "Epoch 371, CIFAR-10 Batch 4:  Loss:     0.9829 Validation Accuracy: 0.725000\n",
      "Epoch 371, CIFAR-10 Batch 5:  Loss:     0.8897 Validation Accuracy: 0.800000\n",
      "Epoch 372, CIFAR-10 Batch 1:  Loss:     0.9631 Validation Accuracy: 0.725000\n",
      "Epoch 372, CIFAR-10 Batch 2:  Loss:     0.5731 Validation Accuracy: 0.850000\n",
      "Epoch 372, CIFAR-10 Batch 3:  Loss:     0.5540 Validation Accuracy: 0.900000\n",
      "Epoch 372, CIFAR-10 Batch 4:  Loss:     0.9746 Validation Accuracy: 0.725000\n",
      "Epoch 372, CIFAR-10 Batch 5:  Loss:     0.8863 Validation Accuracy: 0.775000\n",
      "Epoch 373, CIFAR-10 Batch 1:  Loss:     0.9627 Validation Accuracy: 0.725000\n",
      "Epoch 373, CIFAR-10 Batch 2:  Loss:     0.5664 Validation Accuracy: 0.850000\n",
      "Epoch 373, CIFAR-10 Batch 3:  Loss:     0.5574 Validation Accuracy: 0.900000\n",
      "Epoch 373, CIFAR-10 Batch 4:  Loss:     0.9848 Validation Accuracy: 0.725000\n",
      "Epoch 373, CIFAR-10 Batch 5:  Loss:     0.8906 Validation Accuracy: 0.775000\n",
      "Epoch 374, CIFAR-10 Batch 1:  Loss:     0.9593 Validation Accuracy: 0.725000\n",
      "Epoch 374, CIFAR-10 Batch 2:  Loss:     0.5717 Validation Accuracy: 0.850000\n",
      "Epoch 374, CIFAR-10 Batch 3:  Loss:     0.5584 Validation Accuracy: 0.900000\n",
      "Epoch 374, CIFAR-10 Batch 4:  Loss:     0.9720 Validation Accuracy: 0.725000\n",
      "Epoch 374, CIFAR-10 Batch 5:  Loss:     0.8908 Validation Accuracy: 0.775000\n",
      "Epoch 375, CIFAR-10 Batch 1:  Loss:     0.9602 Validation Accuracy: 0.725000\n",
      "Epoch 375, CIFAR-10 Batch 2:  Loss:     0.5678 Validation Accuracy: 0.850000\n",
      "Epoch 375, CIFAR-10 Batch 3:  Loss:     0.5548 Validation Accuracy: 0.900000\n",
      "Epoch 375, CIFAR-10 Batch 4:  Loss:     0.9732 Validation Accuracy: 0.725000\n",
      "Epoch 375, CIFAR-10 Batch 5:  Loss:     0.8899 Validation Accuracy: 0.800000\n",
      "Epoch 376, CIFAR-10 Batch 1:  Loss:     0.9633 Validation Accuracy: 0.725000\n",
      "Epoch 376, CIFAR-10 Batch 2:  Loss:     0.5693 Validation Accuracy: 0.850000\n",
      "Epoch 376, CIFAR-10 Batch 3:  Loss:     0.5567 Validation Accuracy: 0.900000\n",
      "Epoch 376, CIFAR-10 Batch 4:  Loss:     0.9721 Validation Accuracy: 0.725000\n",
      "Epoch 376, CIFAR-10 Batch 5:  Loss:     0.8880 Validation Accuracy: 0.800000\n",
      "Epoch 377, CIFAR-10 Batch 1:  Loss:     0.9612 Validation Accuracy: 0.750000\n",
      "Epoch 377, CIFAR-10 Batch 2:  Loss:     0.5710 Validation Accuracy: 0.850000\n",
      "Epoch 377, CIFAR-10 Batch 3:  Loss:     0.5580 Validation Accuracy: 0.900000\n",
      "Epoch 377, CIFAR-10 Batch 4:  Loss:     0.9683 Validation Accuracy: 0.725000\n",
      "Epoch 377, CIFAR-10 Batch 5:  Loss:     0.8938 Validation Accuracy: 0.775000\n",
      "Epoch 378, CIFAR-10 Batch 1:  Loss:     0.9605 Validation Accuracy: 0.725000\n",
      "Epoch 378, CIFAR-10 Batch 2:  Loss:     0.5699 Validation Accuracy: 0.850000\n",
      "Epoch 378, CIFAR-10 Batch 3:  Loss:     0.5527 Validation Accuracy: 0.900000\n",
      "Epoch 378, CIFAR-10 Batch 4:  Loss:     0.9678 Validation Accuracy: 0.725000\n",
      "Epoch 378, CIFAR-10 Batch 5:  Loss:     0.8930 Validation Accuracy: 0.800000\n",
      "Epoch 379, CIFAR-10 Batch 1:  Loss:     0.9602 Validation Accuracy: 0.725000\n",
      "Epoch 379, CIFAR-10 Batch 2:  Loss:     0.5693 Validation Accuracy: 0.850000\n",
      "Epoch 379, CIFAR-10 Batch 3:  Loss:     0.5514 Validation Accuracy: 0.900000\n",
      "Epoch 379, CIFAR-10 Batch 4:  Loss:     0.9698 Validation Accuracy: 0.725000\n",
      "Epoch 379, CIFAR-10 Batch 5:  Loss:     0.8897 Validation Accuracy: 0.775000\n",
      "Epoch 380, CIFAR-10 Batch 1:  Loss:     0.9668 Validation Accuracy: 0.725000\n",
      "Epoch 380, CIFAR-10 Batch 2:  Loss:     0.5710 Validation Accuracy: 0.850000\n",
      "Epoch 380, CIFAR-10 Batch 3:  Loss:     0.5535 Validation Accuracy: 0.900000\n",
      "Epoch 380, CIFAR-10 Batch 4:  Loss:     0.9688 Validation Accuracy: 0.725000\n",
      "Epoch 380, CIFAR-10 Batch 5:  Loss:     0.8868 Validation Accuracy: 0.775000\n",
      "Epoch 381, CIFAR-10 Batch 1:  Loss:     0.9614 Validation Accuracy: 0.725000\n",
      "Epoch 381, CIFAR-10 Batch 2:  Loss:     0.5653 Validation Accuracy: 0.850000\n",
      "Epoch 381, CIFAR-10 Batch 3:  Loss:     0.5464 Validation Accuracy: 0.900000\n",
      "Epoch 381, CIFAR-10 Batch 4:  Loss:     0.9694 Validation Accuracy: 0.725000\n",
      "Epoch 381, CIFAR-10 Batch 5:  Loss:     0.8885 Validation Accuracy: 0.775000\n",
      "Epoch 382, CIFAR-10 Batch 1:  Loss:     0.9612 Validation Accuracy: 0.725000\n",
      "Epoch 382, CIFAR-10 Batch 2:  Loss:     0.5683 Validation Accuracy: 0.850000\n",
      "Epoch 382, CIFAR-10 Batch 3:  Loss:     0.5453 Validation Accuracy: 0.900000\n",
      "Epoch 382, CIFAR-10 Batch 4:  Loss:     0.9676 Validation Accuracy: 0.725000\n",
      "Epoch 382, CIFAR-10 Batch 5:  Loss:     0.8852 Validation Accuracy: 0.800000\n",
      "Epoch 383, CIFAR-10 Batch 1:  Loss:     0.9650 Validation Accuracy: 0.750000\n",
      "Epoch 383, CIFAR-10 Batch 2:  Loss:     0.5613 Validation Accuracy: 0.875000\n",
      "Epoch 383, CIFAR-10 Batch 3:  Loss:     0.5519 Validation Accuracy: 0.900000\n",
      "Epoch 383, CIFAR-10 Batch 4:  Loss:     0.9745 Validation Accuracy: 0.725000\n",
      "Epoch 383, CIFAR-10 Batch 5:  Loss:     0.8918 Validation Accuracy: 0.775000\n",
      "Epoch 384, CIFAR-10 Batch 1:  Loss:     0.9697 Validation Accuracy: 0.725000\n",
      "Epoch 384, CIFAR-10 Batch 2:  Loss:     0.5641 Validation Accuracy: 0.850000\n",
      "Epoch 384, CIFAR-10 Batch 3:  Loss:     0.5484 Validation Accuracy: 0.900000\n",
      "Epoch 384, CIFAR-10 Batch 4:  Loss:     0.9633 Validation Accuracy: 0.725000\n",
      "Epoch 384, CIFAR-10 Batch 5:  Loss:     0.8878 Validation Accuracy: 0.775000\n",
      "Epoch 385, CIFAR-10 Batch 1:  Loss:     0.9618 Validation Accuracy: 0.725000\n",
      "Epoch 385, CIFAR-10 Batch 2:  Loss:     0.5663 Validation Accuracy: 0.850000\n",
      "Epoch 385, CIFAR-10 Batch 3:  Loss:     0.5484 Validation Accuracy: 0.900000\n",
      "Epoch 385, CIFAR-10 Batch 4:  Loss:     0.9578 Validation Accuracy: 0.725000\n",
      "Epoch 385, CIFAR-10 Batch 5:  Loss:     0.8784 Validation Accuracy: 0.775000\n",
      "Epoch 386, CIFAR-10 Batch 1:  Loss:     0.9598 Validation Accuracy: 0.750000\n",
      "Epoch 386, CIFAR-10 Batch 2:  Loss:     0.5696 Validation Accuracy: 0.850000\n",
      "Epoch 386, CIFAR-10 Batch 3:  Loss:     0.5489 Validation Accuracy: 0.900000\n",
      "Epoch 386, CIFAR-10 Batch 4:  Loss:     0.9566 Validation Accuracy: 0.725000\n",
      "Epoch 386, CIFAR-10 Batch 5:  Loss:     0.8768 Validation Accuracy: 0.775000\n",
      "Epoch 387, CIFAR-10 Batch 1:  Loss:     0.9620 Validation Accuracy: 0.725000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 387, CIFAR-10 Batch 2:  Loss:     0.5688 Validation Accuracy: 0.850000\n",
      "Epoch 387, CIFAR-10 Batch 3:  Loss:     0.5441 Validation Accuracy: 0.900000\n",
      "Epoch 387, CIFAR-10 Batch 4:  Loss:     0.9544 Validation Accuracy: 0.725000\n",
      "Epoch 387, CIFAR-10 Batch 5:  Loss:     0.8812 Validation Accuracy: 0.775000\n",
      "Epoch 388, CIFAR-10 Batch 1:  Loss:     0.9617 Validation Accuracy: 0.725000\n",
      "Epoch 388, CIFAR-10 Batch 2:  Loss:     0.5689 Validation Accuracy: 0.850000\n",
      "Epoch 388, CIFAR-10 Batch 3:  Loss:     0.5447 Validation Accuracy: 0.900000\n",
      "Epoch 388, CIFAR-10 Batch 4:  Loss:     0.9557 Validation Accuracy: 0.725000\n",
      "Epoch 388, CIFAR-10 Batch 5:  Loss:     0.8795 Validation Accuracy: 0.775000\n",
      "Epoch 389, CIFAR-10 Batch 1:  Loss:     0.9613 Validation Accuracy: 0.725000\n",
      "Epoch 389, CIFAR-10 Batch 2:  Loss:     0.5689 Validation Accuracy: 0.850000\n",
      "Epoch 389, CIFAR-10 Batch 3:  Loss:     0.5410 Validation Accuracy: 0.900000\n",
      "Epoch 389, CIFAR-10 Batch 4:  Loss:     0.9550 Validation Accuracy: 0.725000\n",
      "Epoch 389, CIFAR-10 Batch 5:  Loss:     0.8786 Validation Accuracy: 0.775000\n",
      "Epoch 390, CIFAR-10 Batch 1:  Loss:     0.9540 Validation Accuracy: 0.750000\n",
      "Epoch 390, CIFAR-10 Batch 2:  Loss:     0.5669 Validation Accuracy: 0.875000\n",
      "Epoch 390, CIFAR-10 Batch 3:  Loss:     0.5439 Validation Accuracy: 0.900000\n",
      "Epoch 390, CIFAR-10 Batch 4:  Loss:     0.9610 Validation Accuracy: 0.725000\n",
      "Epoch 390, CIFAR-10 Batch 5:  Loss:     0.8790 Validation Accuracy: 0.775000\n",
      "Epoch 391, CIFAR-10 Batch 1:  Loss:     0.9672 Validation Accuracy: 0.725000\n",
      "Epoch 391, CIFAR-10 Batch 2:  Loss:     0.5706 Validation Accuracy: 0.875000\n",
      "Epoch 391, CIFAR-10 Batch 3:  Loss:     0.5450 Validation Accuracy: 0.900000\n",
      "Epoch 391, CIFAR-10 Batch 4:  Loss:     0.9621 Validation Accuracy: 0.725000\n",
      "Epoch 391, CIFAR-10 Batch 5:  Loss:     0.8773 Validation Accuracy: 0.775000\n",
      "Epoch 392, CIFAR-10 Batch 1:  Loss:     0.9539 Validation Accuracy: 0.725000\n",
      "Epoch 392, CIFAR-10 Batch 2:  Loss:     0.5688 Validation Accuracy: 0.850000\n",
      "Epoch 392, CIFAR-10 Batch 3:  Loss:     0.5425 Validation Accuracy: 0.900000\n",
      "Epoch 392, CIFAR-10 Batch 4:  Loss:     0.9537 Validation Accuracy: 0.725000\n",
      "Epoch 392, CIFAR-10 Batch 5:  Loss:     0.8805 Validation Accuracy: 0.775000\n",
      "Epoch 393, CIFAR-10 Batch 1:  Loss:     0.9707 Validation Accuracy: 0.725000\n",
      "Epoch 393, CIFAR-10 Batch 2:  Loss:     0.5670 Validation Accuracy: 0.875000\n",
      "Epoch 393, CIFAR-10 Batch 3:  Loss:     0.5505 Validation Accuracy: 0.900000\n",
      "Epoch 393, CIFAR-10 Batch 4:  Loss:     0.9595 Validation Accuracy: 0.725000\n",
      "Epoch 393, CIFAR-10 Batch 5:  Loss:     0.8760 Validation Accuracy: 0.800000\n",
      "Epoch 394, CIFAR-10 Batch 1:  Loss:     0.9604 Validation Accuracy: 0.725000\n",
      "Epoch 394, CIFAR-10 Batch 2:  Loss:     0.5626 Validation Accuracy: 0.850000\n",
      "Epoch 394, CIFAR-10 Batch 3:  Loss:     0.5472 Validation Accuracy: 0.900000\n",
      "Epoch 394, CIFAR-10 Batch 4:  Loss:     0.9594 Validation Accuracy: 0.725000\n",
      "Epoch 394, CIFAR-10 Batch 5:  Loss:     0.8756 Validation Accuracy: 0.800000\n",
      "Epoch 395, CIFAR-10 Batch 1:  Loss:     0.9599 Validation Accuracy: 0.725000\n",
      "Epoch 395, CIFAR-10 Batch 2:  Loss:     0.5689 Validation Accuracy: 0.850000\n",
      "Epoch 395, CIFAR-10 Batch 3:  Loss:     0.5438 Validation Accuracy: 0.900000\n",
      "Epoch 395, CIFAR-10 Batch 4:  Loss:     0.9556 Validation Accuracy: 0.725000\n",
      "Epoch 395, CIFAR-10 Batch 5:  Loss:     0.8760 Validation Accuracy: 0.775000\n",
      "Epoch 396, CIFAR-10 Batch 1:  Loss:     0.9590 Validation Accuracy: 0.725000\n",
      "Epoch 396, CIFAR-10 Batch 2:  Loss:     0.5668 Validation Accuracy: 0.850000\n",
      "Epoch 396, CIFAR-10 Batch 3:  Loss:     0.5484 Validation Accuracy: 0.900000\n",
      "Epoch 396, CIFAR-10 Batch 4:  Loss:     0.9545 Validation Accuracy: 0.725000\n",
      "Epoch 396, CIFAR-10 Batch 5:  Loss:     0.8701 Validation Accuracy: 0.775000\n",
      "Epoch 397, CIFAR-10 Batch 1:  Loss:     0.9611 Validation Accuracy: 0.725000\n",
      "Epoch 397, CIFAR-10 Batch 2:  Loss:     0.5673 Validation Accuracy: 0.875000\n",
      "Epoch 397, CIFAR-10 Batch 3:  Loss:     0.5482 Validation Accuracy: 0.900000\n",
      "Epoch 397, CIFAR-10 Batch 4:  Loss:     0.9585 Validation Accuracy: 0.725000\n",
      "Epoch 397, CIFAR-10 Batch 5:  Loss:     0.8704 Validation Accuracy: 0.750000\n",
      "Epoch 398, CIFAR-10 Batch 1:  Loss:     0.9579 Validation Accuracy: 0.725000\n",
      "Epoch 398, CIFAR-10 Batch 2:  Loss:     0.5655 Validation Accuracy: 0.875000\n",
      "Epoch 398, CIFAR-10 Batch 3:  Loss:     0.5440 Validation Accuracy: 0.900000\n",
      "Epoch 398, CIFAR-10 Batch 4:  Loss:     0.9514 Validation Accuracy: 0.725000\n",
      "Epoch 398, CIFAR-10 Batch 5:  Loss:     0.8729 Validation Accuracy: 0.800000\n",
      "Epoch 399, CIFAR-10 Batch 1:  Loss:     0.9579 Validation Accuracy: 0.725000\n",
      "Epoch 399, CIFAR-10 Batch 2:  Loss:     0.5636 Validation Accuracy: 0.850000\n",
      "Epoch 399, CIFAR-10 Batch 3:  Loss:     0.5388 Validation Accuracy: 0.900000\n",
      "Epoch 399, CIFAR-10 Batch 4:  Loss:     0.9540 Validation Accuracy: 0.725000\n",
      "Epoch 399, CIFAR-10 Batch 5:  Loss:     0.8689 Validation Accuracy: 0.775000\n",
      "Epoch 400, CIFAR-10 Batch 1:  Loss:     0.9535 Validation Accuracy: 0.725000\n",
      "Epoch 400, CIFAR-10 Batch 2:  Loss:     0.5640 Validation Accuracy: 0.850000\n",
      "Epoch 400, CIFAR-10 Batch 3:  Loss:     0.5410 Validation Accuracy: 0.900000\n",
      "Epoch 400, CIFAR-10 Batch 4:  Loss:     0.9493 Validation Accuracy: 0.725000\n",
      "Epoch 400, CIFAR-10 Batch 5:  Loss:     0.8687 Validation Accuracy: 0.800000\n",
      "Epoch 401, CIFAR-10 Batch 1:  Loss:     0.9507 Validation Accuracy: 0.725000\n",
      "Epoch 401, CIFAR-10 Batch 2:  Loss:     0.5652 Validation Accuracy: 0.875000\n",
      "Epoch 401, CIFAR-10 Batch 3:  Loss:     0.5427 Validation Accuracy: 0.900000\n",
      "Epoch 401, CIFAR-10 Batch 4:  Loss:     0.9516 Validation Accuracy: 0.725000\n",
      "Epoch 401, CIFAR-10 Batch 5:  Loss:     0.8688 Validation Accuracy: 0.775000\n",
      "Epoch 402, CIFAR-10 Batch 1:  Loss:     0.9509 Validation Accuracy: 0.725000\n",
      "Epoch 402, CIFAR-10 Batch 2:  Loss:     0.5674 Validation Accuracy: 0.875000\n",
      "Epoch 402, CIFAR-10 Batch 3:  Loss:     0.5453 Validation Accuracy: 0.900000\n",
      "Epoch 402, CIFAR-10 Batch 4:  Loss:     0.9540 Validation Accuracy: 0.725000\n",
      "Epoch 402, CIFAR-10 Batch 5:  Loss:     0.8697 Validation Accuracy: 0.800000\n",
      "Epoch 403, CIFAR-10 Batch 1:  Loss:     0.9507 Validation Accuracy: 0.750000\n",
      "Epoch 403, CIFAR-10 Batch 2:  Loss:     0.5648 Validation Accuracy: 0.875000\n",
      "Epoch 403, CIFAR-10 Batch 3:  Loss:     0.5391 Validation Accuracy: 0.900000\n",
      "Epoch 403, CIFAR-10 Batch 4:  Loss:     0.9489 Validation Accuracy: 0.725000\n",
      "Epoch 403, CIFAR-10 Batch 5:  Loss:     0.8679 Validation Accuracy: 0.775000\n",
      "Epoch 404, CIFAR-10 Batch 1:  Loss:     0.9526 Validation Accuracy: 0.725000\n",
      "Epoch 404, CIFAR-10 Batch 2:  Loss:     0.5674 Validation Accuracy: 0.875000\n",
      "Epoch 404, CIFAR-10 Batch 3:  Loss:     0.5451 Validation Accuracy: 0.900000\n",
      "Epoch 404, CIFAR-10 Batch 4:  Loss:     0.9492 Validation Accuracy: 0.725000\n",
      "Epoch 404, CIFAR-10 Batch 5:  Loss:     0.8708 Validation Accuracy: 0.775000\n",
      "Epoch 405, CIFAR-10 Batch 1:  Loss:     0.9547 Validation Accuracy: 0.725000\n",
      "Epoch 405, CIFAR-10 Batch 2:  Loss:     0.5591 Validation Accuracy: 0.875000\n",
      "Epoch 405, CIFAR-10 Batch 3:  Loss:     0.5385 Validation Accuracy: 0.900000\n",
      "Epoch 405, CIFAR-10 Batch 4:  Loss:     0.9577 Validation Accuracy: 0.725000\n",
      "Epoch 405, CIFAR-10 Batch 5:  Loss:     0.8756 Validation Accuracy: 0.800000\n",
      "Epoch 406, CIFAR-10 Batch 1:  Loss:     0.9533 Validation Accuracy: 0.725000\n",
      "Epoch 406, CIFAR-10 Batch 2:  Loss:     0.5655 Validation Accuracy: 0.875000\n",
      "Epoch 406, CIFAR-10 Batch 3:  Loss:     0.5385 Validation Accuracy: 0.900000\n",
      "Epoch 406, CIFAR-10 Batch 4:  Loss:     0.9445 Validation Accuracy: 0.725000\n",
      "Epoch 406, CIFAR-10 Batch 5:  Loss:     0.8668 Validation Accuracy: 0.800000\n",
      "Epoch 407, CIFAR-10 Batch 1:  Loss:     0.9526 Validation Accuracy: 0.725000\n",
      "Epoch 407, CIFAR-10 Batch 2:  Loss:     0.5651 Validation Accuracy: 0.850000\n",
      "Epoch 407, CIFAR-10 Batch 3:  Loss:     0.5384 Validation Accuracy: 0.900000\n",
      "Epoch 407, CIFAR-10 Batch 4:  Loss:     0.9477 Validation Accuracy: 0.725000\n",
      "Epoch 407, CIFAR-10 Batch 5:  Loss:     0.8658 Validation Accuracy: 0.775000\n",
      "Epoch 408, CIFAR-10 Batch 1:  Loss:     0.9532 Validation Accuracy: 0.725000\n",
      "Epoch 408, CIFAR-10 Batch 2:  Loss:     0.5626 Validation Accuracy: 0.875000\n",
      "Epoch 408, CIFAR-10 Batch 3:  Loss:     0.5448 Validation Accuracy: 0.900000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 408, CIFAR-10 Batch 4:  Loss:     0.9523 Validation Accuracy: 0.725000\n",
      "Epoch 408, CIFAR-10 Batch 5:  Loss:     0.8608 Validation Accuracy: 0.800000\n",
      "Epoch 409, CIFAR-10 Batch 1:  Loss:     0.9506 Validation Accuracy: 0.725000\n",
      "Epoch 409, CIFAR-10 Batch 2:  Loss:     0.5628 Validation Accuracy: 0.875000\n",
      "Epoch 409, CIFAR-10 Batch 3:  Loss:     0.5496 Validation Accuracy: 0.900000\n",
      "Epoch 409, CIFAR-10 Batch 4:  Loss:     0.9480 Validation Accuracy: 0.725000\n",
      "Epoch 409, CIFAR-10 Batch 5:  Loss:     0.8659 Validation Accuracy: 0.800000\n",
      "Epoch 410, CIFAR-10 Batch 1:  Loss:     0.9495 Validation Accuracy: 0.725000\n",
      "Epoch 410, CIFAR-10 Batch 2:  Loss:     0.5583 Validation Accuracy: 0.875000\n",
      "Epoch 410, CIFAR-10 Batch 3:  Loss:     0.5437 Validation Accuracy: 0.900000\n",
      "Epoch 410, CIFAR-10 Batch 4:  Loss:     0.9523 Validation Accuracy: 0.725000\n",
      "Epoch 410, CIFAR-10 Batch 5:  Loss:     0.8668 Validation Accuracy: 0.775000\n",
      "Epoch 411, CIFAR-10 Batch 1:  Loss:     0.9523 Validation Accuracy: 0.725000\n",
      "Epoch 411, CIFAR-10 Batch 2:  Loss:     0.5600 Validation Accuracy: 0.875000\n",
      "Epoch 411, CIFAR-10 Batch 3:  Loss:     0.5416 Validation Accuracy: 0.900000\n",
      "Epoch 411, CIFAR-10 Batch 4:  Loss:     0.9487 Validation Accuracy: 0.725000\n",
      "Epoch 411, CIFAR-10 Batch 5:  Loss:     0.8621 Validation Accuracy: 0.800000\n",
      "Epoch 412, CIFAR-10 Batch 1:  Loss:     0.9511 Validation Accuracy: 0.725000\n",
      "Epoch 412, CIFAR-10 Batch 2:  Loss:     0.5623 Validation Accuracy: 0.850000\n",
      "Epoch 412, CIFAR-10 Batch 3:  Loss:     0.5430 Validation Accuracy: 0.900000\n",
      "Epoch 412, CIFAR-10 Batch 4:  Loss:     0.9505 Validation Accuracy: 0.725000\n",
      "Epoch 412, CIFAR-10 Batch 5:  Loss:     0.8670 Validation Accuracy: 0.775000\n",
      "Epoch 413, CIFAR-10 Batch 1:  Loss:     0.9476 Validation Accuracy: 0.725000\n",
      "Epoch 413, CIFAR-10 Batch 2:  Loss:     0.5614 Validation Accuracy: 0.875000\n",
      "Epoch 413, CIFAR-10 Batch 3:  Loss:     0.5384 Validation Accuracy: 0.900000\n",
      "Epoch 413, CIFAR-10 Batch 4:  Loss:     0.9566 Validation Accuracy: 0.725000\n",
      "Epoch 413, CIFAR-10 Batch 5:  Loss:     0.8684 Validation Accuracy: 0.800000\n",
      "Epoch 414, CIFAR-10 Batch 1:  Loss:     0.9464 Validation Accuracy: 0.725000\n",
      "Epoch 414, CIFAR-10 Batch 2:  Loss:     0.5648 Validation Accuracy: 0.875000\n",
      "Epoch 414, CIFAR-10 Batch 3:  Loss:     0.5447 Validation Accuracy: 0.900000\n",
      "Epoch 414, CIFAR-10 Batch 4:  Loss:     0.9544 Validation Accuracy: 0.725000\n",
      "Epoch 414, CIFAR-10 Batch 5:  Loss:     0.8619 Validation Accuracy: 0.775000\n",
      "Epoch 415, CIFAR-10 Batch 1:  Loss:     0.9523 Validation Accuracy: 0.725000\n",
      "Epoch 415, CIFAR-10 Batch 2:  Loss:     0.5615 Validation Accuracy: 0.875000\n",
      "Epoch 415, CIFAR-10 Batch 3:  Loss:     0.5551 Validation Accuracy: 0.900000\n",
      "Epoch 415, CIFAR-10 Batch 4:  Loss:     0.9550 Validation Accuracy: 0.725000\n",
      "Epoch 415, CIFAR-10 Batch 5:  Loss:     0.8621 Validation Accuracy: 0.775000\n",
      "Epoch 416, CIFAR-10 Batch 1:  Loss:     0.9540 Validation Accuracy: 0.725000\n",
      "Epoch 416, CIFAR-10 Batch 2:  Loss:     0.5610 Validation Accuracy: 0.875000\n",
      "Epoch 416, CIFAR-10 Batch 3:  Loss:     0.5458 Validation Accuracy: 0.900000\n",
      "Epoch 416, CIFAR-10 Batch 4:  Loss:     0.9569 Validation Accuracy: 0.750000\n",
      "Epoch 416, CIFAR-10 Batch 5:  Loss:     0.8660 Validation Accuracy: 0.750000\n",
      "Epoch 417, CIFAR-10 Batch 1:  Loss:     0.9644 Validation Accuracy: 0.725000\n",
      "Epoch 417, CIFAR-10 Batch 2:  Loss:     0.5656 Validation Accuracy: 0.875000\n",
      "Epoch 417, CIFAR-10 Batch 3:  Loss:     0.5383 Validation Accuracy: 0.900000\n",
      "Epoch 417, CIFAR-10 Batch 4:  Loss:     0.9570 Validation Accuracy: 0.725000\n",
      "Epoch 417, CIFAR-10 Batch 5:  Loss:     0.8655 Validation Accuracy: 0.775000\n",
      "Epoch 418, CIFAR-10 Batch 1:  Loss:     0.9522 Validation Accuracy: 0.750000\n",
      "Epoch 418, CIFAR-10 Batch 2:  Loss:     0.5665 Validation Accuracy: 0.875000\n",
      "Epoch 418, CIFAR-10 Batch 3:  Loss:     0.5391 Validation Accuracy: 0.900000\n",
      "Epoch 418, CIFAR-10 Batch 4:  Loss:     0.9609 Validation Accuracy: 0.725000\n",
      "Epoch 418, CIFAR-10 Batch 5:  Loss:     0.8638 Validation Accuracy: 0.800000\n",
      "Epoch 419, CIFAR-10 Batch 1:  Loss:     0.9470 Validation Accuracy: 0.725000\n",
      "Epoch 419, CIFAR-10 Batch 2:  Loss:     0.5662 Validation Accuracy: 0.875000\n",
      "Epoch 419, CIFAR-10 Batch 3:  Loss:     0.5399 Validation Accuracy: 0.900000\n",
      "Epoch 419, CIFAR-10 Batch 4:  Loss:     0.9647 Validation Accuracy: 0.725000\n",
      "Epoch 419, CIFAR-10 Batch 5:  Loss:     0.8692 Validation Accuracy: 0.775000\n",
      "Epoch 420, CIFAR-10 Batch 1:  Loss:     0.9668 Validation Accuracy: 0.725000\n",
      "Epoch 420, CIFAR-10 Batch 2:  Loss:     0.5615 Validation Accuracy: 0.875000\n",
      "Epoch 420, CIFAR-10 Batch 3:  Loss:     0.5366 Validation Accuracy: 0.900000\n",
      "Epoch 420, CIFAR-10 Batch 4:  Loss:     0.9594 Validation Accuracy: 0.725000\n",
      "Epoch 420, CIFAR-10 Batch 5:  Loss:     0.8630 Validation Accuracy: 0.800000\n",
      "Epoch 421, CIFAR-10 Batch 1:  Loss:     0.9560 Validation Accuracy: 0.725000\n",
      "Epoch 421, CIFAR-10 Batch 2:  Loss:     0.5605 Validation Accuracy: 0.875000\n",
      "Epoch 421, CIFAR-10 Batch 3:  Loss:     0.5439 Validation Accuracy: 0.900000\n",
      "Epoch 421, CIFAR-10 Batch 4:  Loss:     0.9567 Validation Accuracy: 0.725000\n",
      "Epoch 421, CIFAR-10 Batch 5:  Loss:     0.8627 Validation Accuracy: 0.800000\n",
      "Epoch 422, CIFAR-10 Batch 1:  Loss:     0.9437 Validation Accuracy: 0.725000\n",
      "Epoch 422, CIFAR-10 Batch 2:  Loss:     0.5563 Validation Accuracy: 0.850000\n",
      "Epoch 422, CIFAR-10 Batch 3:  Loss:     0.5352 Validation Accuracy: 0.900000\n",
      "Epoch 422, CIFAR-10 Batch 4:  Loss:     0.9544 Validation Accuracy: 0.725000\n",
      "Epoch 422, CIFAR-10 Batch 5:  Loss:     0.8614 Validation Accuracy: 0.800000\n",
      "Epoch 423, CIFAR-10 Batch 1:  Loss:     0.9594 Validation Accuracy: 0.725000\n",
      "Epoch 423, CIFAR-10 Batch 2:  Loss:     0.5632 Validation Accuracy: 0.875000\n",
      "Epoch 423, CIFAR-10 Batch 3:  Loss:     0.5322 Validation Accuracy: 0.900000\n",
      "Epoch 423, CIFAR-10 Batch 4:  Loss:     0.9598 Validation Accuracy: 0.725000\n",
      "Epoch 423, CIFAR-10 Batch 5:  Loss:     0.8681 Validation Accuracy: 0.800000\n",
      "Epoch 424, CIFAR-10 Batch 1:  Loss:     0.9548 Validation Accuracy: 0.725000\n",
      "Epoch 424, CIFAR-10 Batch 2:  Loss:     0.5647 Validation Accuracy: 0.875000\n",
      "Epoch 424, CIFAR-10 Batch 3:  Loss:     0.5363 Validation Accuracy: 0.900000\n",
      "Epoch 424, CIFAR-10 Batch 4:  Loss:     0.9543 Validation Accuracy: 0.725000\n",
      "Epoch 424, CIFAR-10 Batch 5:  Loss:     0.8608 Validation Accuracy: 0.800000\n",
      "Epoch 425, CIFAR-10 Batch 1:  Loss:     0.9473 Validation Accuracy: 0.750000\n",
      "Epoch 425, CIFAR-10 Batch 2:  Loss:     0.5557 Validation Accuracy: 0.875000\n",
      "Epoch 425, CIFAR-10 Batch 3:  Loss:     0.5385 Validation Accuracy: 0.900000\n",
      "Epoch 425, CIFAR-10 Batch 4:  Loss:     0.9469 Validation Accuracy: 0.725000\n",
      "Epoch 425, CIFAR-10 Batch 5:  Loss:     0.8641 Validation Accuracy: 0.800000\n",
      "Epoch 426, CIFAR-10 Batch 1:  Loss:     0.9447 Validation Accuracy: 0.725000\n",
      "Epoch 426, CIFAR-10 Batch 2:  Loss:     0.5558 Validation Accuracy: 0.875000\n",
      "Epoch 426, CIFAR-10 Batch 3:  Loss:     0.5352 Validation Accuracy: 0.900000\n",
      "Epoch 426, CIFAR-10 Batch 4:  Loss:     0.9500 Validation Accuracy: 0.725000\n",
      "Epoch 426, CIFAR-10 Batch 5:  Loss:     0.8648 Validation Accuracy: 0.800000\n",
      "Epoch 427, CIFAR-10 Batch 1:  Loss:     0.9459 Validation Accuracy: 0.725000\n",
      "Epoch 427, CIFAR-10 Batch 2:  Loss:     0.5572 Validation Accuracy: 0.875000\n",
      "Epoch 427, CIFAR-10 Batch 3:  Loss:     0.5410 Validation Accuracy: 0.900000\n",
      "Epoch 427, CIFAR-10 Batch 4:  Loss:     0.9475 Validation Accuracy: 0.725000\n",
      "Epoch 427, CIFAR-10 Batch 5:  Loss:     0.8612 Validation Accuracy: 0.800000\n",
      "Epoch 428, CIFAR-10 Batch 1:  Loss:     0.9477 Validation Accuracy: 0.725000\n",
      "Epoch 428, CIFAR-10 Batch 2:  Loss:     0.5592 Validation Accuracy: 0.875000\n",
      "Epoch 428, CIFAR-10 Batch 3:  Loss:     0.5392 Validation Accuracy: 0.900000\n",
      "Epoch 428, CIFAR-10 Batch 4:  Loss:     0.9577 Validation Accuracy: 0.725000\n",
      "Epoch 428, CIFAR-10 Batch 5:  Loss:     0.8641 Validation Accuracy: 0.800000\n",
      "Epoch 429, CIFAR-10 Batch 1:  Loss:     0.9380 Validation Accuracy: 0.725000\n",
      "Epoch 429, CIFAR-10 Batch 2:  Loss:     0.5560 Validation Accuracy: 0.875000\n",
      "Epoch 429, CIFAR-10 Batch 3:  Loss:     0.5334 Validation Accuracy: 0.900000\n",
      "Epoch 429, CIFAR-10 Batch 4:  Loss:     0.9525 Validation Accuracy: 0.725000\n",
      "Epoch 429, CIFAR-10 Batch 5:  Loss:     0.8598 Validation Accuracy: 0.800000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 430, CIFAR-10 Batch 1:  Loss:     0.9448 Validation Accuracy: 0.725000\n",
      "Epoch 430, CIFAR-10 Batch 2:  Loss:     0.5624 Validation Accuracy: 0.875000\n",
      "Epoch 430, CIFAR-10 Batch 3:  Loss:     0.5370 Validation Accuracy: 0.900000\n",
      "Epoch 430, CIFAR-10 Batch 4:  Loss:     0.9529 Validation Accuracy: 0.725000\n",
      "Epoch 430, CIFAR-10 Batch 5:  Loss:     0.8620 Validation Accuracy: 0.800000\n",
      "Epoch 431, CIFAR-10 Batch 1:  Loss:     0.9408 Validation Accuracy: 0.725000\n",
      "Epoch 431, CIFAR-10 Batch 2:  Loss:     0.5610 Validation Accuracy: 0.875000\n",
      "Epoch 431, CIFAR-10 Batch 3:  Loss:     0.5300 Validation Accuracy: 0.900000\n",
      "Epoch 431, CIFAR-10 Batch 4:  Loss:     0.9539 Validation Accuracy: 0.725000\n",
      "Epoch 431, CIFAR-10 Batch 5:  Loss:     0.8555 Validation Accuracy: 0.800000\n",
      "Epoch 432, CIFAR-10 Batch 1:  Loss:     0.9381 Validation Accuracy: 0.725000\n",
      "Epoch 432, CIFAR-10 Batch 2:  Loss:     0.5637 Validation Accuracy: 0.875000\n",
      "Epoch 432, CIFAR-10 Batch 3:  Loss:     0.5368 Validation Accuracy: 0.900000\n",
      "Epoch 432, CIFAR-10 Batch 4:  Loss:     0.9610 Validation Accuracy: 0.725000\n",
      "Epoch 432, CIFAR-10 Batch 5:  Loss:     0.8657 Validation Accuracy: 0.800000\n",
      "Epoch 433, CIFAR-10 Batch 1:  Loss:     0.9572 Validation Accuracy: 0.725000\n",
      "Epoch 433, CIFAR-10 Batch 2:  Loss:     0.5660 Validation Accuracy: 0.875000\n",
      "Epoch 433, CIFAR-10 Batch 3:  Loss:     0.5337 Validation Accuracy: 0.900000\n",
      "Epoch 433, CIFAR-10 Batch 4:  Loss:     0.9591 Validation Accuracy: 0.725000\n",
      "Epoch 433, CIFAR-10 Batch 5:  Loss:     0.8661 Validation Accuracy: 0.800000\n",
      "Epoch 434, CIFAR-10 Batch 1:  Loss:     0.9472 Validation Accuracy: 0.725000\n",
      "Epoch 434, CIFAR-10 Batch 2:  Loss:     0.5569 Validation Accuracy: 0.875000\n",
      "Epoch 434, CIFAR-10 Batch 3:  Loss:     0.5378 Validation Accuracy: 0.900000\n",
      "Epoch 434, CIFAR-10 Batch 4:  Loss:     0.9486 Validation Accuracy: 0.725000\n",
      "Epoch 434, CIFAR-10 Batch 5:  Loss:     0.8593 Validation Accuracy: 0.800000\n",
      "Epoch 435, CIFAR-10 Batch 1:  Loss:     0.9399 Validation Accuracy: 0.725000\n",
      "Epoch 435, CIFAR-10 Batch 2:  Loss:     0.5576 Validation Accuracy: 0.875000\n",
      "Epoch 435, CIFAR-10 Batch 3:  Loss:     0.5388 Validation Accuracy: 0.900000\n",
      "Epoch 435, CIFAR-10 Batch 4:  Loss:     0.9519 Validation Accuracy: 0.725000\n",
      "Epoch 435, CIFAR-10 Batch 5:  Loss:     0.8548 Validation Accuracy: 0.775000\n",
      "Epoch 436, CIFAR-10 Batch 1:  Loss:     0.9453 Validation Accuracy: 0.725000\n",
      "Epoch 436, CIFAR-10 Batch 2:  Loss:     0.5577 Validation Accuracy: 0.875000\n",
      "Epoch 436, CIFAR-10 Batch 3:  Loss:     0.5345 Validation Accuracy: 0.900000\n",
      "Epoch 436, CIFAR-10 Batch 4:  Loss:     0.9511 Validation Accuracy: 0.725000\n",
      "Epoch 436, CIFAR-10 Batch 5:  Loss:     0.8576 Validation Accuracy: 0.800000\n",
      "Epoch 437, CIFAR-10 Batch 1:  Loss:     0.9393 Validation Accuracy: 0.725000\n",
      "Epoch 437, CIFAR-10 Batch 2:  Loss:     0.5589 Validation Accuracy: 0.875000\n",
      "Epoch 437, CIFAR-10 Batch 3:  Loss:     0.5326 Validation Accuracy: 0.900000\n",
      "Epoch 437, CIFAR-10 Batch 4:  Loss:     0.9649 Validation Accuracy: 0.725000\n",
      "Epoch 437, CIFAR-10 Batch 5:  Loss:     0.8602 Validation Accuracy: 0.800000\n",
      "Epoch 438, CIFAR-10 Batch 1:  Loss:     0.9409 Validation Accuracy: 0.750000\n",
      "Epoch 438, CIFAR-10 Batch 2:  Loss:     0.5609 Validation Accuracy: 0.875000\n",
      "Epoch 438, CIFAR-10 Batch 3:  Loss:     0.5323 Validation Accuracy: 0.900000\n",
      "Epoch 438, CIFAR-10 Batch 4:  Loss:     0.9549 Validation Accuracy: 0.725000\n",
      "Epoch 438, CIFAR-10 Batch 5:  Loss:     0.8671 Validation Accuracy: 0.800000\n",
      "Epoch 439, CIFAR-10 Batch 1:  Loss:     0.9375 Validation Accuracy: 0.725000\n",
      "Epoch 439, CIFAR-10 Batch 2:  Loss:     0.5638 Validation Accuracy: 0.875000\n",
      "Epoch 439, CIFAR-10 Batch 3:  Loss:     0.5326 Validation Accuracy: 0.900000\n",
      "Epoch 439, CIFAR-10 Batch 4:  Loss:     0.9615 Validation Accuracy: 0.725000\n",
      "Epoch 439, CIFAR-10 Batch 5:  Loss:     0.8569 Validation Accuracy: 0.800000\n",
      "Epoch 440, CIFAR-10 Batch 1:  Loss:     0.9384 Validation Accuracy: 0.725000\n",
      "Epoch 440, CIFAR-10 Batch 2:  Loss:     0.5522 Validation Accuracy: 0.875000\n",
      "Epoch 440, CIFAR-10 Batch 3:  Loss:     0.5358 Validation Accuracy: 0.900000\n",
      "Epoch 440, CIFAR-10 Batch 4:  Loss:     0.9535 Validation Accuracy: 0.700000\n",
      "Epoch 440, CIFAR-10 Batch 5:  Loss:     0.8578 Validation Accuracy: 0.800000\n",
      "Epoch 441, CIFAR-10 Batch 1:  Loss:     0.9389 Validation Accuracy: 0.725000\n",
      "Epoch 441, CIFAR-10 Batch 2:  Loss:     0.5591 Validation Accuracy: 0.875000\n",
      "Epoch 441, CIFAR-10 Batch 3:  Loss:     0.5337 Validation Accuracy: 0.900000\n",
      "Epoch 441, CIFAR-10 Batch 4:  Loss:     0.9519 Validation Accuracy: 0.725000\n",
      "Epoch 441, CIFAR-10 Batch 5:  Loss:     0.8658 Validation Accuracy: 0.775000\n",
      "Epoch 442, CIFAR-10 Batch 1:  Loss:     0.9460 Validation Accuracy: 0.725000\n",
      "Epoch 442, CIFAR-10 Batch 2:  Loss:     0.5529 Validation Accuracy: 0.875000\n",
      "Epoch 442, CIFAR-10 Batch 3:  Loss:     0.5400 Validation Accuracy: 0.900000\n",
      "Epoch 442, CIFAR-10 Batch 4:  Loss:     0.9516 Validation Accuracy: 0.725000\n",
      "Epoch 442, CIFAR-10 Batch 5:  Loss:     0.8568 Validation Accuracy: 0.800000\n",
      "Epoch 443, CIFAR-10 Batch 1:  Loss:     0.9415 Validation Accuracy: 0.750000\n",
      "Epoch 443, CIFAR-10 Batch 2:  Loss:     0.5609 Validation Accuracy: 0.875000\n",
      "Epoch 443, CIFAR-10 Batch 3:  Loss:     0.5341 Validation Accuracy: 0.900000\n",
      "Epoch 443, CIFAR-10 Batch 4:  Loss:     0.9446 Validation Accuracy: 0.725000\n",
      "Epoch 443, CIFAR-10 Batch 5:  Loss:     0.8558 Validation Accuracy: 0.775000\n",
      "Epoch 444, CIFAR-10 Batch 1:  Loss:     0.9441 Validation Accuracy: 0.750000\n",
      "Epoch 444, CIFAR-10 Batch 2:  Loss:     0.5543 Validation Accuracy: 0.850000\n",
      "Epoch 444, CIFAR-10 Batch 3:  Loss:     0.5292 Validation Accuracy: 0.900000\n",
      "Epoch 444, CIFAR-10 Batch 4:  Loss:     0.9504 Validation Accuracy: 0.725000\n",
      "Epoch 444, CIFAR-10 Batch 5:  Loss:     0.8572 Validation Accuracy: 0.775000\n",
      "Epoch 445, CIFAR-10 Batch 1:  Loss:     0.9297 Validation Accuracy: 0.725000\n",
      "Epoch 445, CIFAR-10 Batch 2:  Loss:     0.5590 Validation Accuracy: 0.875000\n",
      "Epoch 445, CIFAR-10 Batch 3:  Loss:     0.5317 Validation Accuracy: 0.900000\n",
      "Epoch 445, CIFAR-10 Batch 4:  Loss:     0.9521 Validation Accuracy: 0.725000\n",
      "Epoch 445, CIFAR-10 Batch 5:  Loss:     0.8570 Validation Accuracy: 0.800000\n",
      "Epoch 446, CIFAR-10 Batch 1:  Loss:     0.9326 Validation Accuracy: 0.725000\n",
      "Epoch 446, CIFAR-10 Batch 2:  Loss:     0.5587 Validation Accuracy: 0.875000\n",
      "Epoch 446, CIFAR-10 Batch 3:  Loss:     0.5346 Validation Accuracy: 0.900000\n",
      "Epoch 446, CIFAR-10 Batch 4:  Loss:     0.9558 Validation Accuracy: 0.725000\n",
      "Epoch 446, CIFAR-10 Batch 5:  Loss:     0.8638 Validation Accuracy: 0.800000\n",
      "Epoch 447, CIFAR-10 Batch 1:  Loss:     0.9372 Validation Accuracy: 0.725000\n",
      "Epoch 447, CIFAR-10 Batch 2:  Loss:     0.5630 Validation Accuracy: 0.875000\n",
      "Epoch 447, CIFAR-10 Batch 3:  Loss:     0.5376 Validation Accuracy: 0.900000\n",
      "Epoch 447, CIFAR-10 Batch 4:  Loss:     0.9557 Validation Accuracy: 0.725000\n",
      "Epoch 447, CIFAR-10 Batch 5:  Loss:     0.8599 Validation Accuracy: 0.800000\n",
      "Epoch 448, CIFAR-10 Batch 1:  Loss:     0.9351 Validation Accuracy: 0.725000\n",
      "Epoch 448, CIFAR-10 Batch 2:  Loss:     0.5626 Validation Accuracy: 0.850000\n",
      "Epoch 448, CIFAR-10 Batch 3:  Loss:     0.5298 Validation Accuracy: 0.900000\n",
      "Epoch 448, CIFAR-10 Batch 4:  Loss:     0.9631 Validation Accuracy: 0.725000\n",
      "Epoch 448, CIFAR-10 Batch 5:  Loss:     0.8567 Validation Accuracy: 0.800000\n",
      "Epoch 449, CIFAR-10 Batch 1:  Loss:     0.9405 Validation Accuracy: 0.750000\n",
      "Epoch 449, CIFAR-10 Batch 2:  Loss:     0.5646 Validation Accuracy: 0.850000\n",
      "Epoch 449, CIFAR-10 Batch 3:  Loss:     0.5412 Validation Accuracy: 0.900000\n",
      "Epoch 449, CIFAR-10 Batch 4:  Loss:     0.9568 Validation Accuracy: 0.725000\n",
      "Epoch 449, CIFAR-10 Batch 5:  Loss:     0.8545 Validation Accuracy: 0.800000\n",
      "Epoch 450, CIFAR-10 Batch 1:  Loss:     0.9344 Validation Accuracy: 0.725000\n",
      "Epoch 450, CIFAR-10 Batch 2:  Loss:     0.5598 Validation Accuracy: 0.875000\n",
      "Epoch 450, CIFAR-10 Batch 3:  Loss:     0.5367 Validation Accuracy: 0.900000\n",
      "Epoch 450, CIFAR-10 Batch 4:  Loss:     0.9561 Validation Accuracy: 0.725000\n",
      "Epoch 450, CIFAR-10 Batch 5:  Loss:     0.8528 Validation Accuracy: 0.800000\n",
      "Epoch 451, CIFAR-10 Batch 1:  Loss:     0.9314 Validation Accuracy: 0.725000\n",
      "Epoch 451, CIFAR-10 Batch 2:  Loss:     0.5600 Validation Accuracy: 0.875000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 451, CIFAR-10 Batch 3:  Loss:     0.5332 Validation Accuracy: 0.900000\n",
      "Epoch 451, CIFAR-10 Batch 4:  Loss:     0.9553 Validation Accuracy: 0.725000\n",
      "Epoch 451, CIFAR-10 Batch 5:  Loss:     0.8553 Validation Accuracy: 0.800000\n",
      "Epoch 452, CIFAR-10 Batch 1:  Loss:     0.9257 Validation Accuracy: 0.725000\n",
      "Epoch 452, CIFAR-10 Batch 2:  Loss:     0.5584 Validation Accuracy: 0.875000\n",
      "Epoch 452, CIFAR-10 Batch 3:  Loss:     0.5305 Validation Accuracy: 0.900000\n",
      "Epoch 452, CIFAR-10 Batch 4:  Loss:     0.9549 Validation Accuracy: 0.725000\n",
      "Epoch 452, CIFAR-10 Batch 5:  Loss:     0.8597 Validation Accuracy: 0.800000\n",
      "Epoch 453, CIFAR-10 Batch 1:  Loss:     0.9345 Validation Accuracy: 0.725000\n",
      "Epoch 453, CIFAR-10 Batch 2:  Loss:     0.5590 Validation Accuracy: 0.850000\n",
      "Epoch 453, CIFAR-10 Batch 3:  Loss:     0.5389 Validation Accuracy: 0.900000\n",
      "Epoch 453, CIFAR-10 Batch 4:  Loss:     0.9660 Validation Accuracy: 0.725000\n",
      "Epoch 453, CIFAR-10 Batch 5:  Loss:     0.8587 Validation Accuracy: 0.800000\n",
      "Epoch 454, CIFAR-10 Batch 1:  Loss:     0.9393 Validation Accuracy: 0.725000\n",
      "Epoch 454, CIFAR-10 Batch 2:  Loss:     0.5630 Validation Accuracy: 0.875000\n",
      "Epoch 454, CIFAR-10 Batch 3:  Loss:     0.5338 Validation Accuracy: 0.900000\n",
      "Epoch 454, CIFAR-10 Batch 4:  Loss:     0.9580 Validation Accuracy: 0.725000\n",
      "Epoch 454, CIFAR-10 Batch 5:  Loss:     0.8538 Validation Accuracy: 0.800000\n",
      "Epoch 455, CIFAR-10 Batch 1:  Loss:     0.9328 Validation Accuracy: 0.725000\n",
      "Epoch 455, CIFAR-10 Batch 2:  Loss:     0.5690 Validation Accuracy: 0.875000\n",
      "Epoch 455, CIFAR-10 Batch 3:  Loss:     0.5391 Validation Accuracy: 0.900000\n",
      "Epoch 455, CIFAR-10 Batch 4:  Loss:     0.9599 Validation Accuracy: 0.725000\n",
      "Epoch 455, CIFAR-10 Batch 5:  Loss:     0.8572 Validation Accuracy: 0.800000\n",
      "Epoch 456, CIFAR-10 Batch 1:  Loss:     0.9277 Validation Accuracy: 0.750000\n",
      "Epoch 456, CIFAR-10 Batch 2:  Loss:     0.5601 Validation Accuracy: 0.875000\n",
      "Epoch 456, CIFAR-10 Batch 3:  Loss:     0.5361 Validation Accuracy: 0.900000\n",
      "Epoch 456, CIFAR-10 Batch 4:  Loss:     0.9544 Validation Accuracy: 0.725000\n",
      "Epoch 456, CIFAR-10 Batch 5:  Loss:     0.8595 Validation Accuracy: 0.800000\n",
      "Epoch 457, CIFAR-10 Batch 1:  Loss:     0.9365 Validation Accuracy: 0.725000\n",
      "Epoch 457, CIFAR-10 Batch 2:  Loss:     0.5581 Validation Accuracy: 0.875000\n",
      "Epoch 457, CIFAR-10 Batch 3:  Loss:     0.5420 Validation Accuracy: 0.900000\n",
      "Epoch 457, CIFAR-10 Batch 4:  Loss:     0.9516 Validation Accuracy: 0.725000\n",
      "Epoch 457, CIFAR-10 Batch 5:  Loss:     0.8572 Validation Accuracy: 0.800000\n",
      "Epoch 458, CIFAR-10 Batch 1:  Loss:     0.9332 Validation Accuracy: 0.725000\n",
      "Epoch 458, CIFAR-10 Batch 2:  Loss:     0.5577 Validation Accuracy: 0.875000\n",
      "Epoch 458, CIFAR-10 Batch 3:  Loss:     0.5372 Validation Accuracy: 0.900000\n",
      "Epoch 458, CIFAR-10 Batch 4:  Loss:     0.9583 Validation Accuracy: 0.725000\n",
      "Epoch 458, CIFAR-10 Batch 5:  Loss:     0.8592 Validation Accuracy: 0.800000\n",
      "Epoch 459, CIFAR-10 Batch 1:  Loss:     0.9338 Validation Accuracy: 0.725000\n",
      "Epoch 459, CIFAR-10 Batch 2:  Loss:     0.5654 Validation Accuracy: 0.875000\n",
      "Epoch 459, CIFAR-10 Batch 3:  Loss:     0.5382 Validation Accuracy: 0.900000\n",
      "Epoch 459, CIFAR-10 Batch 4:  Loss:     0.9501 Validation Accuracy: 0.725000\n",
      "Epoch 459, CIFAR-10 Batch 5:  Loss:     0.8636 Validation Accuracy: 0.800000\n",
      "Epoch 460, CIFAR-10 Batch 1:  Loss:     0.9280 Validation Accuracy: 0.725000\n",
      "Epoch 460, CIFAR-10 Batch 2:  Loss:     0.5560 Validation Accuracy: 0.875000\n",
      "Epoch 460, CIFAR-10 Batch 3:  Loss:     0.5365 Validation Accuracy: 0.900000\n",
      "Epoch 460, CIFAR-10 Batch 4:  Loss:     0.9568 Validation Accuracy: 0.725000\n",
      "Epoch 460, CIFAR-10 Batch 5:  Loss:     0.8588 Validation Accuracy: 0.775000\n",
      "Epoch 461, CIFAR-10 Batch 1:  Loss:     0.9268 Validation Accuracy: 0.725000\n",
      "Epoch 461, CIFAR-10 Batch 2:  Loss:     0.5625 Validation Accuracy: 0.875000\n",
      "Epoch 461, CIFAR-10 Batch 3:  Loss:     0.5437 Validation Accuracy: 0.900000\n",
      "Epoch 461, CIFAR-10 Batch 4:  Loss:     0.9611 Validation Accuracy: 0.725000\n",
      "Epoch 461, CIFAR-10 Batch 5:  Loss:     0.8569 Validation Accuracy: 0.800000\n",
      "Epoch 462, CIFAR-10 Batch 1:  Loss:     0.9323 Validation Accuracy: 0.725000\n",
      "Epoch 462, CIFAR-10 Batch 2:  Loss:     0.5620 Validation Accuracy: 0.875000\n",
      "Epoch 462, CIFAR-10 Batch 3:  Loss:     0.5333 Validation Accuracy: 0.900000\n",
      "Epoch 462, CIFAR-10 Batch 4:  Loss:     0.9618 Validation Accuracy: 0.725000\n",
      "Epoch 462, CIFAR-10 Batch 5:  Loss:     0.8587 Validation Accuracy: 0.800000\n",
      "Epoch 463, CIFAR-10 Batch 1:  Loss:     0.9360 Validation Accuracy: 0.725000\n",
      "Epoch 463, CIFAR-10 Batch 2:  Loss:     0.5576 Validation Accuracy: 0.875000\n",
      "Epoch 463, CIFAR-10 Batch 3:  Loss:     0.5348 Validation Accuracy: 0.900000\n",
      "Epoch 463, CIFAR-10 Batch 4:  Loss:     0.9501 Validation Accuracy: 0.725000\n",
      "Epoch 463, CIFAR-10 Batch 5:  Loss:     0.8542 Validation Accuracy: 0.800000\n",
      "Epoch 464, CIFAR-10 Batch 1:  Loss:     0.9323 Validation Accuracy: 0.725000\n",
      "Epoch 464, CIFAR-10 Batch 2:  Loss:     0.5568 Validation Accuracy: 0.875000\n",
      "Epoch 464, CIFAR-10 Batch 3:  Loss:     0.5366 Validation Accuracy: 0.900000\n",
      "Epoch 464, CIFAR-10 Batch 4:  Loss:     0.9555 Validation Accuracy: 0.725000\n",
      "Epoch 464, CIFAR-10 Batch 5:  Loss:     0.8557 Validation Accuracy: 0.775000\n",
      "Epoch 465, CIFAR-10 Batch 1:  Loss:     0.9318 Validation Accuracy: 0.725000\n",
      "Epoch 465, CIFAR-10 Batch 2:  Loss:     0.5585 Validation Accuracy: 0.875000\n",
      "Epoch 465, CIFAR-10 Batch 3:  Loss:     0.5407 Validation Accuracy: 0.900000\n",
      "Epoch 465, CIFAR-10 Batch 4:  Loss:     0.9500 Validation Accuracy: 0.725000\n",
      "Epoch 465, CIFAR-10 Batch 5:  Loss:     0.8541 Validation Accuracy: 0.800000\n",
      "Epoch 466, CIFAR-10 Batch 1:  Loss:     0.9292 Validation Accuracy: 0.725000\n",
      "Epoch 466, CIFAR-10 Batch 2:  Loss:     0.5532 Validation Accuracy: 0.875000\n",
      "Epoch 466, CIFAR-10 Batch 3:  Loss:     0.5375 Validation Accuracy: 0.900000\n",
      "Epoch 466, CIFAR-10 Batch 4:  Loss:     0.9491 Validation Accuracy: 0.725000\n",
      "Epoch 466, CIFAR-10 Batch 5:  Loss:     0.8534 Validation Accuracy: 0.800000\n",
      "Epoch 467, CIFAR-10 Batch 1:  Loss:     0.9347 Validation Accuracy: 0.725000\n",
      "Epoch 467, CIFAR-10 Batch 2:  Loss:     0.5575 Validation Accuracy: 0.875000\n",
      "Epoch 467, CIFAR-10 Batch 3:  Loss:     0.5338 Validation Accuracy: 0.900000\n",
      "Epoch 467, CIFAR-10 Batch 4:  Loss:     0.9535 Validation Accuracy: 0.725000\n",
      "Epoch 467, CIFAR-10 Batch 5:  Loss:     0.8549 Validation Accuracy: 0.800000\n",
      "Epoch 468, CIFAR-10 Batch 1:  Loss:     0.9273 Validation Accuracy: 0.725000\n",
      "Epoch 468, CIFAR-10 Batch 2:  Loss:     0.5603 Validation Accuracy: 0.875000\n",
      "Epoch 468, CIFAR-10 Batch 3:  Loss:     0.5414 Validation Accuracy: 0.900000\n",
      "Epoch 468, CIFAR-10 Batch 4:  Loss:     0.9529 Validation Accuracy: 0.725000\n",
      "Epoch 468, CIFAR-10 Batch 5:  Loss:     0.8538 Validation Accuracy: 0.775000\n",
      "Epoch 469, CIFAR-10 Batch 1:  Loss:     0.9297 Validation Accuracy: 0.725000\n",
      "Epoch 469, CIFAR-10 Batch 2:  Loss:     0.5526 Validation Accuracy: 0.875000\n",
      "Epoch 469, CIFAR-10 Batch 3:  Loss:     0.5396 Validation Accuracy: 0.900000\n",
      "Epoch 469, CIFAR-10 Batch 4:  Loss:     0.9499 Validation Accuracy: 0.725000\n",
      "Epoch 469, CIFAR-10 Batch 5:  Loss:     0.8561 Validation Accuracy: 0.800000\n",
      "Epoch 470, CIFAR-10 Batch 1:  Loss:     0.9284 Validation Accuracy: 0.725000\n",
      "Epoch 470, CIFAR-10 Batch 2:  Loss:     0.5603 Validation Accuracy: 0.875000\n",
      "Epoch 470, CIFAR-10 Batch 3:  Loss:     0.5372 Validation Accuracy: 0.900000\n",
      "Epoch 470, CIFAR-10 Batch 4:  Loss:     0.9474 Validation Accuracy: 0.725000\n",
      "Epoch 470, CIFAR-10 Batch 5:  Loss:     0.8566 Validation Accuracy: 0.800000\n",
      "Epoch 471, CIFAR-10 Batch 1:  Loss:     0.9255 Validation Accuracy: 0.725000\n",
      "Epoch 471, CIFAR-10 Batch 2:  Loss:     0.5530 Validation Accuracy: 0.875000\n",
      "Epoch 471, CIFAR-10 Batch 3:  Loss:     0.5376 Validation Accuracy: 0.900000\n",
      "Epoch 471, CIFAR-10 Batch 4:  Loss:     0.9518 Validation Accuracy: 0.725000\n",
      "Epoch 471, CIFAR-10 Batch 5:  Loss:     0.8565 Validation Accuracy: 0.800000\n",
      "Epoch 472, CIFAR-10 Batch 1:  Loss:     0.9324 Validation Accuracy: 0.725000\n",
      "Epoch 472, CIFAR-10 Batch 2:  Loss:     0.5540 Validation Accuracy: 0.875000\n",
      "Epoch 472, CIFAR-10 Batch 3:  Loss:     0.5342 Validation Accuracy: 0.900000\n",
      "Epoch 472, CIFAR-10 Batch 4:  Loss:     0.9489 Validation Accuracy: 0.725000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 472, CIFAR-10 Batch 5:  Loss:     0.8582 Validation Accuracy: 0.775000\n",
      "Epoch 473, CIFAR-10 Batch 1:  Loss:     0.9346 Validation Accuracy: 0.750000\n",
      "Epoch 473, CIFAR-10 Batch 2:  Loss:     0.5573 Validation Accuracy: 0.875000\n",
      "Epoch 473, CIFAR-10 Batch 3:  Loss:     0.5370 Validation Accuracy: 0.900000\n",
      "Epoch 473, CIFAR-10 Batch 4:  Loss:     0.9541 Validation Accuracy: 0.725000\n",
      "Epoch 473, CIFAR-10 Batch 5:  Loss:     0.8603 Validation Accuracy: 0.800000\n",
      "Epoch 474, CIFAR-10 Batch 1:  Loss:     0.9283 Validation Accuracy: 0.725000\n",
      "Epoch 474, CIFAR-10 Batch 2:  Loss:     0.5567 Validation Accuracy: 0.850000\n",
      "Epoch 474, CIFAR-10 Batch 3:  Loss:     0.5363 Validation Accuracy: 0.900000\n",
      "Epoch 474, CIFAR-10 Batch 4:  Loss:     0.9461 Validation Accuracy: 0.725000\n",
      "Epoch 474, CIFAR-10 Batch 5:  Loss:     0.8617 Validation Accuracy: 0.800000\n",
      "Epoch 475, CIFAR-10 Batch 1:  Loss:     0.9254 Validation Accuracy: 0.725000\n",
      "Epoch 475, CIFAR-10 Batch 2:  Loss:     0.5560 Validation Accuracy: 0.875000\n",
      "Epoch 475, CIFAR-10 Batch 3:  Loss:     0.5377 Validation Accuracy: 0.900000\n",
      "Epoch 475, CIFAR-10 Batch 4:  Loss:     0.9540 Validation Accuracy: 0.725000\n",
      "Epoch 475, CIFAR-10 Batch 5:  Loss:     0.8593 Validation Accuracy: 0.800000\n",
      "Epoch 476, CIFAR-10 Batch 1:  Loss:     0.9315 Validation Accuracy: 0.725000\n",
      "Epoch 476, CIFAR-10 Batch 2:  Loss:     0.5528 Validation Accuracy: 0.875000\n",
      "Epoch 476, CIFAR-10 Batch 3:  Loss:     0.5393 Validation Accuracy: 0.900000\n",
      "Epoch 476, CIFAR-10 Batch 4:  Loss:     0.9457 Validation Accuracy: 0.725000\n",
      "Epoch 476, CIFAR-10 Batch 5:  Loss:     0.8609 Validation Accuracy: 0.775000\n",
      "Epoch 477, CIFAR-10 Batch 1:  Loss:     0.9359 Validation Accuracy: 0.725000\n",
      "Epoch 477, CIFAR-10 Batch 2:  Loss:     0.5548 Validation Accuracy: 0.875000\n",
      "Epoch 477, CIFAR-10 Batch 3:  Loss:     0.5372 Validation Accuracy: 0.900000\n",
      "Epoch 477, CIFAR-10 Batch 4:  Loss:     0.9424 Validation Accuracy: 0.725000\n",
      "Epoch 477, CIFAR-10 Batch 5:  Loss:     0.8522 Validation Accuracy: 0.800000\n",
      "Epoch 478, CIFAR-10 Batch 1:  Loss:     0.9321 Validation Accuracy: 0.725000\n",
      "Epoch 478, CIFAR-10 Batch 2:  Loss:     0.5499 Validation Accuracy: 0.850000\n",
      "Epoch 478, CIFAR-10 Batch 3:  Loss:     0.5384 Validation Accuracy: 0.900000\n",
      "Epoch 478, CIFAR-10 Batch 4:  Loss:     0.9446 Validation Accuracy: 0.725000\n",
      "Epoch 478, CIFAR-10 Batch 5:  Loss:     0.8539 Validation Accuracy: 0.800000\n",
      "Epoch 479, CIFAR-10 Batch 1:  Loss:     0.9336 Validation Accuracy: 0.725000\n",
      "Epoch 479, CIFAR-10 Batch 2:  Loss:     0.5499 Validation Accuracy: 0.875000\n",
      "Epoch 479, CIFAR-10 Batch 3:  Loss:     0.5399 Validation Accuracy: 0.900000\n",
      "Epoch 479, CIFAR-10 Batch 4:  Loss:     0.9477 Validation Accuracy: 0.725000\n",
      "Epoch 479, CIFAR-10 Batch 5:  Loss:     0.8579 Validation Accuracy: 0.800000\n",
      "Epoch 480, CIFAR-10 Batch 1:  Loss:     0.9287 Validation Accuracy: 0.725000\n",
      "Epoch 480, CIFAR-10 Batch 2:  Loss:     0.5554 Validation Accuracy: 0.875000\n",
      "Epoch 480, CIFAR-10 Batch 3:  Loss:     0.5326 Validation Accuracy: 0.900000\n",
      "Epoch 480, CIFAR-10 Batch 4:  Loss:     0.9491 Validation Accuracy: 0.725000\n",
      "Epoch 480, CIFAR-10 Batch 5:  Loss:     0.8598 Validation Accuracy: 0.800000\n",
      "Epoch 481, CIFAR-10 Batch 1:  Loss:     0.9354 Validation Accuracy: 0.725000\n",
      "Epoch 481, CIFAR-10 Batch 2:  Loss:     0.5513 Validation Accuracy: 0.875000\n",
      "Epoch 481, CIFAR-10 Batch 3:  Loss:     0.5365 Validation Accuracy: 0.900000\n",
      "Epoch 481, CIFAR-10 Batch 4:  Loss:     0.9428 Validation Accuracy: 0.725000\n",
      "Epoch 481, CIFAR-10 Batch 5:  Loss:     0.8577 Validation Accuracy: 0.800000\n",
      "Epoch 482, CIFAR-10 Batch 1:  Loss:     0.9335 Validation Accuracy: 0.725000\n",
      "Epoch 482, CIFAR-10 Batch 2:  Loss:     0.5507 Validation Accuracy: 0.875000\n",
      "Epoch 482, CIFAR-10 Batch 3:  Loss:     0.5379 Validation Accuracy: 0.900000\n",
      "Epoch 482, CIFAR-10 Batch 4:  Loss:     0.9494 Validation Accuracy: 0.725000\n",
      "Epoch 482, CIFAR-10 Batch 5:  Loss:     0.8656 Validation Accuracy: 0.800000\n",
      "Epoch 483, CIFAR-10 Batch 1:  Loss:     0.9392 Validation Accuracy: 0.725000\n",
      "Epoch 483, CIFAR-10 Batch 2:  Loss:     0.5545 Validation Accuracy: 0.875000\n",
      "Epoch 483, CIFAR-10 Batch 3:  Loss:     0.5334 Validation Accuracy: 0.900000\n",
      "Epoch 483, CIFAR-10 Batch 4:  Loss:     0.9384 Validation Accuracy: 0.725000\n",
      "Epoch 483, CIFAR-10 Batch 5:  Loss:     0.8578 Validation Accuracy: 0.800000\n",
      "Epoch 484, CIFAR-10 Batch 1:  Loss:     0.9289 Validation Accuracy: 0.725000\n",
      "Epoch 484, CIFAR-10 Batch 2:  Loss:     0.5495 Validation Accuracy: 0.875000\n",
      "Epoch 484, CIFAR-10 Batch 3:  Loss:     0.5314 Validation Accuracy: 0.900000\n",
      "Epoch 484, CIFAR-10 Batch 4:  Loss:     0.9414 Validation Accuracy: 0.725000\n",
      "Epoch 484, CIFAR-10 Batch 5:  Loss:     0.8630 Validation Accuracy: 0.800000\n",
      "Epoch 485, CIFAR-10 Batch 1:  Loss:     0.9385 Validation Accuracy: 0.725000\n",
      "Epoch 485, CIFAR-10 Batch 2:  Loss:     0.5557 Validation Accuracy: 0.850000\n",
      "Epoch 485, CIFAR-10 Batch 3:  Loss:     0.5384 Validation Accuracy: 0.900000\n",
      "Epoch 485, CIFAR-10 Batch 4:  Loss:     0.9461 Validation Accuracy: 0.725000\n",
      "Epoch 485, CIFAR-10 Batch 5:  Loss:     0.8629 Validation Accuracy: 0.800000\n",
      "Epoch 486, CIFAR-10 Batch 1:  Loss:     0.9456 Validation Accuracy: 0.725000\n",
      "Epoch 486, CIFAR-10 Batch 2:  Loss:     0.5513 Validation Accuracy: 0.875000\n",
      "Epoch 486, CIFAR-10 Batch 3:  Loss:     0.5418 Validation Accuracy: 0.900000\n",
      "Epoch 486, CIFAR-10 Batch 4:  Loss:     0.9357 Validation Accuracy: 0.725000\n",
      "Epoch 486, CIFAR-10 Batch 5:  Loss:     0.8518 Validation Accuracy: 0.800000\n",
      "Epoch 487, CIFAR-10 Batch 1:  Loss:     0.9405 Validation Accuracy: 0.725000\n",
      "Epoch 487, CIFAR-10 Batch 2:  Loss:     0.5549 Validation Accuracy: 0.875000\n",
      "Epoch 487, CIFAR-10 Batch 3:  Loss:     0.5374 Validation Accuracy: 0.900000\n",
      "Epoch 487, CIFAR-10 Batch 4:  Loss:     0.9509 Validation Accuracy: 0.725000\n",
      "Epoch 487, CIFAR-10 Batch 5:  Loss:     0.8575 Validation Accuracy: 0.800000\n",
      "Epoch 488, CIFAR-10 Batch 1:  Loss:     0.9369 Validation Accuracy: 0.725000\n",
      "Epoch 488, CIFAR-10 Batch 2:  Loss:     0.5565 Validation Accuracy: 0.875000\n",
      "Epoch 488, CIFAR-10 Batch 3:  Loss:     0.5470 Validation Accuracy: 0.900000\n",
      "Epoch 488, CIFAR-10 Batch 4:  Loss:     0.9504 Validation Accuracy: 0.725000\n",
      "Epoch 488, CIFAR-10 Batch 5:  Loss:     0.8590 Validation Accuracy: 0.800000\n",
      "Epoch 489, CIFAR-10 Batch 1:  Loss:     0.9408 Validation Accuracy: 0.725000\n",
      "Epoch 489, CIFAR-10 Batch 2:  Loss:     0.5600 Validation Accuracy: 0.850000\n",
      "Epoch 489, CIFAR-10 Batch 3:  Loss:     0.5450 Validation Accuracy: 0.900000\n",
      "Epoch 489, CIFAR-10 Batch 4:  Loss:     0.9423 Validation Accuracy: 0.725000\n",
      "Epoch 489, CIFAR-10 Batch 5:  Loss:     0.8570 Validation Accuracy: 0.800000\n",
      "Epoch 490, CIFAR-10 Batch 1:  Loss:     0.9394 Validation Accuracy: 0.725000\n",
      "Epoch 490, CIFAR-10 Batch 2:  Loss:     0.5568 Validation Accuracy: 0.850000\n",
      "Epoch 490, CIFAR-10 Batch 3:  Loss:     0.5342 Validation Accuracy: 0.900000\n",
      "Epoch 490, CIFAR-10 Batch 4:  Loss:     0.9443 Validation Accuracy: 0.725000\n",
      "Epoch 490, CIFAR-10 Batch 5:  Loss:     0.8582 Validation Accuracy: 0.800000\n",
      "Epoch 491, CIFAR-10 Batch 1:  Loss:     0.9359 Validation Accuracy: 0.725000\n",
      "Epoch 491, CIFAR-10 Batch 2:  Loss:     0.5581 Validation Accuracy: 0.875000\n",
      "Epoch 491, CIFAR-10 Batch 3:  Loss:     0.5373 Validation Accuracy: 0.900000\n",
      "Epoch 491, CIFAR-10 Batch 4:  Loss:     0.9456 Validation Accuracy: 0.725000\n",
      "Epoch 491, CIFAR-10 Batch 5:  Loss:     0.8573 Validation Accuracy: 0.800000\n",
      "Epoch 492, CIFAR-10 Batch 1:  Loss:     0.9367 Validation Accuracy: 0.725000\n",
      "Epoch 492, CIFAR-10 Batch 2:  Loss:     0.5515 Validation Accuracy: 0.875000\n",
      "Epoch 492, CIFAR-10 Batch 3:  Loss:     0.5500 Validation Accuracy: 0.900000\n",
      "Epoch 492, CIFAR-10 Batch 4:  Loss:     0.9450 Validation Accuracy: 0.725000\n",
      "Epoch 492, CIFAR-10 Batch 5:  Loss:     0.8620 Validation Accuracy: 0.800000\n",
      "Epoch 493, CIFAR-10 Batch 1:  Loss:     0.9485 Validation Accuracy: 0.750000\n",
      "Epoch 493, CIFAR-10 Batch 2:  Loss:     0.5519 Validation Accuracy: 0.875000\n",
      "Epoch 493, CIFAR-10 Batch 3:  Loss:     0.5482 Validation Accuracy: 0.900000\n",
      "Epoch 493, CIFAR-10 Batch 4:  Loss:     0.9457 Validation Accuracy: 0.725000\n",
      "Epoch 493, CIFAR-10 Batch 5:  Loss:     0.8564 Validation Accuracy: 0.800000\n",
      "Epoch 494, CIFAR-10 Batch 1:  Loss:     0.9465 Validation Accuracy: 0.750000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 494, CIFAR-10 Batch 2:  Loss:     0.5553 Validation Accuracy: 0.875000\n",
      "Epoch 494, CIFAR-10 Batch 3:  Loss:     0.5458 Validation Accuracy: 0.900000\n",
      "Epoch 494, CIFAR-10 Batch 4:  Loss:     0.9422 Validation Accuracy: 0.725000\n",
      "Epoch 494, CIFAR-10 Batch 5:  Loss:     0.8606 Validation Accuracy: 0.775000\n",
      "Epoch 495, CIFAR-10 Batch 1:  Loss:     0.9588 Validation Accuracy: 0.750000\n",
      "Epoch 495, CIFAR-10 Batch 2:  Loss:     0.5650 Validation Accuracy: 0.875000\n",
      "Epoch 495, CIFAR-10 Batch 3:  Loss:     0.5482 Validation Accuracy: 0.900000\n",
      "Epoch 495, CIFAR-10 Batch 4:  Loss:     0.9441 Validation Accuracy: 0.725000\n",
      "Epoch 495, CIFAR-10 Batch 5:  Loss:     0.8578 Validation Accuracy: 0.775000\n",
      "Epoch 496, CIFAR-10 Batch 1:  Loss:     0.9390 Validation Accuracy: 0.750000\n",
      "Epoch 496, CIFAR-10 Batch 2:  Loss:     0.5467 Validation Accuracy: 0.875000\n",
      "Epoch 496, CIFAR-10 Batch 3:  Loss:     0.5396 Validation Accuracy: 0.900000\n",
      "Epoch 496, CIFAR-10 Batch 4:  Loss:     0.9553 Validation Accuracy: 0.725000\n",
      "Epoch 496, CIFAR-10 Batch 5:  Loss:     0.8568 Validation Accuracy: 0.775000\n",
      "Epoch 497, CIFAR-10 Batch 1:  Loss:     0.9460 Validation Accuracy: 0.750000\n",
      "Epoch 497, CIFAR-10 Batch 2:  Loss:     0.5516 Validation Accuracy: 0.875000\n",
      "Epoch 497, CIFAR-10 Batch 3:  Loss:     0.5379 Validation Accuracy: 0.900000\n",
      "Epoch 497, CIFAR-10 Batch 4:  Loss:     0.9404 Validation Accuracy: 0.725000\n",
      "Epoch 497, CIFAR-10 Batch 5:  Loss:     0.8585 Validation Accuracy: 0.800000\n",
      "Epoch 498, CIFAR-10 Batch 1:  Loss:     0.9446 Validation Accuracy: 0.750000\n",
      "Epoch 498, CIFAR-10 Batch 2:  Loss:     0.5484 Validation Accuracy: 0.875000\n",
      "Epoch 498, CIFAR-10 Batch 3:  Loss:     0.5473 Validation Accuracy: 0.900000\n",
      "Epoch 498, CIFAR-10 Batch 4:  Loss:     0.9431 Validation Accuracy: 0.725000\n",
      "Epoch 498, CIFAR-10 Batch 5:  Loss:     0.8540 Validation Accuracy: 0.775000\n",
      "Epoch 499, CIFAR-10 Batch 1:  Loss:     0.9379 Validation Accuracy: 0.725000\n",
      "Epoch 499, CIFAR-10 Batch 2:  Loss:     0.5527 Validation Accuracy: 0.875000\n",
      "Epoch 499, CIFAR-10 Batch 3:  Loss:     0.5386 Validation Accuracy: 0.900000\n",
      "Epoch 499, CIFAR-10 Batch 4:  Loss:     0.9450 Validation Accuracy: 0.725000\n",
      "Epoch 499, CIFAR-10 Batch 5:  Loss:     0.8602 Validation Accuracy: 0.800000\n",
      "Epoch 500, CIFAR-10 Batch 1:  Loss:     0.9468 Validation Accuracy: 0.725000\n",
      "Epoch 500, CIFAR-10 Batch 2:  Loss:     0.5499 Validation Accuracy: 0.875000\n",
      "Epoch 500, CIFAR-10 Batch 3:  Loss:     0.5393 Validation Accuracy: 0.900000\n",
      "Epoch 500, CIFAR-10 Batch 4:  Loss:     0.9493 Validation Accuracy: 0.725000\n",
      "Epoch 500, CIFAR-10 Batch 5:  Loss:     0.8704 Validation Accuracy: 0.800000\n",
      "Epoch 501, CIFAR-10 Batch 1:  Loss:     0.9451 Validation Accuracy: 0.750000\n",
      "Epoch 501, CIFAR-10 Batch 2:  Loss:     0.5586 Validation Accuracy: 0.875000\n",
      "Epoch 501, CIFAR-10 Batch 3:  Loss:     0.5449 Validation Accuracy: 0.900000\n",
      "Epoch 501, CIFAR-10 Batch 4:  Loss:     0.9405 Validation Accuracy: 0.750000\n",
      "Epoch 501, CIFAR-10 Batch 5:  Loss:     0.8592 Validation Accuracy: 0.775000\n",
      "Epoch 502, CIFAR-10 Batch 1:  Loss:     0.9458 Validation Accuracy: 0.750000\n",
      "Epoch 502, CIFAR-10 Batch 2:  Loss:     0.5462 Validation Accuracy: 0.875000\n",
      "Epoch 502, CIFAR-10 Batch 3:  Loss:     0.5385 Validation Accuracy: 0.900000\n",
      "Epoch 502, CIFAR-10 Batch 4:  Loss:     0.9412 Validation Accuracy: 0.725000\n",
      "Epoch 502, CIFAR-10 Batch 5:  Loss:     0.8583 Validation Accuracy: 0.775000\n",
      "Epoch 503, CIFAR-10 Batch 1:  Loss:     0.9474 Validation Accuracy: 0.750000\n",
      "Epoch 503, CIFAR-10 Batch 2:  Loss:     0.5515 Validation Accuracy: 0.875000\n",
      "Epoch 503, CIFAR-10 Batch 3:  Loss:     0.5326 Validation Accuracy: 0.900000\n",
      "Epoch 503, CIFAR-10 Batch 4:  Loss:     0.9422 Validation Accuracy: 0.725000\n",
      "Epoch 503, CIFAR-10 Batch 5:  Loss:     0.8631 Validation Accuracy: 0.800000\n",
      "Epoch 504, CIFAR-10 Batch 1:  Loss:     0.9438 Validation Accuracy: 0.725000\n",
      "Epoch 504, CIFAR-10 Batch 2:  Loss:     0.5504 Validation Accuracy: 0.875000\n",
      "Epoch 504, CIFAR-10 Batch 3:  Loss:     0.5466 Validation Accuracy: 0.900000\n",
      "Epoch 504, CIFAR-10 Batch 4:  Loss:     0.9449 Validation Accuracy: 0.725000\n",
      "Epoch 504, CIFAR-10 Batch 5:  Loss:     0.8512 Validation Accuracy: 0.775000\n",
      "Epoch 505, CIFAR-10 Batch 1:  Loss:     0.9448 Validation Accuracy: 0.725000\n",
      "Epoch 505, CIFAR-10 Batch 2:  Loss:     0.5547 Validation Accuracy: 0.875000\n",
      "Epoch 505, CIFAR-10 Batch 3:  Loss:     0.5487 Validation Accuracy: 0.900000\n",
      "Epoch 505, CIFAR-10 Batch 4:  Loss:     0.9438 Validation Accuracy: 0.725000\n",
      "Epoch 505, CIFAR-10 Batch 5:  Loss:     0.8558 Validation Accuracy: 0.800000\n",
      "Epoch 506, CIFAR-10 Batch 1:  Loss:     0.9426 Validation Accuracy: 0.750000\n",
      "Epoch 506, CIFAR-10 Batch 2:  Loss:     0.5513 Validation Accuracy: 0.875000\n",
      "Epoch 506, CIFAR-10 Batch 3:  Loss:     0.5389 Validation Accuracy: 0.900000\n",
      "Epoch 506, CIFAR-10 Batch 4:  Loss:     0.9368 Validation Accuracy: 0.725000\n",
      "Epoch 506, CIFAR-10 Batch 5:  Loss:     0.8561 Validation Accuracy: 0.800000\n",
      "Epoch 507, CIFAR-10 Batch 1:  Loss:     0.9440 Validation Accuracy: 0.750000\n",
      "Epoch 507, CIFAR-10 Batch 2:  Loss:     0.5485 Validation Accuracy: 0.875000\n",
      "Epoch 507, CIFAR-10 Batch 3:  Loss:     0.5368 Validation Accuracy: 0.900000\n",
      "Epoch 507, CIFAR-10 Batch 4:  Loss:     0.9514 Validation Accuracy: 0.725000\n",
      "Epoch 507, CIFAR-10 Batch 5:  Loss:     0.8536 Validation Accuracy: 0.775000\n",
      "Epoch 508, CIFAR-10 Batch 1:  Loss:     0.9473 Validation Accuracy: 0.750000\n",
      "Epoch 508, CIFAR-10 Batch 2:  Loss:     0.5603 Validation Accuracy: 0.850000\n",
      "Epoch 508, CIFAR-10 Batch 3:  Loss:     0.5437 Validation Accuracy: 0.900000\n",
      "Epoch 508, CIFAR-10 Batch 4:  Loss:     0.9470 Validation Accuracy: 0.725000\n",
      "Epoch 508, CIFAR-10 Batch 5:  Loss:     0.8524 Validation Accuracy: 0.775000\n",
      "Epoch 509, CIFAR-10 Batch 1:  Loss:     0.9337 Validation Accuracy: 0.725000\n",
      "Epoch 509, CIFAR-10 Batch 2:  Loss:     0.5535 Validation Accuracy: 0.875000\n",
      "Epoch 509, CIFAR-10 Batch 3:  Loss:     0.5317 Validation Accuracy: 0.900000\n",
      "Epoch 509, CIFAR-10 Batch 4:  Loss:     0.9428 Validation Accuracy: 0.725000\n",
      "Epoch 509, CIFAR-10 Batch 5:  Loss:     0.8551 Validation Accuracy: 0.800000\n",
      "Epoch 510, CIFAR-10 Batch 1:  Loss:     0.9433 Validation Accuracy: 0.750000\n",
      "Epoch 510, CIFAR-10 Batch 2:  Loss:     0.5514 Validation Accuracy: 0.875000\n",
      "Epoch 510, CIFAR-10 Batch 3:  Loss:     0.5375 Validation Accuracy: 0.900000\n",
      "Epoch 510, CIFAR-10 Batch 4:  Loss:     0.9471 Validation Accuracy: 0.725000\n",
      "Epoch 510, CIFAR-10 Batch 5:  Loss:     0.8525 Validation Accuracy: 0.800000\n",
      "Epoch 511, CIFAR-10 Batch 1:  Loss:     0.9501 Validation Accuracy: 0.750000\n",
      "Epoch 511, CIFAR-10 Batch 2:  Loss:     0.5451 Validation Accuracy: 0.875000\n",
      "Epoch 511, CIFAR-10 Batch 3:  Loss:     0.5352 Validation Accuracy: 0.900000\n",
      "Epoch 511, CIFAR-10 Batch 4:  Loss:     0.9413 Validation Accuracy: 0.725000\n",
      "Epoch 511, CIFAR-10 Batch 5:  Loss:     0.8499 Validation Accuracy: 0.775000\n",
      "Epoch 512, CIFAR-10 Batch 1:  Loss:     0.9449 Validation Accuracy: 0.700000\n",
      "Epoch 512, CIFAR-10 Batch 2:  Loss:     0.5594 Validation Accuracy: 0.875000\n",
      "Epoch 512, CIFAR-10 Batch 3:  Loss:     0.5357 Validation Accuracy: 0.900000\n",
      "Epoch 512, CIFAR-10 Batch 4:  Loss:     0.9441 Validation Accuracy: 0.725000\n",
      "Epoch 512, CIFAR-10 Batch 5:  Loss:     0.8599 Validation Accuracy: 0.775000\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 检查点\n",
    "\n",
    "该模型已经被存储到你的硬盘中。\n",
    "\n",
    "## 测试模型\n",
    "\n",
    "这部分将在测试数据集上测试你的模型。这边得到的准确率将作为你的最终准确率。你应该得到一个高于 50% 准确率。如果它没有超过 50%，那么你需要继续调整模型架构及参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./image_classification\n",
      "Testing Accuracy: 0.52685546875\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nREQkUnAsIiIiIhIpOBYRERERif4/cpMWYimNcf0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x18bc007e668>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_training.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for train_feature_batch, train_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: train_feature_batch, loaded_y: train_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why 50-80% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. That's because there are many more techniques that can be applied to your model and we recemmond that once you are done with this project, you explore!\n",
    "\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 为什么仅有 50%~ 80% 的准确率？\n",
    "\n",
    "你也许会觉得奇怪，为什么你的准确率总是提高不上去。对于简单的 CNN 网络而言，50% 并非是很差的表现。纯粹的猜测只会得到 10% 的准确率（因为一共有 10 类）。这是因为还有许多许多能够应用到你模型的技巧。在你做完了该项目之后，你可以探索探索我们给你推荐的一些方法。\n",
    "\n",
    "\n",
    "## 提交该项目\n",
    "\n",
    "在提交项目前，请确保你在运行了所有的 cell 之后保存了项目。将项目储存为 \"image_classification.ipynb\" 并导出为一个 HTML 文件。你可以再菜单栏中选择 File -> Download as 进行导出。请将 \"helper.py\" 及  \"problem_unittests.py\" 文件也放在你的提交文件中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
